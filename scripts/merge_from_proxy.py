#!/usr/bin/env python3
"""
M3Uæ–‡ä»¶åˆå¹¶è„šæœ¬
1. ä¸‹è½½BB.m3uï¼ˆåŒ…å«EPGä¿¡æ¯ï¼‰
2. ä»Cloudflareä»£ç†è·å–å†…å®¹
3. æå–4gtvå‰30ä¸ªç›´æ’­ï¼Œåˆ†ç»„æ”¹ä¸ºTW
4. æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHK
5. åˆå¹¶ç”ŸæˆCC.m3u
"""

import requests
import re
import os
import time
from datetime import datetime

# é…ç½®
BB_URL = "https://raw.githubusercontent.com/sufernnet/joker/main/BB.m3u"
CLOUDFLARE_PROXY = "https://smt-proxy.sufern001.workers.dev/"
OUTPUT_FILE = "CC.m3u"

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def download_bb_m3u():
    """ä¸‹è½½BB.m3uå¹¶æå–EPG"""
    try:
        log("ä¸‹è½½BB.m3u...")
        response = requests.get(BB_URL, timeout=10)
        response.raise_for_status()
        
        bb_content = response.text
        log(f"âœ… BB.m3uä¸‹è½½æˆåŠŸ ({len(bb_content)} å­—ç¬¦)")
        
        # æå–EPGä¿¡æ¯
        epg_match = re.search(r'url-tvg="([^"]+)"', bb_content)
        epg_url = epg_match.group(1) if epg_match else None
        
        if epg_url:
            log(f"âœ… ä½¿ç”¨BBçš„EPG: {epg_url}")
        
        return bb_content, epg_url
        
    except Exception as e:
        log(f"âŒ BB.m3uä¸‹è½½å¤±è´¥: {e}")
        return None, None

def get_content_from_proxy():
    """ä»Cloudflareä»£ç†è·å–å†…å®¹"""
    try:
        log("ä»Cloudflareä»£ç†è·å–å†…å®¹...")
        headers = {
            'User-Agent': 'Mozilla/5.0',
            'Accept': '*/*'
        }
        
        response = requests.get(CLOUDFLARE_PROXY, headers=headers, timeout=15)
        
        if response.status_code == 200:
            content = response.text
            
            # å¦‚æœæ˜¯HTMLï¼Œå°è¯•æå–M3Uå†…å®¹
            if '<html' in content.lower():
                # æŸ¥æ‰¾M3Uå†…å®¹
                m3u_match = re.search(r'(#EXTM3U.*?)(?:</pre>|</code>|$)', content, re.DOTALL)
                if m3u_match:
                    content = m3u_match.group(1).strip()
                    log("âœ… ä»HTMLæå–åˆ°M3Uå†…å®¹")
                else:
                    # æå–æ‰€æœ‰å¯èƒ½çš„é¢‘é“è¡Œ
                    lines = content.split('\n')
                    m3u_lines = []
                    for line in lines:
                        line = line.strip()
                        if line.startswith('#EXTINF:') or ('://' in line and not line.startswith('<')):
                            m3u_lines.append(line)
                    
                    if m3u_lines:
                        content = '#EXTM3U\n' + '\n'.join(m3u_lines)
                        log(f"âœ… ä»HTMLæå–åˆ° {len(m3u_lines)} ä¸ªé¢‘é“è¡Œ")
            
            if content and content.strip():
                log(f"âœ… è·å–åˆ°å†…å®¹ ({len(content)} å­—ç¬¦)")
                return content
            else:
                log("âš ï¸  å†…å®¹ä¸ºç©º")
        else:
            log(f"âŒ ä»£ç†è¿”å›é”™è¯¯: {response.status_code}")
            
    except Exception as e:
        log(f"âŒ ä»£ç†è®¿é—®å¤±è´¥: {e}")
    
    return None

def extract_4gtv_channels(content, limit=30):
    """æå–4gtvé¢‘é“ï¼ˆå‰30ä¸ªï¼‰ï¼Œåˆ†ç»„æ”¹ä¸ºTW"""
    if not content:
        return []
    
    log(f"æå–4gtvå‰{limit}ä¸ªç›´æ’­ï¼Œåˆ†ç»„æ”¹ä¸ºTW...")
    
    # è§£æM3Uå†…å®¹
    lines = content.split('\n')
    channels = []
    current_extinf = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            current_extinf = line
        elif current_extinf and '://' in line and not line.startswith('#'):
            channels.append((current_extinf, line))
            current_extinf = None
    
    # è¿‡æ»¤4gtvé¢‘é“ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    filtered_channels = []
    for extinf, url in channels:
        if '4gtv' in extinf.lower():
            filtered_channels.append((extinf, url))
    
    log(f"æ‰¾åˆ° {len(filtered_channels)} ä¸ª4gtvé¢‘é“")
    
    # åªå–å‰limitä¸ª
    if len(filtered_channels) > limit:
        filtered_channels = filtered_channels[:limit]
        log(f"åªå–å‰ {limit} ä¸ª4gtvé¢‘é“")
    
    # é‡å‘½åä¸ºTWåˆ†ç»„
    tw_channels = []
    seen = set()
    
    for extinf, url in filtered_channels:
        # æ›¿æ¢åˆ†ç»„ä¸ºTW
        new_extinf = extinf
        
        # æ›¿æ¢4gtvä¸ºTWï¼ˆåœ¨é¢‘é“åä¸­ï¼‰
        if '4gtv' in new_extinf.lower():
            new_extinf = re.sub(r'4gtv', 'TW', new_extinf, flags=re.IGNORECASE)
        
        # ç¡®ä¿group-titleä¸ºTW
        if 'group-title=' in new_extinf:
            new_extinf = re.sub(r'group-title="[^"]*"', 'group-title="TW"', new_extinf)
        else:
            # æ·»åŠ group-title
            if ',' in new_extinf:
                parts = new_extinf.split(',', 1)
                new_extinf = f'{parts[0]} group-title="TW",{parts[1]}'
        
        # å»é‡
        key = f"{new_extinf}|{url}"
        if key not in seen:
            seen.add(key)
            tw_channels.append((new_extinf, url))
    
    log(f"âœ… æå–åˆ° {len(tw_channels)} ä¸ªTWé¢‘é“ï¼ˆåŸ4gtvï¼‰")
    
    if tw_channels:
        log("TWé¢‘é“ç¤ºä¾‹:")
        for i, (extinf, url) in enumerate(tw_channels[:5]):
            name = extinf.split(',', 1)[1] if ',' in extinf else extinf
            log(f"  {i+1}. {name[:50]}...")
    
    return tw_channels

def extract_hk_channels(content):
    """æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHK"""
    if not content:
        return []
    
    log("æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHK...")
    
    # è§£æM3Uå†…å®¹
    lines = content.split('\n')
    channels = []
    current_extinf = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            current_extinf = line
        elif current_extinf and '://' in line and not line.startswith('#'):
            channels.append((current_extinf, line))
            current_extinf = None
    
    # è¿‡æ»¤JULIé¢‘é“
    hk_channels = []
    seen = set()
    
    for extinf, url in channels:
        if 'JULI' in extinf.upper():
            # é‡å‘½åä¸ºHKåˆ†ç»„
            new_extinf = re.sub(r'JULI', 'HK', extinf, flags=re.IGNORECASE)
            
            # ç¡®ä¿group-titleä¸ºHK
            if 'group-title=' in new_extinf:
                new_extinf = re.sub(r'group-title="[^"]*"', 'group-title="HK"', new_extinf)
            else:
                # æ·»åŠ group-title
                if ',' in new_extinf:
                    parts = new_extinf.split(',', 1)
                    new_extinf = f'{parts[0]} group-title="HK",{parts[1]}'
            
            # å»é‡
            key = f"{new_extinf}|{url}"
            if key not in seen:
                seen.add(key)
                hk_channels.append((new_extinf, url))
    
    log(f"âœ… æå–åˆ° {len(hk_channels)} ä¸ªHKé¢‘é“ï¼ˆåŸJULIï¼‰")
    
    if hk_channels:
        log("HKé¢‘é“ç¤ºä¾‹:")
        for i, (extinf, url) in enumerate(hk_channels[:5]):
            name = extinf.split(',', 1)[1] if ',' in extinf else extinf
            log(f"  {i+1}. {name[:50]}...")
    
    return hk_channels

def main():
    """ä¸»å‡½æ•°"""
    log("å¼€å§‹åˆå¹¶M3Uæ–‡ä»¶...")
    
    # 1. ä¸‹è½½BB.m3uå¹¶è·å–EPG
    bb_content, epg_url = download_bb_m3u()
    if not bb_content:
        log("âŒ æ— æ³•ç»§ç»­ï¼ŒBB.m3uä¸‹è½½å¤±è´¥")
        return
    
    # 2. ä»ä»£ç†è·å–å†…å®¹
    proxy_content = get_content_from_proxy()
    
    # 3. æå–TWé¢‘é“ï¼ˆ4gtvå‰30ä¸ªï¼‰
    tw_channels = []
    if proxy_content:
        tw_channels = extract_4gtv_channels(proxy_content, limit=30)
    else:
        log("âš ï¸  æ— æ³•ä»ä»£ç†è·å–å†…å®¹ï¼Œè·³è¿‡TWé¢‘é“")
    
    # 4. æå–HKé¢‘é“ï¼ˆJULIï¼‰
    hk_channels = []
    if proxy_content:
        hk_channels = extract_hk_channels(proxy_content)
    else:
        log("âš ï¸  æ— æ³•ä»ä»£ç†è·å–å†…å®¹ï¼Œè·³è¿‡HKé¢‘é“")
    
    # 5. æ„å»ºM3Uå†…å®¹
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # M3Uå¤´éƒ¨ï¼ˆä½¿ç”¨BBçš„EPGï¼‰
    if epg_url:
        m3u_header = f'#EXTM3U url-tvg="{epg_url}"\n'
    else:
        m3u_header = '#EXTM3U\n'
    
    output = m3u_header + f"""# è‡ªåŠ¨åˆå¹¶ M3U æ–‡ä»¶
# ç”Ÿæˆæ—¶é—´: {timestamp}
# BBæº: {BB_URL}
# ä»£ç†æº: {CLOUDFLARE_PROXY}
# 4gtvåˆ†ç»„å·²æ”¹ä¸ºTWï¼ˆå‰30ä¸ªï¼‰
# JULIåˆ†ç»„å·²æ”¹ä¸ºHK
# EPG: {epg_url if epg_url else 'BBçš„XML'}
# GitHub Actions è‡ªåŠ¨ç”Ÿæˆ

"""
    
    # æ·»åŠ BBå†…å®¹ï¼ˆè·³è¿‡ç¬¬ä¸€è¡Œï¼‰
    bb_lines = bb_content.split('\n')
    bb_count = 0
    skip_first = True
    
    for line in bb_lines:
        line = line.rstrip()
        if not line:
            continue
        
        if skip_first and line.startswith('#EXTM3U'):
            skip_first = False
            continue
        
        output += line + '\n'
        if line.startswith('#EXTINF:'):
            bb_count += 1
    
    # æ·»åŠ TWé¢‘é“ï¼ˆ4gtvï¼‰
    if tw_channels:
        output += f"\n# TWé¢‘é“ (åŸ4gtvï¼Œå‰30ä¸ª)\n"
        for extinf, url in tw_channels:
            output += extinf + '\n'
            output += url + '\n'
    
    # æ·»åŠ HKé¢‘é“ï¼ˆJULIï¼‰
    if hk_channels:
        output += f"\n# HKé¢‘é“ (åŸJULI)\n"
        for extinf, url in hk_channels:
            output += extinf + '\n'
            output += url + '\n'
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    output += f"""
# ç»Ÿè®¡ä¿¡æ¯
# BB é¢‘é“æ•°: {bb_count}
# TW é¢‘é“æ•°: {len(tw_channels)} (åŸ4gtvå‰30ä¸ª)
# HK é¢‘é“æ•°: {len(hk_channels)} (åŸJULI)
# æ€»é¢‘é“æ•°: {bb_count + len(tw_channels) + len(hk_channels)}
# æ›´æ–°æ—¶é—´: {timestamp}
# æ›´æ–°é¢‘ç‡: æ¯å¤© 06:00 å’Œ 18:00
"""
    
    # 6. ä¿å­˜æ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(output)
    
    log(f"\nğŸ‰ åˆå¹¶å®Œæˆ!")
    log(f"ğŸ“ æ–‡ä»¶: {OUTPUT_FILE}")
    log(f"ğŸ“ å¤§å°: {len(output)} å­—ç¬¦")
    log(f"ğŸ“¡ EPG: {epg_url}")
    log(f"ğŸ“º BBé¢‘é“: {bb_count}")
    log(f"ğŸ“º TWé¢‘é“: {len(tw_channels)} (4gtvå‰30ä¸ª)")
    log(f"ğŸ“º HKé¢‘é“: {len(hk_channels)} (JULI)")
    log(f"ğŸ“º æ€»è®¡: {bb_count + len(tw_channels) + len(hk_channels)}")

if __name__ == "__main__":
    main()
