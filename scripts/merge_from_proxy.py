#!/usr/bin/env python3
"""
M3Uæ–‡ä»¶åˆå¹¶è„šæœ¬ - ä»æ–°åœ°å€æŠ“å–ğŸ”¥å…¨ç½‘é€šæ¸¯æ¾³å°ç›´æ’­æº
1. ä¸‹è½½BB.m3uï¼ˆåŒ…å«EPGä¿¡æ¯ï¼‰
2. ä»æ–°åœ°å€æŠ“å–"ğŸ”¥å…¨ç½‘é€šæ¸¯æ¾³å°"ç›´æ’­æº
3. ä¸BBåˆå¹¶ç”ŸæˆCC.m3u
åŒ—äº¬æ—¶é—´æ¯å¤©6:00ã€18:00è‡ªåŠ¨è¿è¡Œ
"""

import requests
import re
import os
import time
from datetime import datetime

# é…ç½®
BB_URL = "https://raw.githubusercontent.com/sufernnet/joker/main/BB.m3u"
NEW_SOURCE_URL = "https://gh-proxy.org/https://raw.githubusercontent.com/Jsnzkpg/Jsnzkpg/Jsnzkpg/Jsnzkpg1"
TARGET_GROUP = "ğŸ”¥å…¨ç½‘é€šæ¸¯æ¾³å°"
OUTPUT_FILE = "CC.m3u"

# å¤‡é€‰EPGæºï¼ˆå¦‚æœä¸»è¦EPGå¤±æ•ˆï¼‰
BACKUP_EPG_URLS = [
    "https://epg.112114.xyz/pp.xml",  # BBçš„EPG
    "https://epg.946985.filegear-sg.me/t.xml.gz",
    "http://epg.51zmt.top:8000/e.xml"
]

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def test_epg_url(epg_url):
    """æµ‹è¯•EPG URLæ˜¯å¦å¯è®¿é—®"""
    try:
        log(f"æµ‹è¯•EPG: {epg_url}")
        headers = {
            'User-Agent': 'Mozilla/5.0',
            'Accept': '*/*'
        }
        
        # åªä¸‹è½½å‰1KBæ£€æŸ¥
        response = requests.get(epg_url, headers=headers, timeout=10, stream=True)
        
        if response.status_code == 200:
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            
            # è¯»å–å‰1KBæ£€æŸ¥
            chunk = response.raw.read(1024)
            text = chunk.decode('utf-8', errors='ignore')
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯XMLæ ¼å¼
            if '<?xml' in text or '<tv' in text or '<programme' in text:
                log(f"âœ… EPGå¯ç”¨: {epg_url}")
                return True
            else:
                log(f"âš ï¸  EPGä¸æ˜¯XMLæ ¼å¼: {epg_url}")
                return False
        else:
            log(f"âŒ EPGä¸å¯è®¿é—®: {epg_url} (çŠ¶æ€ç : {response.status_code})")
            return False
            
    except Exception as e:
        log(f"âŒ EPGæµ‹è¯•å¤±è´¥ {epg_url}: {e}")
        return False

def get_best_epg_url(epg_urls):
    """è·å–æœ€ä½³çš„EPG URL"""
    log("å¯»æ‰¾æœ€ä½³EPGæº...")
    
    # æµ‹è¯•æ‰€æœ‰EPG
    working_epgs = []
    for epg_url in epg_urls:
        if test_epg_url(epg_url):
            working_epgs.append(epg_url)
    
    if working_epgs:
        # ä¼˜å…ˆä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„
        best_epg = working_epgs[0]
        log(f"âœ… é€‰æ‹©EPG: {best_epg}")
        log(f"   å…¶ä»–å¯ç”¨EPG: {len(working_epgs)-1}ä¸ª")
        return best_epg
    else:
        log("âš ï¸  æ²¡æœ‰å¯ç”¨çš„EPGæº")
        return None

def download_bb_m3u():
    """ä¸‹è½½BB.m3uå¹¶æå–EPG"""
    try:
        log("ä¸‹è½½BB.m3u...")
        response = requests.get(BB_URL, timeout=10)
        response.raise_for_status()
        
        bb_content = response.text
        log(f"âœ… BB.m3uä¸‹è½½æˆåŠŸ ({len(bb_content)} å­—ç¬¦)")
        
        return bb_content
        
    except Exception as e:
        log(f"âŒ BB.m3uä¸‹è½½å¤±è´¥: {e}")
        return None

def get_quanwangtong_gangaotai():
    """ä»æ–°åœ°å€æŠ“å–"ğŸ”¥å…¨ç½‘é€šæ¸¯æ¾³å°"ç›´æ’­æº"""
    try:
        log(f"ä»æ–°åœ°å€æŠ“å–{TARGET_GROUP}ç›´æ’­æº...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://github.com/'
        }
        
        response = requests.get(NEW_SOURCE_URL, headers=headers, timeout=15)
        
        if response.status_code == 200:
            content = response.text
            
            if content and content.strip():
                log(f"âœ… è·å–åˆ°å†…å®¹ ({len(content)} å­—ç¬¦)")
                
                # æŸ¥æ‰¾"ğŸ”¥å…¨ç½‘é€šæ¸¯æ¾³å°"åˆ†ç»„
                if TARGET_GROUP in content:
                    log(f"âœ… æ‰¾åˆ°'{TARGET_GROUP}'åˆ†ç»„")
                    
                    # æå–è¯¥åˆ†ç»„çš„æ‰€æœ‰é¢‘é“
                    extracted_channels = extract_target_group_channels(content, TARGET_GROUP)
                    
                    if extracted_channels:
                        # æ„å»ºM3Uå†…å®¹
                        m3u_content = "#EXTM3U\n" + "\n".join(extracted_channels)
                        log(f"âœ… æå–åˆ° {len(extracted_channels)//2} ä¸ª{TARGET_GROUP}é¢‘é“")
                        return m3u_content
                    else:
                        log(f"âš ï¸  æœªèƒ½æå–åˆ°{TARGET_GROUP}é¢‘é“çš„å…·ä½“å†…å®¹")
                else:
                    log(f"âš ï¸  æœªæ‰¾åˆ°'{TARGET_GROUP}'åˆ†ç»„")
                    
                    # è°ƒè¯•ï¼šåˆ—å‡ºæ‰€æœ‰åˆ†ç»„
                    log("æ­£åœ¨åˆ†æå†…å®¹ä¸­çš„åˆ†ç»„...")
                    groups = re.findall(r'group-title="([^"]+)"', content)
                    unique_groups = list(set(groups))
                    log(f"å‘ç° {len(unique_groups)} ä¸ªåˆ†ç»„:")
                    for group in sorted(unique_groups)[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
                        log(f"  - {group}")
                    
                    # å°è¯•æŸ¥æ‰¾åŒ…å«"æ¸¯æ¾³å°"çš„åˆ†ç»„
                    for group in unique_groups:
                        if "æ¸¯æ¾³å°" in group:
                            log(f"âš ï¸  å‘ç°ç±»ä¼¼åˆ†ç»„: {group}")
                            # å°è¯•æå–è¿™ä¸ªåˆ†ç»„
                            extracted_channels = extract_target_group_channels(content, group)
                            if extracted_channels:
                                m3u_content = "#EXTM3U\n" + "\n".join(extracted_channels)
                                log(f"âœ… æå–åˆ° {len(extracted_channels)//2} ä¸ª'{group}'é¢‘é“")
                                return m3u_content
            else:
                log("âš ï¸  å†…å®¹ä¸ºç©º")
        else:
            log(f"âŒ æ–°åœ°å€è¿”å›é”™è¯¯: {response.status_code}")
            
    except Exception as e:
        log(f"âŒ ä»æ–°åœ°å€æŠ“å–å¤±è´¥: {e}")
    
    return None

def extract_target_group_channels(content, target_group):
    """ä»å†…å®¹ä¸­æå–æŒ‡å®šåˆ†ç»„çš„æ‰€æœ‰é¢‘é“"""
    lines = content.split('\n')
    channels = []
    current_extinf = None
    in_target_group = False
    
    for i, line in enumerate(lines):
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            # æ£€æŸ¥æ˜¯å¦åœ¨ç›®æ ‡åˆ†ç»„ä¸­
            if target_group in line:
                in_target_group = True
                current_extinf = line
            else:
                in_target_group = False
                current_extinf = None
        
        elif in_target_group and current_extinf and '://' in line and not line.startswith('#'):
            # è¿™æ˜¯ä¸€ä¸ªé¢‘é“URLï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            channels.append(current_extinf)
            channels.append(line)
            current_extinf = None
            in_target_group = False
    
    return channels

def extract_epg_urls(content):
    """ä»å†…å®¹ä¸­æå–EPG URL"""
    epg_urls = []
    
    if not content:
        return epg_urls
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„EPG URLæ¨¡å¼
    patterns = [
        r'url-tvg="([^"]+)"',
        r'x-tvg-url="([^"]+)"',
        r'epg-url="([^"]+)"',
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        for match in matches:
            if 'http' in match and match not in epg_urls:
                epg_urls.append(match)
                log(f"æ‰¾åˆ°EPG: {match}")
    
    return epg_urls

def main():
    """ä¸»å‡½æ•°"""
    log("å¼€å§‹åˆå¹¶M3Uæ–‡ä»¶...")
    
    # æ˜¾ç¤ºå½“å‰æ—¶é—´ï¼ˆç”¨äºè°ƒè¯•å®šæ—¶ä»»åŠ¡ï¼‰
    current_time = datetime.now()
    log(f"å½“å‰æ—¶é—´: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
    log(f"ä¸‹æ¬¡è¿è¡Œ: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 18:00")
    log(f"æ–°æºåœ°å€: {NEW_SOURCE_URL}")
    log(f"ç›®æ ‡åˆ†ç»„: {TARGET_GROUP}")
    
    # 1. ä¸‹è½½BB.m3u
    bb_content = download_bb_m3u()
    if not bb_content:
        log("âŒ æ— æ³•ç»§ç»­ï¼ŒBB.m3uä¸‹è½½å¤±è´¥")
        return
    
    # 2. ä»æ–°åœ°å€æŠ“å–"ğŸ”¥å…¨ç½‘é€šæ¸¯æ¾³å°"ç›´æ’­æº
    new_source_content = get_quanwangtong_gangaotai()
    
    # 3. æ”¶é›†æ‰€æœ‰EPGæº
    epg_urls = []
    
    # ä»BB.m3uæå–EPG
    bb_epg_match = re.search(r'url-tvg="([^"]+)"', bb_content)
    if bb_epg_match:
        epg_urls.append(bb_epg_match.group(1))
        log(f"âœ… æ‰¾åˆ°BB EPG: {bb_epg_match.group(1)}")
    
    # ä»æ–°æºå†…å®¹æå–EPG
    if new_source_content:
        new_epgs = extract_epg_urls(new_source_content)
        for epg in new_epgs:
            if epg not in epg_urls:
                epg_urls.append(epg)
    
    # æ·»åŠ å¤‡é€‰EPG
    for backup_epg in BACKUP_EPG_URLS:
        if backup_epg not in epg_urls:
            epg_urls.append(backup_epg)
    
    log(f"æ‰¾åˆ° {len(epg_urls)} ä¸ªEPGæº")
    
    # 4. è·å–æœ€ä½³EPG
    best_epg = get_best_epg_url(epg_urls)
    
    # 5. è§£ææ–°æºå†…å®¹
    new_channels_count = 0
    new_channels_content = ""
    
    if new_source_content:
        # ç›´æ¥ä½¿ç”¨æå–çš„å†…å®¹ï¼Œè·³è¿‡M3Uå¤´
        lines = new_source_content.split('\n')
        for line in lines:
            line = line.rstrip()
            if not line:
                continue
            
            if line.startswith('#EXTM3U'):
                continue
            
            new_channels_content += line + '\n'
            if line.startswith('#EXTINF:'):
                new_channels_count += 1
    
    # 6. æ„å»ºM3Uå†…å®¹
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # M3Uå¤´éƒ¨ï¼ˆä½¿ç”¨æœ€ä½³EPGï¼‰
    if best_epg:
        m3u_header = f'#EXTM3U url-tvg="{best_epg}"\n'
        log(f"âœ… ä½¿ç”¨EPG: {best_epg}")
    else:
        m3u_header = '#EXTM3U\n'
        log("âš ï¸  æœªæ‰¾åˆ°å¯ç”¨EPG")
    
    output = m3u_header + f"""# è‡ªåŠ¨åˆå¹¶ M3U æ–‡ä»¶
# ç”Ÿæˆæ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# ä¸‹æ¬¡æ›´æ–°: æ¯å¤© 06:00 å’Œ 18:00 (åŒ—äº¬æ—¶é—´)
# BBæº: {BB_URL}
# æ–°æºåœ°å€: {NEW_SOURCE_URL}
# æŠ“å–åˆ†ç»„: {TARGET_GROUP}
# EPGæº: {best_epg if best_epg else 'æ— å¯ç”¨EPG'}
# æµ‹è¯•çš„EPGæº: {len(epg_urls)} ä¸ª
# GitHub Actions è‡ªåŠ¨ç”Ÿæˆ

"""
    
    # æ·»åŠ BBå†…å®¹ï¼ˆè·³è¿‡ç¬¬ä¸€è¡Œï¼‰
    bb_lines = bb_content.split('\n')
    bb_count = 0
    skip_first = True
    
    for line in bb_lines:
        line = line.rstrip()
        if not line:
            continue
        
        if skip_first and line.startswith('#EXTM3U'):
            skip_first = False
            continue
        
        output += line + '\n'
        if line.startswith('#EXTINF:'):
            bb_count += 1
    
    # æ·»åŠ ğŸ”¥å…¨ç½‘é€šæ¸¯æ¾³å°é¢‘é“
    if new_channels_content:
        output += f"\n# {TARGET_GROUP}é¢‘é“\n"
        output += f"# æ¥è‡ª: {NEW_SOURCE_URL}\n"
        output += new_channels_content
    
    # æ·»åŠ EPGä¿¡æ¯è¯´æ˜
    if epg_urls:
        output += f"""
# EPGä¿¡æ¯
# ä½¿ç”¨EPG: {best_epg if best_epg else 'æ— '}
# æµ‹è¯•çš„EPGæº ({len(epg_urls)}ä¸ª):"""
        for i, epg in enumerate(epg_urls, 1):
            status = "âœ…" if epg == best_epg else "  "
            output += f"\n#   {status} {epg}"
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    output += f"""
# ç»Ÿè®¡ä¿¡æ¯
# BB é¢‘é“æ•°: {bb_count}
# {TARGET_GROUP} é¢‘é“æ•°: {new_channels_count}
# æ€»é¢‘é“æ•°: {bb_count + new_channels_count}
# EPGçŠ¶æ€: {'âœ… æ­£å¸¸' if best_epg else 'âŒ æ— å¯ç”¨EPG'}
# æ›´æ–°æ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# æ›´æ–°é¢‘ç‡: æ¯å¤© 06:00 å’Œ 18:00 (åŒ—äº¬æ—¶é—´)
"""
    
    # 7. ä¿å­˜æ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(output)
    
    log(f"\nğŸ‰ åˆå¹¶å®Œæˆ!")
    log(f"ğŸ“ æ–‡ä»¶: {OUTPUT_FILE}")
    log(f"ğŸ“ å¤§å°: {len(output)} å­—ç¬¦")
    log(f"ğŸ“¡ EPG: {best_epg if best_epg else 'æ— å¯ç”¨EPG'}")
    log(f"ğŸ“º BBé¢‘é“: {bb_count}")
    log(f"ğŸ“º {TARGET_GROUP}é¢‘é“: {new_channels_count}")
    log(f"ğŸ“º æ€»è®¡: {bb_count + new_channels_count}")
    log(f"ğŸ•’ ä¸‹æ¬¡è‡ªåŠ¨æ›´æ–°: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 18:00")

if __name__ == "__main__":
    main()
