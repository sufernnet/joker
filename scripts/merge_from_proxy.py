#!/usr/bin/env python3
"""
M3Uæ–‡ä»¶åˆå¹¶è„šæœ¬
1. ä¸‹è½½BB.m3uï¼ˆåŒ…å«EPGä¿¡æ¯ï¼‰
2. ä»Cloudflareä»£ç†è·å–å†…å®¹
3. æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHKï¼ˆæ’åœ¨å‰é¢ï¼‰
4. æå–4gtvå‰30ä¸ªç›´æ’­ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼ˆæ’åœ¨åé¢ï¼‰ï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“
5. åˆå¹¶ç”ŸæˆCC.m3u
åŒ—äº¬æ—¶é—´æ¯å¤©6:00ã€18:00è‡ªåŠ¨è¿è¡Œ
"""

import requests
import re
import os
import time
from datetime import datetime

# é…ç½®
BB_URL = "https://raw.githubusercontent.com/sufernnet/joker/main/BB.m3u"
CLOUDFLARE_PROXY = "https://smt-proxy.sufern001.workers.dev/"
OUTPUT_FILE = "CC.m3u"

# éœ€è¦è¿‡æ»¤æ‰çš„TWé¢‘é“å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
BLACKLIST_TW = [
    "Bloomberg TV",
    "Bloomberg",
    "SBNå…¨çƒè´¢ç»å°",
    "SBNè´¢ç»",
    "FRANCE24è‹±æ–‡å°",
    "FRANCE24",
    "åŠå²›å›½é™…æ–°é—»å°",
    "åŠå²›å›½é™…",
    "åŠå³¶",
    "æ—¥æœ¬",
    "SBN",
    "NHK world-japan",
    "NHK world",
    "NHK",
    "CNBC Asia",
    "CNBC"
]

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def download_bb_m3u():
    """ä¸‹è½½BB.m3uå¹¶æå–EPG"""
    try:
        log("ä¸‹è½½BB.m3u...")
        response = requests.get(BB_URL, timeout=10)
        response.raise_for_status()
        
        bb_content = response.text
        log(f"âœ… BB.m3uä¸‹è½½æˆåŠŸ ({len(bb_content)} å­—ç¬¦)")
        
        # æå–EPGä¿¡æ¯
        epg_match = re.search(r'url-tvg="([^"]+)"', bb_content)
        epg_url = epg_match.group(1) if epg_match else None
        
        if epg_url:
            log(f"âœ… ä½¿ç”¨BBçš„EPG: {epg_url}")
        
        return bb_content, epg_url
        
    except Exception as e:
        log(f"âŒ BB.m3uä¸‹è½½å¤±è´¥: {e}")
        return None, None

def get_content_from_proxy():
    """ä»Cloudflareä»£ç†è·å–å†…å®¹"""
    try:
        log("ä»Cloudflareä»£ç†è·å–å†…å®¹...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://smart.946985.filegear-sg.me/'
        }
        
        response = requests.get(CLOUDFLARE_PROXY, headers=headers, timeout=15)
        
        if response.status_code == 200:
            content = response.text
            
            # å¦‚æœæ˜¯HTMLï¼Œå°è¯•æå–M3Uå†…å®¹
            if '<html' in content.lower():
                # æŸ¥æ‰¾M3Uå†…å®¹
                m3u_match = re.search(r'(#EXTM3U.*?)(?:</pre>|</code>|$)', content, re.DOTALL)
                if m3u_match:
                    content = m3u_match.group(1).strip()
                    log("âœ… ä»HTMLæå–åˆ°M3Uå†…å®¹")
                else:
                    # æå–æ‰€æœ‰å¯èƒ½çš„é¢‘é“è¡Œ
                    lines = content.split('\n')
                    m3u_lines = []
                    for line in lines:
                        line = line.strip()
                        if line.startswith('#EXTINF:') or ('://' in line and not line.startswith('<')):
                            m3u_lines.append(line)
                    
                    if m3u_lines:
                        content = '#EXTM3U\n' + '\n'.join(m3u_lines)
                        log(f"âœ… ä»HTMLæå–åˆ° {len(m3u_lines)} ä¸ªé¢‘é“è¡Œ")
            
            if content and content.strip():
                log(f"âœ… è·å–åˆ°å†…å®¹ ({len(content)} å­—ç¬¦)")
                return content
            else:
                log("âš ï¸  å†…å®¹ä¸ºç©º")
        else:
            log(f"âŒ ä»£ç†è¿”å›é”™è¯¯: {response.status_code}")
            
    except Exception as e:
        log(f"âŒ ä»£ç†è®¿é—®å¤±è´¥: {e}")
    
    return None

def extract_hk_channels(content):
    """æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHK"""
    if not content:
        return []
    
    log("æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHK...")
    
    # è§£æM3Uå†…å®¹
    lines = content.split('\n')
    channels = []
    current_extinf = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            current_extinf = line
        elif current_extinf and '://' in line and not line.startswith('#'):
            channels.append((current_extinf, line))
            current_extinf = None
    
    # è¿‡æ»¤JULIé¢‘é“
    hk_channels = []
    seen = set()
    
    for extinf, url in channels:
        if 'JULI' in extinf.upper():
            # é‡å‘½åä¸ºHKåˆ†ç»„
            new_extinf = re.sub(r'JULI', 'HK', extinf, flags=re.IGNORECASE)
            
            # ç¡®ä¿group-titleä¸ºHK
            if 'group-title=' in new_extinf:
                new_extinf = re.sub(r'group-title="[^"]*"', 'group-title="HK"', new_extinf)
            else:
                # æ·»åŠ group-title
                if ',' in new_extinf:
                    parts = new_extinf.split(',', 1)
                    new_extinf = f'{parts[0]} group-title="HK",{parts[1]}'
            
            # å»é‡
            key = f"{new_extinf}|{url}"
            if key not in seen:
                seen.add(key)
                hk_channels.append((new_extinf, url))
    
    log(f"âœ… æå–åˆ° {len(hk_channels)} ä¸ªHKé¢‘é“ï¼ˆåŸJULIï¼‰")
    
    if hk_channels:
        log("HKé¢‘é“ç¤ºä¾‹ï¼ˆæ’åœ¨TWä¹‹å‰ï¼‰:")
        for i, (extinf, url) in enumerate(hk_channels[:5]):
            name = extinf.split(',', 1)[1] if ',' in extinf else extinf
            log(f"  {i+1}. {name[:50]}...")
    
    return hk_channels

def should_skip_channel(channel_name):
    """æ£€æŸ¥é¢‘é“æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤"""
    channel_name_lower = channel_name.lower()
    
    # æ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
    for black_word in BLACKLIST_TW:
        if black_word.lower() in channel_name_lower:
            log(f"  è¿‡æ»¤æ‰: {channel_name} (åŒ…å«: {black_word})")
            return True
    
    return False

def extract_filtered_4gtv_channels(content, limit=30):
    """æå–4gtvé¢‘é“ï¼ˆå‰30ä¸ªï¼‰ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“"""
    if not content:
        return []
    
    log(f"æå–4gtvå‰{limit}ä¸ªç›´æ’­ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“...")
    log(f"è¿‡æ»¤åˆ—è¡¨: {', '.join(BLACKLIST_TW)}")
    
    # è§£æM3Uå†…å®¹
    lines = content.split('\n')
    channels = []
    current_extinf = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            current_extinf = line
        elif current_extinf and '://' in line and not line.startswith('#'):
            channels.append((current_extinf, line))
            current_extinf = None
    
    # è¿‡æ»¤4gtvé¢‘é“ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    filtered_channels = []
    for extinf, url in channels:
        if '4gtv' in extinf.lower():
            filtered_channels.append((extinf, url))
    
    log(f"æ‰¾åˆ° {len(filtered_channels)} ä¸ª4gtvé¢‘é“")
    
    # è¿‡æ»¤é»‘åå•é¢‘é“
    filtered_by_blacklist = []
    for extinf, url in filtered_channels:
        # æå–é¢‘é“å
        channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        if not should_skip_channel(channel_name):
            filtered_by_blacklist.append((extinf, url))
        else:
            log(f"  â›” è¿‡æ»¤: {channel_name}")
    
    log(f"è¿‡æ»¤åå‰©ä½™ {len(filtered_by_blacklist)} ä¸ª4gtvé¢‘é“")
    
    # åªå–å‰limitä¸ª
    if len(filtered_by_blacklist) > limit:
        filtered_by_blacklist = filtered_by_blacklist[:limit]
        log(f"åªå–å‰ {limit} ä¸ªè¿‡æ»¤åçš„4gtvé¢‘é“")
    
    # é‡å‘½åä¸ºTWåˆ†ç»„
    tw_channels = []
    seen = set()
    
    for extinf, url in filtered_by_blacklist:
        # æ›¿æ¢åˆ†ç»„ä¸ºTW
        new_extinf = extinf
        
        # æ›¿æ¢4gtvä¸ºTWï¼ˆåœ¨é¢‘é“åä¸­ï¼‰
        if '4gtv' in new_extinf.lower():
            new_extinf = re.sub(r'4gtv', 'TW', new_extinf, flags=re.IGNORECASE)
        
        # ç¡®ä¿group-titleä¸ºTW
        if 'group-title=' in new_extinf:
            new_extinf = re.sub(r'group-title="[^"]*"', 'group-title="TW"', new_extinf)
        else:
            # æ·»åŠ group-title
            if ',' in new_extinf:
                parts = new_extinf.split(',', 1)
                new_extinf = f'{parts[0]} group-title="TW",{parts[1]}'
        
        # å»é‡
        key = f"{new_extinf}|{url}"
        if key not in seen:
            seen.add(key)
            tw_channels.append((new_extinf, url))
    
    log(f"âœ… æå–åˆ° {len(tw_channels)} ä¸ªTWé¢‘é“ï¼ˆåŸ4gtvï¼Œå·²è¿‡æ»¤ï¼‰")
    
    # æ˜¾ç¤ºè¿‡æ»¤æ‰çš„é¢‘é“ç»Ÿè®¡
    filtered_count = len(filtered_channels) - len(tw_channels)
    if filtered_count > 0:
        log(f"â›” è¿‡æ»¤æ‰äº† {filtered_count} ä¸ªTWé¢‘é“")
    
    if tw_channels:
        log("TWé¢‘é“ç¤ºä¾‹ï¼ˆå·²è¿‡æ»¤æŒ‡å®šé¢‘é“ï¼‰:")
        for i, (extinf, url) in enumerate(tw_channels[:5]):
            name = extinf.split(',', 1)[1] if ',' in extinf else extinf
            log(f"  {i+1}. {name[:50]}...")
    
    return tw_channels

def main():
    """ä¸»å‡½æ•°"""
    log("å¼€å§‹åˆå¹¶M3Uæ–‡ä»¶...")
    
    # æ˜¾ç¤ºå½“å‰æ—¶é—´ï¼ˆç”¨äºè°ƒè¯•å®šæ—¶ä»»åŠ¡ï¼‰
    current_time = datetime.now()
    log(f"å½“å‰æ—¶é—´: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
    log(f"ä¸‹æ¬¡è¿è¡Œ: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 18:00")
    log(f"TWé¢‘é“è¿‡æ»¤åˆ—è¡¨: {', '.join(BLACKLIST_TW)}")
    
    # 1. ä¸‹è½½BB.m3uå¹¶è·å–EPG
    bb_content, epg_url = download_bb_m3u()
    if not bb_content:
        log("âŒ æ— æ³•ç»§ç»­ï¼ŒBB.m3uä¸‹è½½å¤±è´¥")
        return
    
    # 2. ä»ä»£ç†è·å–å†…å®¹
    proxy_content = get_content_from_proxy()
    
    # 3. å…ˆæå–HKé¢‘é“ï¼ˆJULIï¼‰- æ’åœ¨å‰é¢
    hk_channels = []
    if proxy_content:
        hk_channels = extract_hk_channels(proxy_content)
    else:
        log("âš ï¸  æ— æ³•ä»ä»£ç†è·å–å†…å®¹ï¼Œè·³è¿‡HKé¢‘é“")
    
    # 4. å†æå–TWé¢‘é“ï¼ˆ4gtvå‰30ä¸ªï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“ï¼‰- æ’åœ¨åé¢
    tw_channels = []
    if proxy_content:
        tw_channels = extract_filtered_4gtv_channels(proxy_content, limit=30)
    else:
        log("âš ï¸  æ— æ³•ä»ä»£ç†è·å–å†…å®¹ï¼Œè·³è¿‡TWé¢‘é“")
    
    # 5. æ„å»ºM3Uå†…å®¹
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # M3Uå¤´éƒ¨ï¼ˆä½¿ç”¨BBçš„EPGï¼‰
    if epg_url:
        m3u_header = f'#EXTM3U url-tvg="{epg_url}"\n'
    else:
        m3u_header = '#EXTM3U\n'
    
    output = m3u_header + f"""# è‡ªåŠ¨åˆå¹¶ M3U æ–‡ä»¶
# ç”Ÿæˆæ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# ä¸‹æ¬¡æ›´æ–°: æ¯å¤© 06:00 å’Œ 18:00 (åŒ—äº¬æ—¶é—´)
# BBæº: {BB_URL}
# ä»£ç†æº: {CLOUDFLARE_PROXY}
# JULIåˆ†ç»„å·²æ”¹ä¸ºHK (æ’åœ¨å‰é¢)
# 4gtvåˆ†ç»„å·²æ”¹ä¸ºTW (å‰30ä¸ªï¼Œæ’åœ¨åé¢ï¼Œå·²è¿‡æ»¤æŒ‡å®šé¢‘é“)
# è¿‡æ»¤é¢‘é“: {', '.join(BLACKLIST_TW)}
# EPG: {epg_url if epg_url else 'BBçš„XML'}
# GitHub Actions è‡ªåŠ¨ç”Ÿæˆ

"""
    
    # æ·»åŠ BBå†…å®¹ï¼ˆè·³è¿‡ç¬¬ä¸€è¡Œï¼‰
    bb_lines = bb_content.split('\n')
    bb_count = 0
    skip_first = True
    
    for line in bb_lines:
        line = line.rstrip()
        if not line:
            continue
        
        if skip_first and line.startswith('#EXTM3U'):
            skip_first = False
            continue
        
        output += line + '\n'
        if line.startswith('#EXTINF:'):
            bb_count += 1
    
    # æ·»åŠ HKé¢‘é“ï¼ˆJULIï¼‰- æ’åœ¨å‰é¢
    if hk_channels:
        output += f"\n# HKé¢‘é“ (åŸJULIï¼Œæ’åœ¨TWä¹‹å‰)\n"
        for extinf, url in hk_channels:
            output += extinf + '\n'
            output += url + '\n'
    
    # æ·»åŠ TWé¢‘é“ï¼ˆ4gtvï¼‰- æ’åœ¨åé¢ï¼ˆå·²è¿‡æ»¤ï¼‰
    if tw_channels:
        output += f"\n# TWé¢‘é“ (åŸ4gtvï¼Œå‰30ä¸ªï¼Œå·²è¿‡æ»¤æŒ‡å®šé¢‘é“ï¼Œæ’åœ¨HKä¹‹å)\n"
        output += f"# å·²è¿‡æ»¤: {', '.join(BLACKLIST_TW)}\n"
        for extinf, url in tw_channels:
            output += extinf + '\n'
            output += url + '\n'
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    output += f"""
# ç»Ÿè®¡ä¿¡æ¯
# BB é¢‘é“æ•°: {bb_count}
# HK é¢‘é“æ•°: {len(hk_channels)} (åŸJULIï¼Œæ’åœ¨å‰)
# TW é¢‘é“æ•°: {len(tw_channels)} (åŸ4gtvå‰30ä¸ªï¼Œå·²è¿‡æ»¤ï¼Œæ’åœ¨å)
# è¿‡æ»¤é¢‘é“: {len(BLACKLIST_TW)} ä¸ª
# æ€»é¢‘é“æ•°: {bb_count + len(hk_channels) + len(tw_channels)}
# æ›´æ–°æ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# æ›´æ–°é¢‘ç‡: æ¯å¤© 06:00 å’Œ 18:00 (åŒ—äº¬æ—¶é—´)
# æ’åºè§„åˆ™: BB â†’ HK â†’ TW
"""
    
    # 6. ä¿å­˜æ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(output)
    
    log(f"\nğŸ‰ åˆå¹¶å®Œæˆ!")
    log(f"ğŸ“ æ–‡ä»¶: {OUTPUT_FILE}")
    log(f"ğŸ“ å¤§å°: {len(output)} å­—ç¬¦")
    log(f"ğŸ“¡ EPG: {epg_url}")
    log(f"ğŸ“º BBé¢‘é“: {bb_count}")
    log(f"ğŸ“º HKé¢‘é“: {len(hk_channels)} (JULIï¼Œæ’åœ¨å‰)")
    log(f"ğŸ“º TWé¢‘é“: {len(tw_channels)} (4gtvå‰30ä¸ªï¼Œå·²è¿‡æ»¤{len(BLACKLIST_TW)}ä¸ªé¢‘é“ï¼Œæ’åœ¨å)")
    log(f"ğŸ“º æ€»è®¡: {bb_count + len(hk_channels) + len(tw_channels)}")
    log(f"ğŸ•’ ä¸‹æ¬¡è‡ªåŠ¨æ›´æ–°: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 18:00")
    log(f"â›” TWè¿‡æ»¤åˆ—è¡¨: {', '.join(BLACKLIST_TW)}")

if __name__ == "__main__":
    main()
