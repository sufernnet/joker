#!/usr/bin/env python3
"""
M3Uæ–‡ä»¶åˆå¹¶è„šæœ¬ - å¢å¼ºEPGæ”¯æŒ
1. ä¸‹è½½BB.m3uï¼ˆåŒ…å«EPGä¿¡æ¯ï¼‰
2. ä»Cloudflareä»£ç†è·å–å†…å®¹
3. æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHKï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—
4. æå–4gtvå‰30ä¸ªç›´æ’­ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“
5. åˆå¹¶ç”ŸæˆCC.m3uï¼ŒåŒ…å«å¤šä¸ªEPGæº
åŒ—äº¬æ—¶é—´æ¯å¤©6:00ã€17:00è‡ªåŠ¨è¿è¡Œ
"""

import requests
import re
import os
import time
from datetime import datetime

# é…ç½®
BB_URL = "https://raw.githubusercontent.com/sufernnet/joker/main/BB.m3u"
CLOUDFLARE_PROXY = "https://smt-proxy.sufern001.workers.dev/"
OUTPUT_FILE = "CC.m3u"

# éœ€è¦è¿‡æ»¤æ‰çš„TWé¢‘é“å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
BLACKLIST_TW = [
    "Bloomberg TV",
    "Bloomberg",
    "SBNå…¨çƒè´¢ç»å°",
    "SBNè´¢ç»",
    "FRANCE24è‹±æ–‡å°",
    "FRANCE24",
    "åŠå²›å›½é™…æ–°é—»å°",
    "åŠå³¶å›½é™…",
    "NHK world-japan",
    "NHK world",
    "SBN",
    "æ—¥æœ¬",
    "NHK",
    "CNBC Asia",
    "CNBC"
]

# HKé¢‘é“ä¼˜å…ˆé¡ºåºï¼ˆæŒ‰è¿™ä¸ªé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢ï¼‰
HK_PRIORITY_ORDER = [
    "å‡¤å‡°ä¸­æ–‡",
    "å‡¤å‡°èµ„è®¯", 
    "å‡¤å‡°é¦™æ¸¯",
    "NOWæ–°é—»å°",
    "NOWæ˜Ÿå½±",
    "NOWçˆ†è°·"
]

# å¤‡é€‰EPGæºï¼ˆå¦‚æœä¸»è¦EPGå¤±æ•ˆï¼‰
BACKUP_EPG_URLS = [
    "https://epg.112114.xyz/pp.xml",  # BBçš„EPG
    "https://epg.946985.filegear-sg.me/t.xml.gz",  # JULIçš„EPG
    "https://epg.112114.xyz/pp.xml",
    "http://epg.51zmt.top:8000/e.xml"
]

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def test_epg_url(epg_url):
    """æµ‹è¯•EPG URLæ˜¯å¦å¯è®¿é—®"""
    try:
        log(f"æµ‹è¯•EPG: {epg_url}")
        headers = {
            'User-Agent': 'Mozilla/5.0',
            'Accept': '*/*'
        }
        
        response = requests.get(epg_url, headers=headers, timeout=10, stream=True)
        
        if response.status_code == 200:
            content_type = response.headers.get('content-type', '').lower()
            chunk = response.raw.read(1024)
            text = chunk.decode('utf-8', errors='ignore')
            
            if '<?xml' in text or '<tv' in text or '<programme' in text:
                log(f"âœ… EPGå¯ç”¨: {epg_url}")
                return True
            else:
                log(f"âš ï¸  EPGä¸æ˜¯XMLæ ¼å¼: {epg_url}")
                return False
        else:
            log(f"âŒ EPGä¸å¯è®¿é—®: {epg_url} (çŠ¶æ€ç : {response.status_code})")
            return False
            
    except Exception as e:
        log(f"âŒ EPGæµ‹è¯•å¤±è´¥ {epg_url}: {e}")
        return False

def get_best_epg_url(epg_urls):
    """è·å–æœ€ä½³çš„EPG URL"""
    log("å¯»æ‰¾æœ€ä½³EPGæº...")
    working_epgs = []
    
    for epg_url in epg_urls:
        if test_epg_url(epg_url):
            working_epgs.append(epg_url)
    
    if working_epgs:
        best_epg = working_epgs[0]
        log(f"âœ… é€‰æ‹©EPG: {best_epg}")
        log(f"   å…¶ä»–å¯ç”¨EPG: {len(working_epgs)-1}ä¸ª")
        return best_epg
    else:
        log("âš ï¸  æ²¡æœ‰å¯ç”¨çš„EPGæº")
        return None

def download_bb_m3u():
    """ä¸‹è½½BB.m3uå¹¶æå–EPG"""
    try:
        log("ä¸‹è½½BB.m3u...")
        response = requests.get(BB_URL, timeout=10)
        response.raise_for_status()
        bb_content = response.text
        log(f"âœ… BB.m3uä¸‹è½½æˆåŠŸ ({len(bb_content)} å­—ç¬¦)")
        return bb_content
    except Exception as e:
        log(f"âŒ BB.m3uä¸‹è½½å¤±è´¥: {e}")
        return None

def get_content_from_proxy():
    """ä»Cloudflareä»£ç†è·å–å†…å®¹"""
    try:
        log("ä»Cloudflareä»£ç†è·å–å†…å®¹...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://smart.946985.filegear-sg.me/'
        }
        
        response = requests.get(CLOUDFLARE_PROXY, headers=headers, timeout=15)
        
        if response.status_code == 200:
            content = response.text
            if '<html' in content.lower():
                m3u_match = re.search(r'(#EXTM3U.*?)(?:</pre>|</code>|$)', content, re.DOTALL)
                if m3u_match:
                    content = m3u_match.group(1).strip()
                    log("âœ… ä»HTMLæå–åˆ°M3Uå†…å®¹")
            if content.strip():
                log(f"âœ… è·å–åˆ°å†…å®¹ ({len(content)} å­—ç¬¦)")
                return content
        else:
            log(f"âŒ ä»£ç†è¿”å›é”™è¯¯: {response.status_code}")
    except Exception as e:
        log(f"âŒ ä»£ç†è®¿é—®å¤±è´¥: {e}")
    return None

def main():
    log("å¼€å§‹åˆå¹¶M3Uæ–‡ä»¶...")
    log(f"å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    log(f"ä¸‹æ¬¡è¿è¡Œ: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 17:00")

    bb_content = download_bb_m3u()
    if not bb_content:
        return

    proxy_content = get_content_from_proxy()

    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    output = f"""#EXTM3U
# è‡ªåŠ¨åˆå¹¶ M3U æ–‡ä»¶
# ç”Ÿæˆæ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# ä¸‹æ¬¡æ›´æ–°: æ¯å¤© 06:00 å’Œ 17:00 (åŒ—äº¬æ—¶é—´)
# GitHub Actions è‡ªåŠ¨ç”Ÿæˆ
"""

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(output)

    log("ğŸ‰ åˆå¹¶å®Œæˆ!")
    log("ğŸ•’ ä¸‹æ¬¡è‡ªåŠ¨æ›´æ–°: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 17:00")

if __name__ == "__main__":
    main()
