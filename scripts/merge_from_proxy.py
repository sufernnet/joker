# scripts/merge_from_proxy.py
#!/usr/bin/env python3
"""
ä» Cloudflare ä»£ç†é¡µé¢æå–å¹¶åˆå¹¶ M3U
ä»£ç†åœ°å€: https://smt-proxy.sufern001.workers.dev/
"""

import requests
import re
import os
from datetime import datetime

# é…ç½®
PROXY_URL = "https://smt-proxy.sufern001.workers.dev/"
BB_URL = "https://raw.githubusercontent.com/sufernnet/joker/main/BB.m3u"

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def download_bb_m3u():
    """ä¸‹è½½BB.m3u"""
    try:
        log("ä¸‹è½½ BB.m3u...")
        response = requests.get(BB_URL, timeout=10)
        response.raise_for_status()
        log(f"âœ… BB.m3u ä¸‹è½½æˆåŠŸ ({len(response.text)} å­—ç¬¦)")
        return response.text
    except Exception as e:
        log(f"âŒ BB.m3u ä¸‹è½½å¤±è´¥: {e}")
        return ""

def extract_m3u_from_proxy():
    """ä»ä»£ç†é¡µé¢æå–M3Uå†…å®¹"""
    log("ä»ä»£ç†é¡µé¢æå–å†…å®¹...")
    
    try:
        # è·å–ä»£ç†é¡µé¢
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(PROXY_URL, headers=headers, timeout=15)
        response.raise_for_status()
        
        html_content = response.text
        log(f"ä»£ç†é¡µé¢è·å–æˆåŠŸ ({len(html_content)} å­—ç¬¦)")
        
        # æ–¹æ³•1ï¼šç›´æ¥æŸ¥æ‰¾M3Ué“¾æ¥ï¼ˆå¦‚æœé¡µé¢æœ‰ç›´æ¥é“¾æ¥ï¼‰
        m3u_links = re.findall(r'https?://[^\s"\']+\.m3u(?:\?[^\s"\']*)?', html_content, re.IGNORECASE)
        
        if m3u_links:
            log(f"æ‰¾åˆ° {len(m3u_links)} ä¸ªM3Ué“¾æ¥")
            for link in m3u_links:
                log(f"  - {link}")
            
            # å°è¯•ä¸‹è½½ç¬¬ä¸€ä¸ªM3Ué“¾æ¥
            try:
                m3u_response = requests.get(m3u_links[0], timeout=10)
                if m3u_response.status_code == 200:
                    log(f"âœ… æˆåŠŸä¸‹è½½M3Uæ–‡ä»¶ ({len(m3u_response.text)} å­—ç¬¦)")
                    return m3u_response.text
            except Exception as e:
                log(f"ä¸‹è½½M3Ué“¾æ¥å¤±è´¥: {e}")
        
        # æ–¹æ³•2ï¼šå¦‚æœé¡µé¢ç›´æ¥åŒ…å«M3Uå†…å®¹ï¼ˆå¯èƒ½åœ¨<pre>æ ‡ç­¾æˆ–æ–‡æœ¬ä¸­ï¼‰
        log("å°è¯•ç›´æ¥æå–M3Uå†…å®¹...")
        
        # æŸ¥æ‰¾å¯èƒ½åŒ…å«M3Uå†…å®¹çš„åŒºåŸŸ
        patterns = [
            r'(#EXTM3U.*?)(?:</pre>|</code>|</textarea>|$)',
            r'<pre[^>]*>(.*?#EXTM3U.*?)</pre>',
            r'<code[^>]*>(.*?#EXTM3U.*?)</code>',
            r'<textarea[^>]*>(.*?#EXTM3U.*?)</textarea>',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html_content, re.DOTALL | re.IGNORECASE)
            if matches:
                log(f"æ‰¾åˆ°æ¨¡å¼åŒ¹é…: {len(matches)} å¤„")
                for i, match in enumerate(matches[:2]):
                    content = match.strip()
                    if content.startswith('#EXTM3U'):
                        log(f"âœ… æ‰¾åˆ°æœ‰æ•ˆçš„M3Uå†…å®¹ ({len(content)} å­—ç¬¦)")
                        return content
        
        # æ–¹æ³•3ï¼šå¦‚æœé¡µé¢æ˜¯çº¯æ–‡æœ¬æ ¼å¼çš„M3U
        if html_content.startswith('#EXTM3U'):
            log(f"âœ… é¡µé¢æœ¬èº«å°±æ˜¯M3Uæ–‡ä»¶ ({len(html_content)} å­—ç¬¦)")
            return html_content
        
        # æ–¹æ³•4ï¼šæå–æ‰€æœ‰å¯èƒ½åŒ…å«é¢‘é“ä¿¡æ¯çš„è¡Œ
        log("å°è¯•æå–é¢‘é“ä¿¡æ¯è¡Œ...")
        lines = html_content.split('\n')
        m3u_content = []
        
        for line in lines:
            line = line.strip()
            if line.startswith('#EXTINF:') or ('://' in line and not line.startswith('<')):
                m3u_content.append(line)
        
        if m3u_content:
            log(f"âœ… æå–åˆ° {len(m3u_content)} è¡Œé¢‘é“ä¿¡æ¯")
            return '#EXTM3U\n' + '\n'.join(m3u_content)
        
        log("âŒ æ— æ³•ä»é¡µé¢æå–M3Uå†…å®¹")
        log("é¡µé¢å¼€å¤´1000å­—ç¬¦:")
        print(html_content[:1000])
        
        # ä¿å­˜HTMLä¾›è°ƒè¯•
        with open("proxy_debug.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        
        return ""
        
    except Exception as e:
        log(f"âŒ ä»ä»£ç†æå–å¤±è´¥: {e}")
        return ""

def extract_juli_channels(m3u_content):
    """ä»M3Uå†…å®¹ä¸­æå–JULIé¢‘é“"""
    if not m3u_content:
        return []
    
    log("æå–JULIé¢‘é“...")
    lines = m3u_content.split('\n')
    channels = []
    current_extinf = None
    
    for i in range(len(lines)):
        line = lines[i].strip()
        
        # å¯»æ‰¾JULIé¢‘é“
        if 'JULI' in line.upper():
            # å¦‚æœæ˜¯EXTINFè¡Œ
            if line.startswith('#EXTINF:'):
                current_extinf = line
                # æŸ¥æ‰¾å¯¹åº”çš„URL
                for j in range(i+1, min(i+3, len(lines))):
                    next_line = lines[j].strip()
                    if next_line and '://' in next_line and not next_line.startswith('#'):
                        channels.append((current_extinf, next_line))
                        break
            
            # å¦‚æœåœ¨å…¶ä»–è¡Œæ‰¾åˆ°JULIï¼Œå‘å‰æ‰¾EXTINF
            elif i > 0:
                for j in range(max(0, i-3), i):
                    if lines[j].startswith('#EXTINF:'):
                        current_extinf = lines[j]
                        # æŸ¥æ‰¾URL
                        for k in range(i, min(i+3, len(lines))):
                            url_line = lines[k].strip()
                            if url_line and '://' in url_line and not url_line.startswith('#'):
                                channels.append((current_extinf, url_line))
                                break
                        break
    
    # å»é‡
    unique_channels = []
    seen = set()
    for extinf, url in channels:
        key = f"{extinf}|{url}"
        if key not in seen:
            seen.add(key)
            unique_channels.append((extinf, url))
    
    log(f"âœ… æå–åˆ° {len(unique_channels)} ä¸ªJULIé¢‘é“")
    
    # æ˜¾ç¤ºéƒ¨åˆ†é¢‘é“
    for i, (extinf, url) in enumerate(unique_channels[:5]):
        channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
        log(f"  {i+1}. {channel_name[:50]}...")
    
    return unique_channels

def main():
    """ä¸»å‡½æ•°"""
    log("å¼€å§‹åˆå¹¶M3Uæ–‡ä»¶...")
    
    # 1. ä¸‹è½½BB.m3u
    bb_content = download_bb_m3u()
    if not bb_content:
        log("âŒ æ— æ³•ç»§ç»­ï¼ŒBB.m3uä¸‹è½½å¤±è´¥")
        return
    
    # 2. ä»ä»£ç†è·å–JULIå†…å®¹
    proxy_content = extract_m3u_from_proxy()
    if not proxy_content:
        log("âš ï¸  æ— æ³•ä»ä»£ç†è·å–å†…å®¹ï¼Œåªä½¿ç”¨BB.m3u")
        juli_channels = []
    else:
        # 3. æå–JULIé¢‘é“
        juli_channels = extract_juli_channels(proxy_content)
    
    # 4. åˆå¹¶å†…å®¹
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    output = f"""#EXTM3U
# è‡ªåŠ¨åˆå¹¶ M3U æ–‡ä»¶
# ç”Ÿæˆæ—¶é—´: {timestamp}
# ä»£ç†æº: {PROXY_URL}
# GitHub Actions è‡ªåŠ¨ç”Ÿæˆ

"""
    
    # æ·»åŠ BBå†…å®¹ï¼ˆè·³è¿‡å¼€å¤´çš„#EXTM3Uï¼‰
    bb_lines = bb_content.split('\n')
    bb_count = 0
    for line in bb_lines:
        if line.strip() and not line.startswith('#EXTM3U'):
            output += line + '\n'
            if line.startswith('#EXTINF:'):
                bb_count += 1
    
    # æ·»åŠ JULIé¢‘é“
    if juli_channels:
        output += f"\n# JULI é¢‘é“ (ä»ä»£ç†æå–)\n"
        for extinf, url in juli_channels:
            output += extinf + '\n'
            output += url + '\n'
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    output += f"""
# ç»Ÿè®¡ä¿¡æ¯
# BB é¢‘é“æ•°: {bb_count}
# JULI é¢‘é“æ•°: {len(juli_channels)}
# æ€»é¢‘é“æ•°: {bb_count + len(juli_channels)}
# æ›´æ–°æ—¶é—´: {timestamp}
"""
    
    # 5. ä¿å­˜æ–‡ä»¶
    with open("CC.m3u", "w", encoding="utf-8") as f:
        f.write(output)
    
    log(f"\nğŸ‰ åˆå¹¶å®Œæˆ!")
    log(f"ğŸ“ æ–‡ä»¶: CC.m3u")
    log(f"ğŸ“ å¤§å°: {len(output)} å­—ç¬¦")
    log(f"ğŸ“º BBé¢‘é“: {bb_count}")
    log(f"ğŸ“º JULIé¢‘é“: {len(juli_channels)}")
    log(f"ğŸ“º æ€»è®¡: {bb_count + len(juli_channels)}")

if __name__ == "__main__":
    main()
