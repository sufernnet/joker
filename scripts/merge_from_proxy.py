#!/usr/bin/env python3
"""
M3Uæ–‡ä»¶åˆå¹¶è„šæœ¬ - å®Œæ•´EPGè§£å†³æ–¹æ¡ˆ
1. ä¸‹è½½BB.m3uï¼ˆåŒ…å«EPGä¿¡æ¯ï¼‰
2. ä»Cloudflareä»£ç†è·å–å†…å®¹
3. æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHKï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—
4. æå–4gtvå‰30ä¸ªç›´æ’­ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“
5. ä¸ºHK/TWé¢‘é“æ·»åŠ tvg-idï¼Œç¡®ä¿EPGåŒ¹é…
6. åˆå¹¶ç”ŸæˆCC.m3uï¼ŒåŒ…å«å¤šä¸ªEPGæº
åŒ—äº¬æ—¶é—´æ¯å¤©6:00ã€18:00è‡ªåŠ¨è¿è¡Œ
"""

import requests
import re
import os
import time
import hashlib
from datetime import datetime
from urllib.parse import urlparse, parse_qs

# é…ç½®
BB_URL = "https://raw.githubusercontent.com/sufernnet/joker/main/BB.m3u"
CLOUDFLARE_PROXY = "https://smt-proxy.sufern001.workers.dev/"
OUTPUT_FILE = "CC.m3u"

# éœ€è¦è¿‡æ»¤æ‰çš„TWé¢‘é“å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
BLACKLIST_TW = [
    "Bloomberg TV",
    "Bloomberg",
    "SBNå…¨çƒè´¢ç»å°",
    "SBNè´¢ç»",
    "FRANCE24è‹±æ–‡å°",
    "FRANCE24",
    "åŠå²›å›½é™…æ–°é—»å°",
    "åŠå²›å›½é™…",
    "NHK world-japan",
    "NHK world",
    "NHK",
    "åŠå³¶",
    "æ—¥æœ¬",
    "SBN",
    "CNBC Asia",
    "CNBC"
]

# HKé¢‘é“ä¼˜å…ˆé¡ºåºï¼ˆæŒ‰è¿™ä¸ªé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢ï¼‰
HK_PRIORITY_ORDER = [
    "å‡¤å‡°ä¸­æ–‡",
    "å‡¤å‡°èµ„è®¯", 
    "å‡¤å‡°é¦™æ¸¯",
    "NOWæ–°é—»å°",
    "NOWæ˜Ÿå½±",
    "NOWçˆ†è°·"
]

# EPGé¢‘é“IDæ˜ å°„ï¼ˆæ‰‹åŠ¨åŒ¹é…ï¼‰
CHANNEL_ID_MAPPING = {
    # å‡¤å‡°ç³»åˆ—
    "å‡¤å‡°ä¸­æ–‡": "å‡¤å‡°ä¸­æ–‡",
    "å‡¤å‡°èµ„è®¯": "å‡¤å‡°èµ„è®¯", 
    "å‡¤å‡°é¦™æ¸¯": "å‡¤å‡°é¦™æ¸¯",
    
    # NOWç³»åˆ—
    "NOWæ–°é—»å°": "NOWæ–°é—»å°",
    "NOWæ˜Ÿå½±": "NOWæ˜Ÿå½±",
    "NOWçˆ†è°·": "NOWçˆ†è°·",
    
    # å¸¸è§HKé¢‘é“
    "TVBæ–°é—»": "TVBæ–°é—»",
    "TVBè´¢ç»": "TVBè´¢ç»",
    "æœ‰çº¿æ–°é—»": "æœ‰çº¿æ–°é—»",
    "é¦™æ¸¯å¼€ç”µè§†": "é¦™æ¸¯å¼€ç”µè§†",
    "VIU TV": "VIU TV",
    
    # å¸¸è§TWé¢‘é“
    "æ°‘è§†": "æ°‘è§†",
    "ä¸­è§†": "ä¸­è§†",
    "åè§†": "åè§†",
    "å°è§†": "å°è§†",
    "ä¸‰ç«‹": "ä¸‰ç«‹",
    "ä¸œæ£®": "ä¸œæ£®",
    "TVBS": "TVBS",
    "ä¸­å¤©": "ä¸­å¤©",
    "å¯°å®‡": "å¯°å®‡",
    "éå‡¡": "éå‡¡",
}

# å¤‡é€‰EPGæºï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
EPG_SOURCES = [
    "https://epg.112114.xyz/pp.xml",  # ä¸»è¦EPG
    "https://epg.112114.xyz/pp.xml",
    "http://epg.51zmt.top:8000/e.xml",  # å¤‡ç”¨EPG
    "https://epg.112114.xyz/pp.xml",
]

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def generate_tvg_id(channel_name, channel_url):
    """ä¸ºé¢‘é“ç”Ÿæˆç¨³å®šçš„tvg-id"""
    # æ¸…ç†é¢‘é“å
    clean_name = re.sub(r'[^\w\u4e00-\u9fff]', '', channel_name)
    
    # æ–¹æ³•1ï¼šä½¿ç”¨é¢„å®šä¹‰æ˜ å°„
    for key, value in CHANNEL_ID_MAPPING.items():
        if key in channel_name:
            return value
    
    # æ–¹æ³•2ï¼šä»URLæå–ID
    if channel_url:
        parsed = urlparse(channel_url)
        if parsed.query:
            params = parse_qs(parsed.query)
            for key in ['id', 'channel', 'ch', 'freq']:
                if key in params:
                    return params[key][0]
        
        # ä»è·¯å¾„æå–
        path_parts = parsed.path.split('/')
        if len(path_parts) > 1:
            last_part = path_parts[-1].split('.')[0]
            if last_part and len(last_part) > 3:
                return last_part
    
    # æ–¹æ³•3ï¼šä½¿ç”¨å“ˆå¸Œç”Ÿæˆç¨³å®šID
    hash_input = f"{clean_name}_{channel_url}"
    short_hash = hashlib.md5(hash_input.encode()).hexdigest()[:8]
    
    # æ–¹æ³•4ï¼šä½¿ç”¨é¢‘é“åå…³é”®è¯
    for keyword in ["å‡¤å‡°", "NOW", "TVB", "æœ‰çº¿", "æ°‘è§†", "ä¸­è§†", "åè§†", "å°è§†", "ä¸‰ç«‹", "ä¸œæ£®"]:
        if keyword in channel_name:
            return f"{keyword}_{short_hash[:4]}"
    
    return f"CH_{short_hash}"

def enhance_extinf_with_epg(extinf_line, channel_name, channel_url, epg_source):
    """å¢å¼ºEXTINFè¡Œï¼Œæ·»åŠ EPGä¿¡æ¯"""
    # ç”Ÿæˆtvg-id
    tvg_id = generate_tvg_id(channel_name, channel_url)
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰tvg-id
    if 'tvg-id=' in extinf_line:
        # ä¿ç•™åŸæœ‰çš„tvg-id
        return extinf_line
    
    # æå–åŸæœ‰å±æ€§
    attributes_match = re.search(r'^#EXTINF:(-?\d+)(.*?),(.+)$', extinf_line)
    if not attributes_match:
        return extinf_line
    
    duration = attributes_match.group(1)
    attributes = attributes_match.group(2).strip()
    display_name = attributes_match.group(3)
    
    # æ„å»ºæ–°çš„å±æ€§
    new_attributes = f' tvg-id="{tvg_id}" tvg-name="{channel_name}"'
    
    # å¦‚æœå·²æœ‰group-titleï¼Œä¿ç•™ï¼›å¦åˆ™æ·»åŠ 
    if 'group-title=' not in attributes:
        # æ ¹æ®é¢‘é“ååˆ¤æ–­åˆ†ç»„
        if any(hk in channel_name for hk in ["å‡¤å‡°", "NOW", "TVB", "æœ‰çº¿", "VIU"]):
            new_attributes += ' group-title="HK"'
        elif any(tw in channel_name for tw in ["æ°‘è§†", "ä¸­è§†", "åè§†", "å°è§†", "ä¸‰ç«‹", "ä¸œæ£®", "TVBS"]):
            new_attributes += ' group-title="TW"'
    
    # ç»„åˆæ–°çš„EXTINFè¡Œ
    if attributes:
        new_extinf = f'#EXTINF:{duration}{attributes}{new_attributes},{display_name}'
    else:
        new_extinf = f'#EXTINF:{duration}{new_attributes},{display_name}'
    
    return new_extinf

def test_epg_coverage(epg_url, channels):
    """æµ‹è¯•EPGå¯¹é¢‘é“çš„è¦†ç›–ç‡"""
    try:
        log(f"æµ‹è¯•EPGè¦†ç›–ç‡: {epg_url}")
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        # åªä¸‹è½½éƒ¨åˆ†å†…å®¹æ£€æŸ¥
        response = requests.get(epg_url, headers=headers, timeout=15, stream=True)
        
        if response.status_code != 200:
            return 0
        
        # è¯»å–å‰50KBæ£€æŸ¥
        content = b""
        for chunk in response.iter_content(chunk_size=1024):
            content += chunk
            if len(content) > 50000:  # 50KB
                break
        
        epg_content = content.decode('utf-8', errors='ignore')
        
        # ç»Ÿè®¡åŒ¹é…çš„é¢‘é“
        matched_channels = 0
        total_channels = len(channels)
        
        for channel_name, _ in channels[:20]:  # åªæ£€æŸ¥å‰20ä¸ªé¢‘é“
            # ç®€åŒ–é¢‘é“åç”¨äºåŒ¹é…
            simple_name = re.sub(r'[^\w\u4e00-\u9fff]', '', channel_name)
            
            # æ£€æŸ¥EPGä¸­æ˜¯å¦æœ‰è¿™ä¸ªé¢‘é“
            if simple_name in epg_content:
                matched_channels += 1
        
        coverage = (matched_channels / min(20, total_channels)) * 100 if total_channels > 0 else 0
        
        log(f"  EPGè¦†ç›–ç‡: {coverage:.1f}% ({matched_channels}/{min(20, total_channels)})")
        return coverage
        
    except Exception as e:
        log(f"  EPGè¦†ç›–ç‡æµ‹è¯•å¤±è´¥: {e}")
        return 0

def get_best_epg_for_channels(channels):
    """ä¸ºé¢‘é“é€‰æ‹©æœ€ä½³çš„EPGæº"""
    log(f"ä¸º {len(channels)} ä¸ªé¢‘é“é€‰æ‹©æœ€ä½³EPG...")
    
    best_epg = None
    best_coverage = 0
    
    for epg_url in EPG_SOURCES:
        coverage = test_epg_coverage(epg_url, channels)
        if coverage > best_coverage:
            best_coverage = coverage
            best_epg = epg_url
    
    if best_epg and best_coverage > 30:  # è¦†ç›–ç‡è‡³å°‘30%
        log(f"âœ… é€‰æ‹©EPG: {best_epg} (è¦†ç›–ç‡: {best_coverage:.1f}%)")
        return best_epg
    else:
        log(f"âš ï¸  æ²¡æœ‰åˆé€‚çš„EPGæº (æœ€ä½³è¦†ç›–ç‡: {best_coverage:.1f}%)")
        return EPG_SOURCES[0]  # è¿”å›ç¬¬ä¸€ä¸ªä½œä¸ºé»˜è®¤

def download_bb_m3u():
    """ä¸‹è½½BB.m3u"""
    try:
        log("ä¸‹è½½BB.m3u...")
        response = requests.get(BB_URL, timeout=10)
        response.raise_for_status()
        
        bb_content = response.text
        log(f"âœ… BB.m3uä¸‹è½½æˆåŠŸ ({len(bb_content)} å­—ç¬¦)")
        
        return bb_content
        
    except Exception as e:
        log(f"âŒ BB.m3uä¸‹è½½å¤±è´¥: {e}")
        return None

def get_content_from_proxy():
    """ä»Cloudflareä»£ç†è·å–å†…å®¹"""
    try:
        log("ä»Cloudflareä»£ç†è·å–å†…å®¹...")
        headers = {
            'User-Agent': 'Mozilla/5.0',
            'Accept': '*/*',
            'Referer': 'https://smart.946985.filegear-sg.me/'
        }
        
        response = requests.get(CLOUDFLARE_PROXY, headers=headers, timeout=15)
        
        if response.status_code == 200:
            content = response.text
            
            # å¦‚æœæ˜¯HTMLï¼Œå°è¯•æå–M3Uå†…å®¹
            if '<html' in content.lower():
                m3u_match = re.search(r'(#EXTM3U.*?)(?:</pre>|</code>|$)', content, re.DOTALL)
                if m3u_match:
                    content = m3u_match.group(1).strip()
                    log("âœ… ä»HTMLæå–åˆ°M3Uå†…å®¹")
            
            if content and content.strip():
                log(f"âœ… è·å–åˆ°å†…å®¹ ({len(content)} å­—ç¬¦)")
                return content
        else:
            log(f"âŒ ä»£ç†è¿”å›é”™è¯¯: {response.status_code}")
            
    except Exception as e:
        log(f"âŒ ä»£ç†è®¿é—®å¤±è´¥: {e}")
    
    return None

def extract_and_enhance_hk_channels(content, epg_url):
    """æå–å¹¶å¢å¼ºHKé¢‘é“ï¼ˆæ·»åŠ EPGä¿¡æ¯ï¼‰"""
    if not content:
        return []
    
    log("æå–å¹¶å¢å¼ºHKé¢‘é“...")
    
    # è§£æM3Uå†…å®¹
    lines = content.split('\n')
    channels = []
    current_extinf = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            current_extinf = line
        elif current_extinf and '://' in line and not line.startswith('#'):
            channels.append((current_extinf, line))
            current_extinf = None
    
    # è¿‡æ»¤JULIé¢‘é“
    hk_channels_info = []  # å­˜å‚¨(priority, extinf, url, channel_name, enhanced_extinf)
    
    for extinf, url in channels:
        if 'JULI' in extinf.upper():
            # æå–é¢‘é“å
            channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
            
            # è®¡ç®—ä¼˜å…ˆçº§
            priority = len(HK_PRIORITY_ORDER)
            for i, priority_channel in enumerate(HK_PRIORITY_ORDER):
                if priority_channel in channel_name:
                    priority = i
                    break
            
            # å¢å¼ºEXTINFï¼ˆæ·»åŠ EPGä¿¡æ¯ï¼‰
            enhanced_extinf = enhance_extinf_with_epg(extinf, channel_name, url, epg_url)
            
            # æ›¿æ¢åˆ†ç»„ä¸ºHK
            enhanced_extinf = re.sub(r'group-title="[^"]*"', 'group-title="HK"', enhanced_extinf)
            if 'group-title=' not in enhanced_extinf:
                enhanced_extinf = enhanced_extinf.replace('#EXTINF:', '#EXTINF: group-title="HK",', 1)
            
            hk_channels_info.append((priority, extinf, url, channel_name, enhanced_extinf))
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    hk_channels_info.sort(key=lambda x: x[0])
    
    # æå–æ’åºå’Œå¢å¼ºåçš„é¢‘é“
    hk_channels = [(enhanced_extinf, url) for _, _, url, _, enhanced_extinf in hk_channels_info]
    
    log(f"âœ… æå–å¹¶å¢å¼º {len(hk_channels)} ä¸ªHKé¢‘é“")
    
    return hk_channels

def extract_and_enhance_tw_channels(content, epg_url, limit=30):
    """æå–å¹¶å¢å¼ºTWé¢‘é“ï¼ˆæ·»åŠ EPGä¿¡æ¯ï¼‰"""
    if not content:
        return []
    
    log(f"æå–å¹¶å¢å¼ºTWé¢‘é“ï¼ˆå‰{limit}ä¸ªï¼‰...")
    
    # è§£æM3Uå†…å®¹
    lines = content.split('\n')
    channels = []
    current_extinf = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            current_extinf = line
        elif current_extinf and '://' in line and not line.startswith('#'):
            channels.append((current_extinf, line))
            current_extinf = None
    
    # è¿‡æ»¤4gtvé¢‘é“
    tw_channels = []
    
    for extinf, url in channels:
        if '4gtv' in extinf.lower():
            # æå–é¢‘é“å
            channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
            
            # æ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
            skip = False
            for black_word in BLACKLIST_TW:
                if black_word.lower() in channel_name.lower():
                    skip = True
                    break
            
            if not skip:
                # å¢å¼ºEXTINFï¼ˆæ·»åŠ EPGä¿¡æ¯ï¼‰
                enhanced_extinf = enhance_extinf_with_epg(extinf, channel_name, url, epg_url)
                
                # æ›¿æ¢åˆ†ç»„ä¸ºTW
                enhanced_extinf = re.sub(r'group-title="[^"]*"', 'group-title="TW"', enhanced_extinf)
                if 'group-title=' not in enhanced_extinf:
                    enhanced_extinf = enhanced_extinf.replace('#EXTINF:', '#EXTINF: group-title="TW",', 1)
                
                tw_channels.append((enhanced_extinf, url, channel_name))
    
    # åªå–å‰limitä¸ª
    if len(tw_channels) > limit:
        tw_channels = tw_channels[:limit]
    
    # æœ€ç»ˆæ ¼å¼
    final_channels = [(extinf, url) for extinf, url, _ in tw_channels]
    
    log(f"âœ… æå–å¹¶å¢å¼º {len(final_channels)} ä¸ªTWé¢‘é“")
    
    return final_channels

def main():
    """ä¸»å‡½æ•°"""
    log("å¼€å§‹åˆå¹¶M3Uæ–‡ä»¶ï¼ˆå®Œæ•´EPGè§£å†³æ–¹æ¡ˆï¼‰...")
    
    # 1. ä¸‹è½½BB.m3u
    bb_content = download_bb_m3u()
    if not bb_content:
        log("âŒ æ— æ³•ç»§ç»­ï¼ŒBB.m3uä¸‹è½½å¤±è´¥")
        return
    
    # 2. ä»ä»£ç†è·å–å†…å®¹
    proxy_content = get_content_from_proxy()
    
    # 3. å…ˆæå–é¢‘é“ä¿¡æ¯ç”¨äºEPGæµ‹è¯•
    test_channels = []
    if proxy_content:
        lines = proxy_content.split('\n')
        current_extinf = None
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if line.startswith('#EXTINF:'):
                current_extinf = line
            elif current_extinf and '://' in line and not line.startswith('#'):
                channel_name = current_extinf.split(',', 1)[1] if ',' in current_extinf else current_extinf
                test_channels.append((channel_name, line))
    
    # 4. é€‰æ‹©æœ€ä½³EPG
    epg_url = get_best_epg_for_channels(test_channels)
    
    # 5. æå–å¹¶å¢å¼ºHKé¢‘é“
    hk_channels = []
    if proxy_content:
        hk_channels = extract_and_enhance_hk_channels(proxy_content, epg_url)
    
    # 6. æå–å¹¶å¢å¼ºTWé¢‘é“
    tw_channels = []
    if proxy_content:
        tw_channels = extract_and_enhance_tw_channels(proxy_content, epg_url, limit=30)
    
    # 7. æ„å»ºM3Uå†…å®¹
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # M3Uå¤´éƒ¨ï¼ˆåŒ…å«EPGå’ŒEPGå‚æ•°ï¼‰
    m3u_header = f"""#EXTM3U x-tvg-url="{epg_url}" url-tvg="{epg_url}"
#EXTVLCOPT:program=999999
"""
    
    output = m3u_header + f"""# è‡ªåŠ¨åˆå¹¶ M3U æ–‡ä»¶
# ç”Ÿæˆæ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# ä¸‹æ¬¡æ›´æ–°: æ¯å¤© 06:00 å’Œ 18:00 (åŒ—äº¬æ—¶é—´)
# BBæº: {BB_URL}
# ä»£ç†æº: {CLOUDFLARE_PROXY}
# HKé¢‘é“: {len(hk_channels)} ä¸ª (å·²æ·»åŠ EPGä¿¡æ¯)
# TWé¢‘é“: {len(tw_channels)} ä¸ª (å·²æ·»åŠ EPGä¿¡æ¯)
# EPGæº: {epg_url}
# GitHub Actions è‡ªåŠ¨ç”Ÿæˆ

"""
    
    # æ·»åŠ BBå†…å®¹
    bb_lines = bb_content.split('\n')
    bb_count = 0
    skip_first = True
    
    for line in bb_lines:
        line = line.rstrip()
        if not line:
            continue
        
        if skip_first and line.startswith('#EXTM3U'):
            skip_first = False
            continue
        
        output += line + '\n'
        if line.startswith('#EXTINF:'):
            bb_count += 1
    
    # æ·»åŠ HKé¢‘é“
    if hk_channels:
        output += f"\n# HKé¢‘é“ ({len(hk_channels)}ä¸ªï¼Œå·²æ·»åŠ tvg-id)\n"
        
        # ä¼˜å…ˆé¢‘é“
        priority_channels = [(extinf, url) for extinf, url in hk_channels 
                           if any(priority in extinf for priority in HK_PRIORITY_ORDER)]
        
        if priority_channels:
            output += f"# --- ä¼˜å…ˆé¢‘é“ ---\n"
            for extinf, url in priority_channels:
                output += extinf + '\n'
                output += url + '\n'
        
        # å…¶ä»–HKé¢‘é“
        other_channels = [(extinf, url) for extinf, url in hk_channels 
                         if not any(priority in extinf for priority in HK_PRIORITY_ORDER)]
        
        if other_channels:
            output += f"# --- å…¶ä»–HKé¢‘é“ ---\n"
            for extinf, url in other_channels:
                output += extinf + '\n'
                output += url + '\n'
    
    # æ·»åŠ TWé¢‘é“
    if tw_channels:
        output += f"\n# TWé¢‘é“ ({len(tw_channels)}ä¸ªï¼Œå·²æ·»åŠ tvg-id)\n"
        for extinf, url in tw_channels:
            output += extinf + '\n'
            output += url + '\n'
    
    # æ·»åŠ EPGä½¿ç”¨è¯´æ˜
    output += f"""
# EPGä½¿ç”¨è¯´æ˜
# 1. æ­¤æ–‡ä»¶å·²æ·»åŠ  tvg-id å’Œ tvg-name å±æ€§
# 2. EPGæº: {epg_url}
# 3. å¦‚æœEPGä¸æ˜¾ç¤ºï¼Œè¯·æ£€æŸ¥æ’­æ”¾å™¨è®¾ç½®:
#    - ç¡®ä¿å¯ç”¨EPGåŠŸèƒ½
#    - æ£€æŸ¥æ—¶åŒºè®¾ç½®ï¼ˆå»ºè®®ä½¿ç”¨UTC+8ï¼‰
#    - æ¸…é™¤EPGç¼“å­˜åé‡æ–°åŠ è½½
# 4. å¸¸è§EPGé—®é¢˜:
#    - é¢‘é“IDä¸åŒ¹é…ï¼šæˆ‘ä»¬å·²ä¸ºæ¯ä¸ªé¢‘é“ç”Ÿæˆç¨³å®šID
#    - EPGæºå¤±æ•ˆï¼šè‡ªåŠ¨é€‰æ‹©æœ€ä½³å¯ç”¨æº
#    - æ—¶åŒºä¸å¯¹ï¼šEPGä½¿ç”¨UTC+8æ—¶é—´

# ç»Ÿè®¡ä¿¡æ¯
# BB é¢‘é“æ•°: {bb_count}
# HK é¢‘é“æ•°: {len(hk_channels)} (å·²å¢å¼ºEPG)
# TW é¢‘é“æ•°: {len(tw_channels)} (å·²å¢å¼ºEPG)
# æ€»é¢‘é“æ•°: {bb_count + len(hk_channels) + len(tw_channels)}
# EPGçŠ¶æ€: âœ… å·²é…ç½®
# æ›´æ–°æ—¶é—´: {timestamp}
# æ›´æ–°é¢‘ç‡: æ¯å¤© 06:00 å’Œ 18:00 (åŒ—äº¬æ—¶é—´)
"""
    
    # 8. ä¿å­˜æ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(output)
    
    log(f"\nğŸ‰ åˆå¹¶å®Œæˆ!")
    log(f"ğŸ“ æ–‡ä»¶: {OUTPUT_FILE}")
    log(f"ğŸ“ å¤§å°: {len(output)} å­—ç¬¦")
    log(f"ğŸ“¡ EPG: {epg_url}")
    log(f"ğŸ¯ HKé¢‘é“: {len(hk_channels)} (å·²æ·»åŠ tvg-id)")
    log(f"ğŸ¯ TWé¢‘é“: {len(tw_channels)} (å·²æ·»åŠ tvg-id)")
    log(f"ğŸ“º æ€»è®¡: {bb_count + len(hk_channels) + len(tw_channels)}")
    log(f"ğŸ•’ ä¸‹æ¬¡è‡ªåŠ¨æ›´æ–°: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 18:00")

if __name__ == "__main__":
    main()
