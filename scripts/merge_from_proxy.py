#!/usr/bin/env python3
"""
M3Uæ–‡ä»¶åˆå¹¶è„šæœ¬ - ä»GitHubç›´æ¥æŠ“å–å…¨ç½‘é€šæ¸¯æ¾³å°ç›´æ’­æº
1. ä¸‹è½½BB.m3uï¼ˆåŒ…å«EPGä¿¡æ¯ï¼‰
2. ä»GitHubç›´æ¥æŠ“å–"å…¨ç½‘é€šæ¸¯æ¾³å°"ç›´æ’­æº
3. ä¸BBåˆå¹¶ç”ŸæˆCC.m3u
åŒ—äº¬æ—¶é—´æ¯å¤©6:00ã€18:00è‡ªåŠ¨è¿è¡Œ
"""

import requests
import re
import os
import time
from datetime import datetime

# é…ç½®
BB_URL = "https://raw.githubusercontent.com/sufernnet/joker/main/BB.m3u"
GITHUB_RAW_URL = "https://raw.githubusercontent.com/Jsnzkpg/Jsnzkpg/main/Jsnzkpg1"
KEYWORD = "å…¨ç½‘é€šæ¸¯æ¾³å°"
OUTPUT_FILE = "CC.m3u"

# å¤‡é€‰EPGæºï¼ˆå¦‚æœä¸»è¦EPGå¤±æ•ˆï¼‰
BACKUP_EPG_URLS = [
    "https://epg.112114.xyz/pp.xml",  # BBçš„EPG
    "https://epg.946985.filegear-sg.me/t.xml.gz",
    "http://epg.51zmt.top:8000/e.xml"
]

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def test_epg_url(epg_url):
    """æµ‹è¯•EPG URLæ˜¯å¦å¯è®¿é—®"""
    try:
        log(f"æµ‹è¯•EPG: {epg_url}")
        headers = {
            'User-Agent': 'Mozilla/5.0',
            'Accept': '*/*'
        }
        
        # åªä¸‹è½½å‰1KBæ£€æŸ¥
        response = requests.get(epg_url, headers=headers, timeout=10, stream=True)
        
        if response.status_code == 200:
            # è¯»å–å‰1KBæ£€æŸ¥
            chunk = response.raw.read(1024)
            text = chunk.decode('utf-8', errors='ignore')
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯XMLæ ¼å¼
            if '<?xml' in text or '<tv' in text or '<programme' in text:
                log(f"âœ… EPGå¯ç”¨: {epg_url}")
                return True
            else:
                log(f"âš ï¸  EPGä¸æ˜¯XMLæ ¼å¼: {epg_url}")
                return False
        else:
            log(f"âŒ EPGä¸å¯è®¿é—®: {epg_url} (çŠ¶æ€ç : {response.status_code})")
            return False
            
    except Exception as e:
        log(f"âŒ EPGæµ‹è¯•å¤±è´¥ {epg_url}: {e}")
        return False

def get_best_epg_url(epg_urls):
    """è·å–æœ€ä½³çš„EPG URL"""
    log("å¯»æ‰¾æœ€ä½³EPGæº...")
    
    # æµ‹è¯•æ‰€æœ‰EPG
    working_epgs = []
    for epg_url in epg_urls:
        if test_epg_url(epg_url):
            working_epgs.append(epg_url)
    
    if working_epgs:
        # ä¼˜å…ˆä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„
        best_epg = working_epgs[0]
        log(f"âœ… é€‰æ‹©EPG: {best_epg}")
        log(f"   å…¶ä»–å¯ç”¨EPG: {len(working_epgs)-1}ä¸ª")
        return best_epg
    else:
        log("âš ï¸  æ²¡æœ‰å¯ç”¨çš„EPGæº")
        return None

def download_bb_m3u():
    """ä¸‹è½½BB.m3uå¹¶æå–EPG"""
    try:
        log("ä¸‹è½½BB.m3u...")
        response = requests.get(BB_URL, timeout=10)
        response.raise_for_status()
        
        bb_content = response.text
        log(f"âœ… BB.m3uä¸‹è½½æˆåŠŸ ({len(bb_content)} å­—ç¬¦)")
        
        return bb_content
        
    except Exception as e:
        log(f"âŒ BB.m3uä¸‹è½½å¤±è´¥: {e}")
        return None

def get_gangaotai_channels():
    """ä»GitHubç›´æ¥æŠ“å–åŒ…å«"å…¨ç½‘é€šæ¸¯æ¾³å°"çš„é¢‘é“"""
    try:
        log(f"ä»GitHubç›´æ¥æŠ“å–åŒ…å«'{KEYWORD}'çš„é¢‘é“...")
        log(f"åœ°å€: {GITHUB_RAW_URL}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        
        response = requests.get(GITHUB_RAW_URL, headers=headers, timeout=15)
        
        if response.status_code == 200:
            content = response.text
            
            if content and content.strip():
                log(f"âœ… ä¸‹è½½æˆåŠŸ ({len(content)} å­—ç¬¦)")
                
                # æ£€æŸ¥å†…å®¹æ ¼å¼
                if content.startswith('#EXTM3U'):
                    log("âœ… å†…å®¹ä¸ºæœ‰æ•ˆçš„M3Uæ ¼å¼")
                else:
                    log("âš ï¸  å†…å®¹ä¸æ˜¯æ ‡å‡†M3Uæ ¼å¼ï¼Œä½†ä»å°è¯•è§£æ")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                if KEYWORD in content:
                    log(f"âœ… æ‰¾åˆ°'{KEYWORD}'ç›¸å…³å†…å®¹")
                    
                    # æå–æ‰€æœ‰åŒ…å«å…³é”®è¯çš„é¢‘é“
                    channels = extract_channels_by_keyword(content, KEYWORD)
                    
                    if channels:
                        log(f"âœ… æå–åˆ° {len(channels)//2} ä¸ªåŒ…å«'{KEYWORD}'çš„é¢‘é“")
                        
                        # æ˜¾ç¤ºå‰å‡ ä¸ªé¢‘é“åç§°ç”¨äºéªŒè¯
                        log("å‰5ä¸ªé¢‘é“:")
                        for i in range(0, min(5*2, len(channels)), 2):
                            if i+1 < len(channels):
                                extinf = channels[i]
                                # æå–é¢‘é“å
                                channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
                                log(f"  {i//2+1}. {channel_name[:60]}")
                        
                        m3u_content = "#EXTM3U\n" + "\n".join(channels)
                        return m3u_content
                    else:
                        log(f"âŒ æ‰¾åˆ°å…³é”®è¯ä½†æœªèƒ½æå–åˆ°é¢‘é“ï¼Œå°è¯•è°ƒè¯•...")
                        
                        # è°ƒè¯•ï¼šæ˜¾ç¤ºå†…å®¹ç»“æ„
                        debug_content_structure(content)
                        
                else:
                    log(f"âŒ æ²¡æœ‰æ‰¾åˆ°'{KEYWORD}'ç›¸å…³å†…å®¹")
                    
                    # æ˜¾ç¤ºå†…å®¹å‰500å­—ç¬¦æŸ¥çœ‹æ ¼å¼
                    log(f"å†…å®¹å‰500å­—ç¬¦:")
                    log(content[:500])
                    
                    # å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„åˆ†ç»„å
                    find_all_groups(content)
                    
                    return None
            else:
                log("âš ï¸  å†…å®¹ä¸ºç©º")
        else:
            log(f"âŒ ä¸‹è½½å¤±è´¥: HTTP {response.status_code}")
            
    except Exception as e:
        log(f"âŒ æŠ“å–å¤±è´¥: {e}")
    
    return None

def extract_channels_by_keyword(content, keyword):
    """æå–æ‰€æœ‰åŒ…å«å…³é”®è¯çš„é¢‘é“"""
    channels = []
    lines = content.split('\n')
    current_extinf = None
    
    for line in lines:
        line = line.rstrip()
        if not line:
            continue
            
        if line.startswith('#EXTINF:'):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
            if keyword in line:
                current_extinf = line
            else:
                current_extinf = None
                
        elif current_extinf and '://' in line and not line.startswith('#'):
            # è¿™æ˜¯ä¸€ä¸ªé¢‘é“URLï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            channels.append(current_extinf)
            channels.append(line)
            current_extinf = None
    
    return channels

def debug_content_structure(content):
    """è°ƒè¯•å†…å®¹ç»“æ„"""
    log("è°ƒè¯•å†…å®¹ç»“æ„...")
    lines = content.split('\n')
    
    # ç»Ÿè®¡æ€»è¡Œæ•°
    log(f"æ€»è¡Œæ•°: {len(lines)}")
    
    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«å…³é”®è¯çš„è¡Œ
    keyword_lines = [line for line in lines if KEYWORD in line]
    log(f"åŒ…å«'{KEYWORD}'çš„è¡Œæ•°: {len(keyword_lines)}")
    
    if keyword_lines:
        log(f"å‰5ä¸ªåŒ…å«'{KEYWORD}'çš„è¡Œ:")
        for i, line in enumerate(keyword_lines[:5], 1):
            log(f"  {i}: {line[:100]}...")
    
    # ç»Ÿè®¡EXTINFè¡Œæ•°
    extinf_lines = [line for line in lines if line.startswith('#EXTINF:')]
    log(f"#EXTINFè¡Œæ•°: {len(extinf_lines)}")
    
    # ç»Ÿè®¡URLè¡Œæ•°
    url_lines = [line for line in lines if '://' in line and not line.startswith('#')]
    log(f"URLè¡Œæ•°: {len(url_lines)}")

def find_all_groups(content):
    """æŸ¥æ‰¾å†…å®¹ä¸­çš„æ‰€æœ‰åˆ†ç»„"""
    log("æŸ¥æ‰¾æ‰€æœ‰åˆ†ç»„...")
    
    # æŸ¥æ‰¾æ‰€æœ‰åˆ†ç»„
    groups = re.findall(r'group-title="([^"]+)"', content)
    if groups:
        unique_groups = list(set(groups))
        log(f"å‘ç° {len(unique_groups)} ä¸ªå”¯ä¸€åˆ†ç»„")
        
        # æŒ‰é¢‘é“æ•°æ’åº
        group_counts = {}
        for group in groups:
            group_counts[group] = group_counts.get(group, 0) + 1
        
        # æ˜¾ç¤ºå‰20ä¸ªåˆ†ç»„
        log("åˆ†ç»„åˆ—è¡¨ï¼ˆæŒ‰é¢‘é“æ•°æ’åºï¼‰:")
        sorted_groups = sorted(group_counts.items(), key=lambda x: x[1], reverse=True)
        for i, (group, count) in enumerate(sorted_groups[:20], 1):
            log(f"  {i:2d}. {group} ({count}ä¸ªé¢‘é“)")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«"æ¸¯æ¾³å°"ç›¸å…³å…³é”®è¯
            if "æ¸¯æ¾³å°" in group or "HK" in group or "TW" in group or "Hongkong" in group or "Taiwan" in group:
                log(f"       âš ï¸  å¯èƒ½æ˜¯ç›®æ ‡åˆ†ç»„")
    
    # æŸ¥æ‰¾å¯èƒ½çš„EPGä¿¡æ¯
    epg_patterns = [
        (r'url-tvg="([^"]+)"', 'url-tvg'),
        (r'x-tvg-url="([^"]+)"', 'x-tvg-url'),
    ]
    
    for pattern, name in epg_patterns:
        matches = re.findall(pattern, content)
        if matches:
            log(f"æ‰¾åˆ° {len(matches)} ä¸ª{name} EPG:")
            for match in matches[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                log(f"  - {match}")

def extract_epg_urls(content):
    """ä»å†…å®¹ä¸­æå–EPG URL"""
    epg_urls = []
    
    if not content:
        return epg_urls
    
    patterns = [
        r'url-tvg="([^"]+)"',
        r'x-tvg-url="([^"]+)"',
        r'tvg-url="([^"]+)"',
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        for match in matches:
            if 'http' in match and match not in epg_urls:
                epg_urls.append(match)
    
    return epg_urls

def main():
    """ä¸»å‡½æ•°"""
    log("å¼€å§‹åˆå¹¶M3Uæ–‡ä»¶...")
    
    current_time = datetime.now()
    log(f"å½“å‰æ—¶é—´: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
    log(f"ä¸‹æ¬¡è¿è¡Œ: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 18:00")
    log(f"GitHubåœ°å€: {GITHUB_RAW_URL}")
    log(f"æœç´¢å…³é”®è¯: {KEYWORD}")
    
    # 1. ä¸‹è½½BB.m3u
    bb_content = download_bb_m3u()
    if not bb_content:
        log("âŒ æ— æ³•ç»§ç»­ï¼ŒBB.m3uä¸‹è½½å¤±è´¥")
        return
    
    # 2. ä»GitHubæŠ“å–åŒ…å«"å…¨ç½‘é€šæ¸¯æ¾³å°"çš„é¢‘é“
    gangaotai_content = get_gangaotai_channels()
    
    # 3. æ”¶é›†æ‰€æœ‰EPGæº
    epg_urls = []
    
    # ä»BB.m3uæå–EPG
    bb_epg_match = re.search(r'url-tvg="([^"]+)"', bb_content)
    if bb_epg_match:
        epg_urls.append(bb_epg_match.group(1))
        log(f"âœ… æ‰¾åˆ°BB EPG: {bb_epg_match.group(1)}")
    
    # ä»æ¸¯æ¾³å°å†…å®¹æå–EPG
    if gangaotai_content:
        new_epgs = extract_epg_urls(gangaotai_content)
        for epg in new_epgs:
            if epg not in epg_urls:
                epg_urls.append(epg)
                log(f"âœ… æ‰¾åˆ°æ–°EPG: {epg}")
    
    # æ·»åŠ å¤‡é€‰EPG
    for backup_epg in BACKUP_EPG_URLS:
        if backup_epg not in epg_urls:
            epg_urls.append(backup_epg)
    
    log(f"æ€»å…±æ‰¾åˆ° {len(epg_urls)} ä¸ªEPGæº")
    
    # 4. è·å–æœ€ä½³EPG
    best_epg = get_best_epg_url(epg_urls)
    
    # 5. è§£ææ¸¯æ¾³å°é¢‘é“å†…å®¹
    gangaotai_count = 0
    gangaotai_channels_content = ""
    
    if gangaotai_content:
        lines = gangaotai_content.split('\n')
        for line in lines:
            line = line.rstrip()
            if not line:
                continue
            
            if line.startswith('#EXTM3U'):
                continue
            
            gangaotai_channels_content += line + '\n'
            if line.startswith('#EXTINF:'):
                gangaotai_count += 1
    
    # 6. æ„å»ºM3Uå†…å®¹
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # M3Uå¤´éƒ¨ï¼ˆä½¿ç”¨æœ€ä½³EPGï¼‰
    if best_epg:
        m3u_header = f'#EXTM3U url-tvg="{best_epg}"\n'
        log(f"âœ… ä½¿ç”¨EPG: {best_epg}")
    else:
        m3u_header = '#EXTM3U\n'
        log("âš ï¸  æœªæ‰¾åˆ°å¯ç”¨EPG")
    
    output = m3u_header + f"""# è‡ªåŠ¨åˆå¹¶ M3U æ–‡ä»¶
# ç”Ÿæˆæ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# ä¸‹æ¬¡æ›´æ–°: æ¯å¤© 06:00 å’Œ 18:00 (åŒ—äº¬æ—¶é—´)
# BBæº: {BB_URL}
# GitHubæº: {GITHUB_RAW_URL}
# æœç´¢å…³é”®è¯: {KEYWORD}
# EPGæº: {best_epg if best_epg else 'æ— å¯ç”¨EPG'}
# GitHub Actions è‡ªåŠ¨ç”Ÿæˆ

"""
    
    # æ·»åŠ BBå†…å®¹ï¼ˆè·³è¿‡ç¬¬ä¸€è¡Œï¼‰
    bb_lines = bb_content.split('\n')
    bb_count = 0
    skip_first = True
    
    for line in bb_lines:
        line = line.rstrip()
        if not line:
            continue
        
        if skip_first and line.startswith('#EXTM3U'):
            skip_first = False
            continue
        
        output += line + '\n'
        if line.startswith('#EXTINF:'):
            bb_count += 1
    
    # æ·»åŠ æ¸¯æ¾³å°é¢‘é“
    if gangaotai_channels_content:
        output += f"\n# {KEYWORD}é¢‘é“\n"
        output += f"# æ¥è‡ª: {GITHUB_RAW_URL}\n"
        output += gangaotai_channels_content
        
        # æ·»åŠ é¢‘é“ç»Ÿè®¡
        output += f"\n# {KEYWORD}é¢‘é“ç»Ÿè®¡\n"
        output += f"# å…±æå–åˆ° {gangaotai_count} ä¸ªé¢‘é“\n"
    else:
        log("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ¸¯æ¾³å°ç›¸å…³é¢‘é“")
        output += f"\n# æ³¨æ„ï¼šæœªæ‰¾åˆ°åŒ…å«'{KEYWORD}'çš„é¢‘é“\n"
        output += f"# GitHubæº: {GITHUB_RAW_URL}\n"
    
    # æ·»åŠ EPGä¿¡æ¯è¯´æ˜
    if epg_urls:
        output += f"""
# EPGä¿¡æ¯
# ä½¿ç”¨EPG: {best_epg if best_epg else 'æ— '}
# æµ‹è¯•çš„EPGæº ({len(epg_urls)}ä¸ª):"""
        for i, epg in enumerate(epg_urls, 1):
            status = "âœ…" if epg == best_epg else "  "
            output += f"\n#   {status} {epg}"
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    output += f"""
# ç»Ÿè®¡ä¿¡æ¯
# BB é¢‘é“æ•°: {bb_count}
# {KEYWORD} é¢‘é“æ•°: {gangaotai_count}
# æ€»é¢‘é“æ•°: {bb_count + gangaotai_count}
# EPGçŠ¶æ€: {'âœ… æ­£å¸¸' if best_epg else 'âŒ æ— å¯ç”¨EPG'}
# æ›´æ–°æ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# æ›´æ–°é¢‘ç‡: æ¯å¤© 06:00 å’Œ 18:00 (åŒ—äº¬æ—¶é—´)
"""
    
    # 7. ä¿å­˜æ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(output)
    
    log(f"\nğŸ‰ åˆå¹¶å®Œæˆ!")
    log(f"ğŸ“ æ–‡ä»¶: {OUTPUT_FILE}")
    log(f"ğŸ“ å¤§å°: {len(output)} å­—ç¬¦")
    log(f"ğŸ“¡ EPG: {best_epg if best_epg else 'æ— å¯ç”¨EPG'}")
    log(f"ğŸ“º BBé¢‘é“: {bb_count}")
    log(f"ğŸ“º {KEYWORD}é¢‘é“: {gangaotai_count}")
    log(f"ğŸ“º æ€»è®¡: {bb_count + gangaotai_count}")
    log(f"ğŸ•’ ä¸‹æ¬¡è‡ªåŠ¨æ›´æ–°: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 18:00")

if __name__ == "__main__":
    main()
