#!/usr/bin/env python3
"""
M3Uæ–‡ä»¶åˆå¹¶è„šæœ¬ - ä½¿ç”¨Cloudflareä»£ç†
1. é€šè¿‡ä»£ç†æ›´æ–°å¹¶è·å–JULIè®¢é˜…
2. ä»æ›´æ–°çš„è®¢é˜…ä¸­æå–JULIé¢‘é“å¹¶æ”¹ä¸ºHKåˆ†ç»„
3. åˆå¹¶BB.m3uå’Œæå–çš„HKé¢‘é“
4. ç”Ÿæˆæ–°çš„CC.m3u
"""

import requests
import re
import os
import time
from datetime import datetime

# é…ç½®
BB_URL = "https://raw.githubusercontent.com/sufernnet/joker/main/BB.m3u"
CLOUDFLARE_PROXY = "https://smt-proxy.sufern001.workers.dev/"
OUTPUT_FILE = "CC.m3u"

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def download_with_retry(url, description, max_retries=3):
    """ä¸‹è½½æ–‡ä»¶ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': '*/*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    }
    
    for attempt in range(max_retries):
        try:
            log(f"ä¸‹è½½{description} (å°è¯• {attempt + 1}/{max_retries})...")
            
            # å¦‚æœæ˜¯Cloudflareä»£ç†ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
            if 'workers.dev' in url:
                headers['Referer'] = 'https://smart.946985.filegear-sg.me/'
                headers['Origin'] = 'https://smart.946985.filegear-sg.me'
            
            response = requests.get(url, headers=headers, timeout=30)
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                content = response.text.strip()
                if content:
                    log(f"âœ… {description} ä¸‹è½½æˆåŠŸ ({len(content)} å­—ç¬¦)")
                    return content
                else:
                    log(f"âš ï¸  {description} å†…å®¹ä¸ºç©º")
            else:
                log(f"âŒ {description} HTTPé”™è¯¯: {response.status_code}")
                
        except requests.exceptions.Timeout:
            log(f"âŒ {description} è¶…æ—¶")
        except requests.exceptions.ConnectionError:
            log(f"âŒ {description} è¿æ¥é”™è¯¯")
        except Exception as e:
            log(f"âŒ {description} é”™è¯¯: {e}")
        
        if attempt < max_retries - 1:
            wait_time = (attempt + 1) * 5
            log(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)
    
    return None

def get_juli_content_from_proxy():
    """ä»Cloudflareä»£ç†è·å–JULIå†…å®¹"""
    log("ä»Cloudflareä»£ç†è·å–JULIå†…å®¹...")
    
    # å°è¯•ä¸åŒçš„è®¿é—®æ–¹å¼
    test_urls = [
        CLOUDFLARE_PROXY,
        f"{CLOUDFLARE_PROXY}?url=https://smart.946985.filegear-sg.me/sub.php?user=tg_Thinkoo_bot",
        f"{CLOUDFLARE_PROXY}?target=https://smart.946985.filegear-sg.me/sub.php?user=tg_Thinkoo_bot",
        f"{CLOUDFLARE_PROXY}get-juli",
        f"{CLOUDFLARE_PROXY}juli",
        f"{CLOUDFLARE_PROXY}proxy",
    ]
    
    for url in test_urls:
        log(f"å°è¯•URL: {url}")
        content = download_with_retry(url, f"ä»£ç† {url}")
        
        if content:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„M3Uå†…å®¹
            if content.startswith('#EXTM3U'):
                log(f"âœ… æ‰¾åˆ°æœ‰æ•ˆçš„M3Uå†…å®¹")
                return content
            elif 'JULI' in content.upper():
                log(f"âœ… æ‰¾åˆ°åŒ…å«JULIçš„å†…å®¹")
                return content
            elif '<html' in content.lower():
                # å¦‚æœæ˜¯HTMLé¡µé¢ï¼Œå°è¯•æå–M3Ué“¾æ¥
                log("å°è¯•ä»HTMLé¡µé¢æå–...")
                m3u_content = extract_m3u_from_html(content)
                if m3u_content:
                    return m3u_content
    
    log("âŒ æ— æ³•ä»ä»£ç†è·å–JULIå†…å®¹")
    return None

def extract_m3u_from_html(html_content):
    """ä»HTMLé¡µé¢ä¸­æå–M3Uå†…å®¹"""
    # æ–¹æ³•1ï¼šæŸ¥æ‰¾#EXTM3Uå¼€å¤´çš„æ–‡æœ¬
    pattern = r'(#EXTM3U.*?)(?:</pre>|</code>|</textarea>|</script>|$)'
    match = re.search(pattern, html_content, re.DOTALL)
    
    if match:
        content = match.group(1).strip()
        log(f"âœ… ä»HTMLæå–åˆ°M3Uå†…å®¹ ({len(content)} å­—ç¬¦)")
        return content
    
    # æ–¹æ³•2ï¼šæŸ¥æ‰¾é¢‘é“è¡Œ
    lines = html_content.split('\n')
    m3u_lines = []
    
    for line in lines:
        line = line.strip()
        if line.startswith('#EXTINF:') or ('://' in line and not line.startswith('<')):
            m3u_lines.append(line)
    
    if m3u_lines:
        content = '#EXTM3U\n' + '\n'.join(m3u_lines)
        log(f"âœ… ä»HTMLæå–åˆ° {len(m3u_lines)} ä¸ªé¢‘é“")
        return content
    
    # æ–¹æ³•3ï¼šæŸ¥æ‰¾M3Ué“¾æ¥å¹¶ä¸‹è½½
    m3u_links = re.findall(r'https?://[^\s"\']+\.m3u(?:\?[^\s"\']*)?', html_content, re.IGNORECASE)
    
    if m3u_links:
        log(f"æ‰¾åˆ° {len(m3u_links)} ä¸ªM3Ué“¾æ¥")
        for link in m3u_links[:2]:  # åªå°è¯•å‰2ä¸ª
            try:
                content = download_with_retry(link, f"M3Ué“¾æ¥ {link}")
                if content and content.startswith('#EXTM3U'):
                    return content
            except:
                continue
    
    return None

def extract_hk_channels_from_content(content):
    """ä»å†…å®¹ä¸­æå–JULIé¢‘é“å¹¶æ”¹ä¸ºHKåˆ†ç»„"""
    if not content:
        return []
    
    log("ä»å†…å®¹ä¸­æå–JULIé¢‘é“å¹¶æ”¹ä¸ºHKåˆ†ç»„...")
    
    # å…ˆæå–æ‰€æœ‰é¢‘é“
    channels = []
    lines = content.split('\n')
    current_extinf = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            current_extinf = line
        elif current_extinf and '://' in line and not line.startswith('#'):
            channels.append((current_extinf, line))
            current_extinf = None
    
    # è¿‡æ»¤å¹¶é‡å‘½åJULIé¢‘é“
    hk_channels = []
    seen = set()
    
    for extinf, url in channels:
        # æ£€æŸ¥æ˜¯å¦æ˜¯JULIé¢‘é“
        if 'JULI' in extinf.upper():
            # é‡å‘½åä¸ºHKåˆ†ç»„
            new_extinf = re.sub(r'JULI', 'HK', extinf, flags=re.IGNORECASE)
            
            # æ·»åŠ æˆ–ä¿®æ”¹group-title
            if 'group-title=' in new_extinf:
                new_extinf = re.sub(r'group-title="[^"]*"', 'group-title="HK"', new_extinf)
            else:
                # å¦‚æœæ²¡æœ‰group-titleï¼Œæ·»åŠ ä¸€ä¸ª
                if ',' in new_extinf:
                    parts = new_extinf.split(',', 1)
                    new_extinf = f'{parts[0]} group-title="HK",{parts[1]}'
            
            # å»é‡
            channel_key = f"{new_extinf}|{url}"
            if channel_key not in seen:
                seen.add(channel_key)
                hk_channels.append((new_extinf, url))
    
    log(f"âœ… æå–åˆ° {len(hk_channels)} ä¸ªHKé¢‘é“ï¼ˆåŸJULIé¢‘é“ï¼‰")
    
    # æ˜¾ç¤ºéƒ¨åˆ†é¢‘é“
    if hk_channels:
        log("éƒ¨åˆ†HKé¢‘é“:")
        for i, (extinf, url) in enumerate(hk_channels[:3]):
            if ',' in extinf:
                name = extinf.split(',', 1)[1]
                log(f"  {i+1}. {name[:50]}{'...' if len(name) > 50 else ''}")
    
    return hk_channels

def get_epg_url(content):
    """ä»å†…å®¹ä¸­æå–EPG URL"""
    if not content:
        return None
    
    # æŸ¥æ‰¾url-tvg
    match = re.search(r'url-tvg="([^"]+)"', content)
    if match:
        return match.group(1)
    
    # æŸ¥æ‰¾x-tvg-url
    match = re.search(r'x-tvg-url="([^"]+)"', content)
    if match:
        return match.group(1)
    
    return None

def get_existing_hk_channels():
    """ä»ç°æœ‰çš„CC.m3uä¸­è·å–HKé¢‘é“ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    if not os.path.exists(OUTPUT_FILE):
        return []
    
    try:
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŸ¥æ‰¾HKé¢‘é“åŒºåŸŸ
        lines = content.split('\n')
        hk_channels = []
        in_hk_section = False
        current_extinf = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if '# HKé¢‘é“' in line or '# HK é¢‘é“' in line:
                in_hk_section = True
                continue
            
            if in_hk_section and line.startswith('#EXTINF:'):
                current_extinf = line
            elif in_hk_section and current_extinf and '://' in line and not line.startswith('#'):
                if 'HK' in current_extinf.upper():
                    hk_channels.append((current_extinf, line))
                current_extinf = None
        
        if hk_channels:
            log(f"âœ… ä»ç°æœ‰æ–‡ä»¶æ‰¾åˆ° {len(hk_channels)} ä¸ªHKé¢‘é“")
        return hk_channels
        
    except Exception as e:
        log(f"âŒ è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}")
        return []

def main():
    """ä¸»å‡½æ•°"""
    log("å¼€å§‹æ›´æ–°å’Œåˆå¹¶M3Uæ–‡ä»¶...")
    
    # 1. ä¸‹è½½BB.m3u
    bb_content = download_with_retry(BB_URL, "BB.m3u")
    if not bb_content:
        log("âŒ BB.m3uä¸‹è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    
    # æå–BBçš„EPG
    bb_epg = get_epg_url(bb_content)
    if bb_epg:
        log(f"âœ… BB EPG: {bb_epg}")
    
    # 2. ä»Cloudflareä»£ç†è·å–JULIå†…å®¹
    juli_content = get_juli_content_from_proxy()
    
    # æå–HKé¢‘é“
    hk_channels = []
    if juli_content:
        hk_channels = extract_hk_channels_from_content(juli_content)
    
    # 3. å¦‚æœæ²¡æå–åˆ°ï¼Œä½¿ç”¨ç°æœ‰çš„HKé¢‘é“
    if not hk_channels:
        log("âš ï¸  æ— æ³•ä»ä»£ç†æå–HKé¢‘é“ï¼Œä½¿ç”¨ç°æœ‰æ–‡ä»¶ä¸­çš„HKé¢‘é“")
        hk_channels = get_existing_hk_channels()
    
    # æå–JULIçš„EPG
    juli_epg = get_epg_url(juli_content) if juli_content else None
    if juli_epg:
        log(f"âœ… JULI EPG: {juli_epg}")
    
    # 4. é€‰æ‹©EPGï¼ˆä¼˜å…ˆä½¿ç”¨BBçš„ï¼‰
    epg_url = bb_epg or juli_epg
    if epg_url:
        log(f"âœ… ä½¿ç”¨EPG: {epg_url}")
    
    # 5. æ„å»ºåˆå¹¶åçš„M3U
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # M3Uå¤´éƒ¨
    if epg_url:
        output = f'#EXTM3U url-tvg="{epg_url}"\n'
    else:
        output = '#EXTM3U\n'
    
    output += f"""# è‡ªåŠ¨åˆå¹¶ M3U æ–‡ä»¶
# ç”Ÿæˆæ—¶é—´: {timestamp}
# BBæº: {BB_URL}
# ä»£ç†æº: {CLOUDFLARE_PROXY}
# JULIåˆ†ç»„å·²æ”¹ä¸ºHKåˆ†ç»„
# EPGæº: {epg_url if epg_url else 'æ— '}
# æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# GitHub Actions è‡ªåŠ¨ç”Ÿæˆ

"""
    
    # æ·»åŠ BBå†…å®¹ï¼ˆè·³è¿‡å¼€å¤´çš„#EXTM3Uè¡Œï¼‰
    bb_lines = bb_content.split('\n')
    bb_count = 0
    skip_header = True
    
    for line in bb_lines:
        line = line.rstrip()
        if not line:
            continue
        
        if skip_header and line.startswith('#EXTM3U'):
            skip_header = False
            continue
        
        output += line + '\n'
        if line.startswith('#EXTINF:'):
            bb_count += 1
    
    # æ·»åŠ HKé¢‘é“
    if hk_channels:
        output += f"\n# HKé¢‘é“ (åŸJULIé¢‘é“ï¼Œé€šè¿‡Cloudflareä»£ç†æ›´æ–°)\n"
        for extinf, url in hk_channels:
            output += extinf + '\n'
            output += url + '\n'
    else:
        log("âš ï¸  æ²¡æœ‰æ‰¾åˆ°HKé¢‘é“")
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    output += f"""
# ç»Ÿè®¡ä¿¡æ¯
# BB é¢‘é“æ•°: {bb_count}
# HK é¢‘é“æ•°: {len(hk_channels)}
# æ€»é¢‘é“æ•°: {bb_count + len(hk_channels)}
# æ›´æ–°æ—¶é—´: {timestamp}
# ä¸‹æ¬¡æ›´æ–°: æ¯å¤© 06:00 å’Œ 18:00 (åŒ—äº¬æ—¶é—´)
# ä»£ç†çŠ¶æ€: {"âœ… æ­£å¸¸" if juli_content else "âš ï¸  ä½¿ç”¨ç¼“å­˜"}
"""
    
    # 6. ä¿å­˜æ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(output)
    
    log(f"\nğŸ‰ åˆå¹¶å®Œæˆ!")
    log(f"ğŸ“ æ–‡ä»¶: {OUTPUT_FILE}")
    log(f"ğŸ“ å¤§å°: {len(output)} å­—ç¬¦")
    log(f"ğŸ“¡ EPG: {epg_url if epg_url else 'æ— '}")
    log(f"ğŸ“º BBé¢‘é“: {bb_count}")
    log(f"ğŸ“º HKé¢‘é“: {len(hk_channels)}")
    log(f"ğŸ“º æ€»è®¡: {bb_count + len(hk_channels)}")
    log(f"ğŸŒ ä»£ç†çŠ¶æ€: {'âœ… é€šè¿‡ä»£ç†æ›´æ–°' if juli_content else 'âš ï¸  ä½¿ç”¨ç°æœ‰é¢‘é“'}")
    
    # 7. ä¿å­˜æ›´æ–°è®°å½•
    with open("update_log.txt", "a", encoding="utf-8") as f:
        f.write(f"{timestamp} | BB:{bb_count} | HK:{len(hk_channels)} | EPG:{epg_url or 'none'} | PROXY:{'OK' if juli_content else 'CACHE'}\n")

if __name__ == "__main__":
    main()
