#!/usr/bin/env python3
"""
M3Uæ–‡ä»¶åˆå¹¶è„šæœ¬ - å¢å¼ºEPGæ”¯æŒ
1. ä¸‹è½½BB.m3uï¼ˆåŒ…å«EPGä¿¡æ¯ï¼‰
2. ä»Cloudflareä»£ç†è·å–å†…å®¹
3. æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHKï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—
4. æå–4gtvå‰30ä¸ªç›´æ’­ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“
5. åˆå¹¶ç”ŸæˆCC.m3uï¼ŒåŒ…å«å¤šä¸ªEPGæº
6. ä¸‹è½½å¹¶åˆå¹¶å¤šä¸ªEPGæºï¼Œç”Ÿæˆæ–°çš„EPGæ–‡ä»¶
åŒ—äº¬æ—¶é—´æ¯å¤©6:00ã€17:00è‡ªåŠ¨è¿è¡Œ
"""

import requests
import re
import os
import time
import gzip
import io
from datetime import datetime
import xml.etree.ElementTree as ET
from collections import defaultdict

# é…ç½®
BB_URL = "https://raw.githubusercontent.com/sufernnet/joker/main/BB.m3u"
CLOUDFLARE_PROXY = "https://smt-proxy.sufern001.workers.dev/"
OUTPUT_FILE = "CC.m3u"
EPG_OUTPUT_FILE = "merged_epg.xml"
EPG_SOURCES = [
    "https://epg.112114.xyz/pp.xml",
    "https://epg.946985.filegear-sg.me/t.xml.gz",
    "http://epg.51zmt.top:8000/e.xml"
]

# éœ€è¦è¿‡æ»¤æ‰çš„TWé¢‘é“å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
BLACKLIST_TW = [
    "Bloomberg TV",
    "Bloomberg",
    "SBNå…¨çƒè´¢ç»å°",
    "SBNè´¢ç»",
    "FRANCE24è‹±æ–‡å°",
    "FRANCE24",
    "åŠå²›å›½é™…æ–°é—»å°",
    "åŠå³¶å›½é™…",
    "NHK world-japan",
    "NHK world",
    "SBN",
    "æ—¥æœ¬",
    "NHK",
    "CNBC Asia",
    "CNBC"
]

# HKé¢‘é“ä¼˜å…ˆé¡ºåºï¼ˆæŒ‰è¿™ä¸ªé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢ï¼‰
HK_PRIORITY_ORDER = [
    "å‡¤å‡°ä¸­æ–‡",
    "å‡¤å‡°èµ„è®¯", 
    "å‡¤å‡°é¦™æ¸¯",
    "NOWæ–°é—»å°",
    "NOWæ˜Ÿå½±",
    "NOWçˆ†è°·"
]

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def download_epg_source(url):
    """ä¸‹è½½EPGæº"""
    try:
        log(f"ä¸‹è½½EPGæº: {url}")
        headers = {
            'User-Agent': 'Mozilla/5.0',
            'Accept': '*/*',
            'Accept-Encoding': 'gzip, deflate'
        }
        
        response = requests.get(url, headers=headers, timeout=15, stream=True)
        
        if response.status_code == 200:
            # æ£€æŸ¥æ˜¯å¦ä¸ºgzipå‹ç¼©
            content_type = response.headers.get('content-type', '').lower()
            content_encoding = response.headers.get('content-encoding', '').lower()
            
            if url.endswith('.gz') or 'gzip' in content_encoding or 'application/gzip' in content_type:
                # è§£å‹gzipå†…å®¹
                log(f"  æ£€æµ‹åˆ°gzipå‹ç¼©ï¼Œæ­£åœ¨è§£å‹...")
                with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as gz_file:
                    content = gz_file.read().decode('utf-8', errors='ignore')
            else:
                content = response.text
            
            if content and len(content) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å®¹
                log(f"  âœ… ä¸‹è½½æˆåŠŸ ({len(content)} å­—ç¬¦)")
                return content
            else:
                log(f"  âš ï¸  å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º")
                return None
        else:
            log(f"  âŒ ä¸‹è½½å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
            return None
            
    except Exception as e:
        log(f"  âŒ ä¸‹è½½å¤±è´¥: {e}")
        return None

def merge_epg_sources():
    """åˆå¹¶å¤šä¸ªEPGæº"""
    log(f"å¼€å§‹åˆå¹¶EPGæºï¼Œå…± {len(EPG_SOURCES)} ä¸ª")
    
    all_channels = defaultdict(list)  # channel_id -> [programmes]
    channel_info = {}  # channel_id -> channel_info
    
    for i, epg_url in enumerate(EPG_SOURCES, 1):
        log(f"\nå¤„ç†EPGæº {i}/{len(EPG_SOURCES)}: {epg_url}")
        content = download_epg_source(epg_url)
        
        if not content:
            continue
        
        try:
            # æ¸…ç†XMLï¼Œç§»é™¤æ— æ•ˆå­—ç¬¦
            content_clean = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', content)
            
            # å°è¯•è§£æXML
            root = ET.fromstring(content_clean)
            
            # æå–é¢‘é“ä¿¡æ¯
            channels = 0
            programmes = 0
            
            for element in root:
                if element.tag == 'channel':
                    # æå–é¢‘é“ä¿¡æ¯
                    channel_id = element.get('id')
                    if channel_id:
                        channel_info[channel_id] = ET.tostring(element, encoding='unicode')
                        channels += 1
                
                elif element.tag == 'programme':
                    # æå–èŠ‚ç›®ä¿¡æ¯
                    channel_id = element.get('channel')
                    if channel_id:
                        all_channels[channel_id].append(ET.tostring(element, encoding='unicode'))
                        programmes += 1
            
            log(f"  è§£ææˆåŠŸ: {channels} ä¸ªé¢‘é“, {programmes} ä¸ªèŠ‚ç›®")
            
        except ET.ParseError as e:
            log(f"  âš ï¸  XMLè§£æå¤±è´¥: {e}")
            # å°è¯•ä¿®å¤å¸¸è§é—®é¢˜
            try:
                # ç§»é™¤æ— æ•ˆçš„XMLå£°æ˜
                content_fixed = re.sub(r'<\?xml[^>]*\?>', '', content_clean)
                content_fixed = f'<?xml version="1.0" encoding="UTF-8"?><tv>{content_fixed}</tv>'
                
                root = ET.fromstring(content_fixed)
                channels = 0
                programmes = 0
                
                for element in root:
                    if element.tag == 'channel':
                        channel_id = element.get('id')
                        if channel_id:
                            channel_info[channel_id] = ET.tostring(element, encoding='unicode')
                            channels += 1
                    
                    elif element.tag == 'programme':
                        channel_id = element.get('channel')
                        if channel_id:
                            all_channels[channel_id].append(ET.tostring(element, encoding='unicode'))
                            programmes += 1
                
                log(f"  ä¿®å¤åè§£ææˆåŠŸ: {channels} ä¸ªé¢‘é“, {programmes} ä¸ªèŠ‚ç›®")
                
            except Exception as e2:
                log(f"  âŒ ä¿®å¤åä»ç„¶è§£æå¤±è´¥: {e2}")
                continue
    
    # ç”Ÿæˆåˆå¹¶åçš„EPG
    log(f"\nç”Ÿæˆåˆå¹¶åçš„EPG...")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_channels = len(channel_info)
    total_programmes = sum(len(progs) for progs in all_channels.values())
    
    # åˆ›å»ºXMLå¤´éƒ¨
    merged_epg = '''<?xml version="1.0" encoding="UTF-8"?>
<tv generator-info-name="Merged EPG" generator-info-url="https://github.com/your-repo">
<!-- 
  åˆå¹¶EPGæºä¿¡æ¯:
  â€¢ https://epg.112114.xyz/pp.xml
  â€¢ https://epg.946985.filegear-sg.me/t.xml.gz
  â€¢ http://epg.51zmt.top:8000/e.xml
  
  ç”Ÿæˆæ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
  æ›´æ–°é¢‘ç‡: æ¯å¤© 06:00 å’Œ 17:00 (åŒ—äº¬æ—¶é—´)
  é¢‘é“æ€»æ•°: {channels}
  èŠ‚ç›®æ€»æ•°: {programmes}
-->
'''.format(
        timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        channels=total_channels,
        programmes=total_programmes
    )
    
    # æ·»åŠ é¢‘é“ä¿¡æ¯
    if channel_info:
        merged_epg += "\n<!-- é¢‘é“ä¿¡æ¯ -->\n"
        for channel_id, channel_xml in sorted(channel_info.items()):
            merged_epg += channel_xml + "\n"
    
    # æ·»åŠ èŠ‚ç›®ä¿¡æ¯
    if all_channels:
        merged_epg += "\n<!-- èŠ‚ç›®ä¿¡æ¯ -->\n"
        for channel_id in sorted(all_channels.keys()):
            for programme_xml in all_channels[channel_id]:
                merged_epg += programme_xml + "\n"
    
    # å…³é—­XMLæ ‡ç­¾
    merged_epg += "</tv>"
    
    # ä¿å­˜åˆå¹¶åçš„EPGæ–‡ä»¶
    with open(EPG_OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(merged_epg)
    
    log(f"âœ… EPGåˆå¹¶å®Œæˆ: {EPG_OUTPUT_FILE}")
    log(f"   é¢‘é“æ•°: {total_channels}")
    log(f"   èŠ‚ç›®æ•°: {total_programmes}")
    log(f"   æ–‡ä»¶å¤§å°: {len(merged_epg)} å­—ç¬¦")
    
    # è¿”å›å¯è®¿é—®çš„URLï¼ˆå‡è®¾éƒ¨ç½²åœ¨GitHub Pagesæˆ–åŒä¸€ç›®å½•ä¸‹ï¼‰
    return EPG_OUTPUT_FILE

def download_bb_m3u():
    """ä¸‹è½½BB.m3uå¹¶æå–EPG"""
    try:
        log("ä¸‹è½½BB.m3u...")
        response = requests.get(BB_URL, timeout=10)
        response.raise_for_status()
        
        bb_content = response.text
        log(f"âœ… BB.m3uä¸‹è½½æˆåŠŸ ({len(bb_content)} å­—ç¬¦)")
        
        return bb_content
        
    except Exception as e:
        log(f"âŒ BB.m3uä¸‹è½½å¤±è´¥: {e}")
        return None

def get_content_from_proxy():
    """ä»Cloudflareä»£ç†è·å–å†…å®¹"""
    try:
        log("ä»Cloudflareä»£ç†è·å–å†…å®¹...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://smart.946985.filegear-sg.me/'
        }
        
        response = requests.get(CLOUDFLARE_PROXY, headers=headers, timeout=15)
        
        if response.status_code == 200:
            content = response.text
            
            # å¦‚æœæ˜¯HTMLï¼Œå°è¯•æå–M3Uå†…å®¹
            if '<html' in content.lower():
                # æŸ¥æ‰¾M3Uå†…å®¹
                m3u_match = re.search(r'(#EXTM3U.*?)(?:</pre>|</code>|$)', content, re.DOTALL)
                if m3u_match:
                    content = m3u_match.group(1).strip()
                    log("âœ… ä»HTMLæå–åˆ°M3Uå†…å®¹")
                else:
                    # æå–æ‰€æœ‰å¯èƒ½çš„é¢‘é“è¡Œ
                    lines = content.split('\n')
                    m3u_lines = []
                    for line in lines:
                        line = line.strip()
                        if line.startswith('#EXTINF:') or ('://' in line and not line.startswith('<')):
                            m3u_lines.append(line)
                    
                    if m3u_lines:
                        content = '#EXTM3U\n' + '\n'.join(m3u_lines)
                        log(f"âœ… ä»HTMLæå–åˆ° {len(m3u_lines)} ä¸ªé¢‘é“è¡Œ")
            
            if content and content.strip():
                log(f"âœ… è·å–åˆ°å†…å®¹ ({len(content)} å­—ç¬¦)")
                return content
            else:
                log("âš ï¸  å†…å®¹ä¸ºç©º")
        else:
            log(f"âŒ ä»£ç†è¿”å›é”™è¯¯: {response.status_code}")
            
    except Exception as e:
        log(f"âŒ ä»£ç†è®¿é—®å¤±è´¥: {e}")
    
    return None

def get_channel_priority(channel_name):
    """è·å–é¢‘é“çš„ä¼˜å…ˆçº§ï¼ˆè¶Šå°è¶Šé å‰ï¼‰"""
    channel_name_lower = channel_name.lower()
    
    for i, priority_channel in enumerate(HK_PRIORITY_ORDER):
        if priority_channel.lower() in channel_name_lower:
            return i  # è¿”å›ä¼˜å…ˆçº§ç´¢å¼•ï¼Œè¶Šå°è¶Šé å‰
    
    return len(HK_PRIORITY_ORDER)  # éä¼˜å…ˆé¢‘é“æ’åœ¨æœ€å

def extract_and_sort_hk_channels(content):
    """æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHKï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—"""
    if not content:
        return []
    
    log("æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHKï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—...")
    log(f"HKä¼˜å…ˆé¡ºåº: {', '.join(HK_PRIORITY_ORDER)}")
    
    # è§£æM3Uå†…å®¹
    lines = content.split('\n')
    channels = []
    current_extinf = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            current_extinf = line
        elif current_extinf and '://' in line and not line.startswith('#'):
            channels.append((current_extinf, line))
            current_extinf = None
    
    # è¿‡æ»¤JULIé¢‘é“å¹¶é‡å‘½å
    hk_channels_with_priority = []
    seen = set()
    
    for extinf, url in channels:
        if 'JULI' in extinf.upper():
            # æå–åŸå§‹é¢‘é“å
            channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
            
            # é‡å‘½åä¸ºHKåˆ†ç»„
            new_extinf = re.sub(r'JULI', 'HK', extinf, flags=re.IGNORECASE)
            
            # ç¡®ä¿group-titleä¸ºHK
            if 'group-title=' in new_extinf:
                new_extinf = re.sub(r'group-title="[^"]*"', 'group-title="HK"', new_extinf)
            else:
                # æ·»åŠ group-title
                if ',' in new_extinf:
                    parts = new_extinf.split(',', 1)
                    new_extinf = f'{parts[0]} group-title="HK",{parts[1]}'
            
            # å»é‡
            key = f"{new_extinf}|{url}"
            if key not in seen:
                seen.add(key)
                # è®¡ç®—ä¼˜å…ˆçº§
                priority = get_channel_priority(channel_name)
                hk_channels_with_priority.append((priority, new_extinf, url, channel_name))
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    hk_channels_with_priority.sort(key=lambda x: x[0])
    
    # æå–æ’åºåçš„é¢‘é“
    hk_channels = [(extinf, url) for _, extinf, url, _ in hk_channels_with_priority]
    
    log(f"âœ… æå–åˆ° {len(hk_channels)} ä¸ªHKé¢‘é“ï¼ˆåŸJULIï¼‰")
    
    return hk_channels

def should_skip_channel(channel_name):
    """æ£€æŸ¥é¢‘é“æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤"""
    channel_name_lower = channel_name.lower()
    
    # æ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
    for black_word in BLACKLIST_TW:
        if black_word.lower() in channel_name_lower:
            log(f"  è¿‡æ»¤æ‰: {channel_name} (åŒ…å«: {black_word})")
            return True
    
    return False

def extract_filtered_4gtv_channels(content, limit=30):
    """æå–4gtvé¢‘é“ï¼ˆå‰30ä¸ªï¼‰ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“"""
    if not content:
        return []
    
    log(f"æå–4gtvå‰{limit}ä¸ªç›´æ’­ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“...")
    log(f"è¿‡æ»¤åˆ—è¡¨: {', '.join(BLACKLIST_TW)}")
    
    # è§£æM3Uå†…å®¹
    lines = content.split('\n')
    channels = []
    current_extinf = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            current_extinf = line
        elif current_extinf and '://' in line and not line.startswith('#'):
            channels.append((current_extinf, line))
            current_extinf = None
    
    # è¿‡æ»¤4gtvé¢‘é“ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    filtered_channels = []
    for extinf, url in channels:
        if '4gtv' in extinf.lower():
            filtered_channels.append((extinf, url))
    
    log(f"æ‰¾åˆ° {len(filtered_channels)} ä¸ª4gtvé¢‘é“")
    
    # è¿‡æ»¤é»‘åå•é¢‘é“
    filtered_by_blacklist = []
    for extinf, url in filtered_channels:
        # æå–é¢‘é“å
        channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        if not should_skip_channel(channel_name):
            filtered_by_blacklist.append((extinf, url))
        else:
            log(f"  â›” è¿‡æ»¤: {channel_name}")
    
    log(f"è¿‡æ»¤åå‰©ä½™ {len(filtered_by_blacklist)} ä¸ª4gtvé¢‘é“")
    
    # åªå–å‰limitä¸ª
    if len(filtered_by_blacklist) > limit:
        filtered_by_blacklist = filtered_by_blacklist[:limit]
        log(f"åªå–å‰ {limit} ä¸ªè¿‡æ»¤åçš„4gtvé¢‘é“")
    
    # é‡å‘½åä¸ºTWåˆ†ç»„
    tw_channels = []
    seen = set()
    
    for extinf, url in filtered_by_blacklist:
        # æ›¿æ¢åˆ†ç»„ä¸ºTW
        new_extinf = extinf
        
        # æ›¿æ¢4gtvä¸ºTWï¼ˆåœ¨é¢‘é“åä¸­ï¼‰
        if '4gtv' in new_extinf.lower():
            new_extinf = re.sub(r'4gtv', 'TW', new_extinf, flags=re.IGNORECASE)
        
        # ç¡®ä¿group-titleä¸ºTW
        if 'group-title=' in new_extinf:
            new_extinf = re.sub(r'group-title="[^"]*"', 'group-title="TW"', new_extinf)
        else:
            # æ·»åŠ group-title
            if ',' in new_extinf:
                parts = new_extinf.split(',', 1)
                new_extinf = f'{parts[0]} group-title="TW",{parts[1]}'
        
        # å»é‡
        key = f"{new_extinf}|{url}"
        if key not in seen:
            seen.add(key)
            tw_channels.append((new_extinf, url))
    
    log(f"âœ… æå–åˆ° {len(tw_channels)} ä¸ªTWé¢‘é“ï¼ˆåŸ4gtvï¼Œå·²è¿‡æ»¤ï¼‰")
    
    return tw_channels

def main():
    """ä¸»å‡½æ•°"""
    log("å¼€å§‹åˆå¹¶M3Uæ–‡ä»¶...")
    
    # æ˜¾ç¤ºå½“å‰æ—¶é—´ï¼ˆç”¨äºè°ƒè¯•å®šæ—¶ä»»åŠ¡ï¼‰
    current_time = datetime.now()
    log(f"å½“å‰æ—¶é—´: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
    log(f"ä¸‹æ¬¡è¿è¡Œ: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 17:00")
    log(f"HKä¼˜å…ˆé¡ºåº: {', '.join(HK_PRIORITY_ORDER)}")
    log(f"TWé¢‘é“è¿‡æ»¤åˆ—è¡¨: {', '.join(BLACKLIST_TW)}")
    log(f"EPGæºåˆ—è¡¨: {', '.join(EPG_SOURCES)}")
    
    # 1. ä¸‹è½½å¹¶åˆå¹¶EPGæº
    log("\n" + "="*50)
    log("æ­¥éª¤1: åˆå¹¶EPGæº")
    merged_epg_url = merge_epg_sources()
    
    # 2. ä¸‹è½½BB.m3u
    log("\n" + "="*50)
    log("æ­¥éª¤2: ä¸‹è½½BB.m3u")
    bb_content = download_bb_m3u()
    if not bb_content:
        log("âŒ æ— æ³•ç»§ç»­ï¼ŒBB.m3uä¸‹è½½å¤±è´¥")
        return
    
    # 3. ä»ä»£ç†è·å–å†…å®¹
    log("\n" + "="*50)
    log("æ­¥éª¤3: ä»ä»£ç†è·å–å†…å®¹")
    proxy_content = get_content_from_proxy()
    
    # 4. å…ˆæå–HKé¢‘é“ï¼ˆJULIï¼‰- æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢
    log("\n" + "="*50)
    log("æ­¥éª¤4: æå–HKé¢‘é“")
    hk_channels = []
    if proxy_content:
        hk_channels = extract_and_sort_hk_channels(proxy_content)
    else:
        log("âš ï¸  æ— æ³•ä»ä»£ç†è·å–å†…å®¹ï¼Œè·³è¿‡HKé¢‘é“")
    
    # 5. å†æå–TWé¢‘é“ï¼ˆ4gtvå‰30ä¸ªï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“ï¼‰- æ’åœ¨åé¢
    log("\n" + "="*50)
    log("æ­¥éª¤5: æå–TWé¢‘é“")
    tw_channels = []
    if proxy_content:
        tw_channels = extract_filtered_4gtv_channels(proxy_content, limit=30)
    else:
        log("âš ï¸  æ— æ³•ä»ä»£ç†è·å–å†…å®¹ï¼Œè·³è¿‡TWé¢‘é“")
    
    # 6. æ„å»ºM3Uå†…å®¹
    log("\n" + "="*50)
    log("æ­¥éª¤6: æ„å»ºM3Uæ–‡ä»¶")
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # M3Uå¤´éƒ¨ï¼ˆä½¿ç”¨åˆå¹¶åçš„EPGï¼‰
    m3u_header = f'#EXTM3U url-tvg="{merged_epg_url}"\n'
    log(f"âœ… ä½¿ç”¨åˆå¹¶åçš„EPG: {merged_epg_url}")
    
    output = m3u_header + f"""# è‡ªåŠ¨åˆå¹¶ M3U æ–‡ä»¶
# ç”Ÿæˆæ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# ä¸‹æ¬¡æ›´æ–°: æ¯å¤© 06:00 å’Œ 17:00 (åŒ—äº¬æ—¶é—´)
# BBæº: {BB_URL}
# ä»£ç†æº: {CLOUDFLARE_PROXY}
# JULIåˆ†ç»„å·²æ”¹ä¸ºHK (æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢)
# HKä¼˜å…ˆé¡ºåº: {', '.join(HK_PRIORITY_ORDER)}
# 4gtvåˆ†ç»„å·²æ”¹ä¸ºTW (å‰30ä¸ªï¼Œæ’åœ¨åé¢ï¼Œå·²è¿‡æ»¤æŒ‡å®šé¢‘é“)
# è¿‡æ»¤é¢‘é“: {', '.join(BLACKLIST_TW)}
# EPGæº: {merged_epg_url}
# åˆå¹¶çš„EPGæº: {len(EPG_SOURCES)} ä¸ª
#      {EPG_SOURCES[0]}
#      {EPG_SOURCES[1]}
#      {EPG_SOURCES[2]}
# GitHub Actions è‡ªåŠ¨ç”Ÿæˆ

"""
    
    # æ·»åŠ BBå†…å®¹ï¼ˆè·³è¿‡ç¬¬ä¸€è¡Œï¼‰
    bb_lines = bb_content.split('\n')
    bb_count = 0
    skip_first = True
    
    for line in bb_lines:
        line = line.rstrip()
        if not line:
            continue
        
        if skip_first and line.startswith('#EXTM3U'):
            skip_first = False
            continue
        
        output += line + '\n'
        if line.startswith('#EXTINF:'):
            bb_count += 1
    
    # æ·»åŠ HKé¢‘é“ï¼ˆJULIï¼‰- æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢
    if hk_channels:
        output += f"\n# HKé¢‘é“ (åŸJULIï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢)\n"
        output += f"# ä¼˜å…ˆé¡ºåº: {', '.join(HK_PRIORITY_ORDER)}\n"
        
        # æ˜¾ç¤ºä¼˜å…ˆé¢‘é“
        priority_added = False
        for channel_type in HK_PRIORITY_ORDER:
            type_channels = [(extinf, url) for extinf, url in hk_channels if channel_type.lower() in extinf.lower()]
            if type_channels:
                if not priority_added:
                    output += f"# --- ä¼˜å…ˆé¢‘é“ ---\n"
                    priority_added = True
                
                for extinf, url in type_channels:
                    output += extinf + '\n'
                    output += url + '\n'
        
        # æ˜¾ç¤ºå…¶ä»–HKé¢‘é“
        other_hk_channels = [(extinf, url) for extinf, url in hk_channels 
                           if not any(channel_type.lower() in extinf.lower() for channel_type in HK_PRIORITY_ORDER)]
        
        if other_hk_channels:
            output += f"# --- å…¶ä»–HKé¢‘é“ ---\n"
            for extinf, url in other_hk_channels:
                output += extinf + '\n'
                output += url + '\n'
    
    # æ·»åŠ TWé¢‘é“ï¼ˆ4gtvï¼‰- æ’åœ¨åé¢ï¼ˆå·²è¿‡æ»¤ï¼‰
    if tw_channels:
        output += f"\n# TWé¢‘é“ (åŸ4gtvï¼Œå‰30ä¸ªï¼Œå·²è¿‡æ»¤æŒ‡å®šé¢‘é“ï¼Œæ’åœ¨HKä¹‹å)\n"
        output += f"# å·²è¿‡æ»¤: {', '.join(BLACKLIST_TW)}\n"
        for extinf, url in tw_channels:
            output += extinf + '\n'
            output += url + '\n'
    
    # æ·»åŠ EPGä¿¡æ¯è¯´æ˜
    output += f"""
# EPGä¿¡æ¯
# ä½¿ç”¨åˆå¹¶åçš„EPG: {merged_epg_url}
# åˆå¹¶çš„EPGæº ({len(EPG_SOURCES)}ä¸ª):"""
    for i, epg in enumerate(EPG_SOURCES, 1):
        output += f"\n#      {epg}"
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    output += f"""
# ç»Ÿè®¡ä¿¡æ¯
# BB é¢‘é“æ•°: {bb_count}
# HK é¢‘é“æ•°: {len(hk_channels)} (åŸJULIï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—)
# TW é¢‘é“æ•°: {len(tw_channels)} (åŸ4gtvå‰30ä¸ªï¼Œå·²è¿‡æ»¤ï¼Œæ’åœ¨å)
# è¿‡æ»¤é¢‘é“: {len(BLACKLIST_TW)} ä¸ª
# æ€»é¢‘é“æ•°: {bb_count + len(hk_channels) + len(tw_channels)}
# EPGçŠ¶æ€: âœ… åˆå¹¶ {len(EPG_SOURCES)} ä¸ªEPGæº
# æ›´æ–°æ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# æ›´æ–°é¢‘ç‡: æ¯å¤© 06:00 å’Œ 17:00 (åŒ—äº¬æ—¶é—´)
# æ’åºè§„åˆ™: BB â†’ HK(å‡¤å‡°/NOWä¼˜å…ˆ) â†’ TW(å·²è¿‡æ»¤)
"""
    
    # 7. ä¿å­˜æ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(output)
    
    log(f"\n" + "="*50)
    log("ğŸ‰ åˆå¹¶å®Œæˆ!")
    log(f"ğŸ“ M3Uæ–‡ä»¶: {OUTPUT_FILE}")
    log(f"ğŸ“ M3Uå¤§å°: {len(output)} å­—ç¬¦")
    log(f"ğŸ“ EPGæ–‡ä»¶: {EPG_OUTPUT_FILE}")
    log(f"ğŸ“¡ EPGçŠ¶æ€: âœ… åˆå¹¶ {len(EPG_SOURCES)} ä¸ªEPGæº")
    log(f"ğŸ“º BBé¢‘é“: {bb_count}")
    log(f"ğŸ“º HKé¢‘é“: {len(hk_channels)} (æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—)")
    log(f"ğŸ“º TWé¢‘é“: {len(tw_channels)} (å·²è¿‡æ»¤æŒ‡å®šé¢‘é“)")
    log(f"ğŸ“º æ€»è®¡: {bb_count + len(hk_channels) + len(tw_channels)}")
    log(f"ğŸ•’ ä¸‹æ¬¡è‡ªåŠ¨æ›´æ–°: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 17:00")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if os.path.exists(EPG_OUTPUT_FILE):
        epg_size = os.path.getsize(EPG_OUTPUT_FILE)
        log(f"ğŸ“Š EPGæ–‡ä»¶å¤§å°: {epg_size} å­—èŠ‚ ({epg_size/1024:.1f} KB)")
    else:
        log(f"âš ï¸  EPGæ–‡ä»¶æœªç”Ÿæˆ")

if __name__ == "__main__":
    main()
