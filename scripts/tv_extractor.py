#!/usr/bin/env python3
"""
ä»ä¸¤ä¸ªTVæºä¸­æå–HKå’ŒTWé¢‘é“ï¼Œæ ¡éªŒæ’­æ”¾çŠ¶æ€åä¸BB.m3uåˆå¹¶
æ”¯æŒé¢‘é“è¿‡æ»¤å’Œæ’åº
"""

import requests
import re
import os
import sys
import time
import subprocess
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å¸¸é‡å®šä¹‰
HK_SOURCE_URL = "https://hacks.sufern001.workers.dev/?type=hk"
TW_SOURCE_URL = "https://hacks.sufern001.workers.dev/?type=tw"
EPG_URL = "http://epg.51zmt.top:8000/e.xml"
BB_FILE = "BB.m3u"
OUTPUT_FILE = "EE.m3u"
TIMEOUT = 10  # æ’­æ”¾æ ¡éªŒè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
MAX_WORKERS = 5  # å¹¶å‘æ ¡éªŒæœ€å¤§çº¿ç¨‹æ•°
MAX_RETRIES = 2  # æœ€å¤§é‡è¯•æ¬¡æ•°

# HKé¢‘é“é»‘åå•ï¼ˆè¦è¿‡æ»¤æ‰çš„é¢‘é“ï¼‰
HK_BLACKLIST = [
    'snaap',
    'C+',
    'ç”„å­ä¸¹',
    'SNAAP',
    'C+',
]

# TWé¢‘é“é»‘åå•ï¼ˆè¦è¿‡æ»¤æ‰çš„é¢‘é“ï¼‰
TW_BLACKLIST = [
    'åœ‹æœƒé »é“',
    'åŸä½æ°‘',
    'liveABC',
    'UDN TV',
    'rollor',
    'C+é »é“',
    'MOMO',
    'å¤§æ„›',
    'å¥½è¨Šæ¯',
    'Smith',
    'FOX MOVIES',
    'PETP',
    'åœ‹æœƒ',
    'åŸæ°‘',
    'ABC',
    'UDN',
    'rollor',
    'Momo',
    'å¥½è¨Šæ¯ 1',
    'å¥½è¨Šæ¯ 2',
    'FOX MOVIE',
    'PET CLUB TV'
]

# å‡¤å‡°é¢‘é“å…³é”®è¯ï¼ˆç”¨äºæ’åºï¼‰
PHOENIX_KEYWORDS = ['é³³å‡°', 'å‡¤å‡°']

# NOWé¢‘é“å…³é”®è¯ï¼ˆç”¨äºæ’åºï¼‰
NOW_KEYWORDS = ['NOW']

def fetch_m3u_content(url, source_name, retry_count=MAX_RETRIES):
    """è·å–M3Uæ–‡ä»¶å†…å®¹"""
    for attempt in range(retry_count):
        try:
            logger.info(f"æ­£åœ¨ä» {source_name} ä¸‹è½½M3Uæ–‡ä»¶ (å°è¯• {attempt+1}/{retry_count})...")
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            content = response.text
            
            if not content.strip().startswith("#EXTM3U"):
                logger.warning(f"{source_name} å†…å®¹å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„M3Uæ ¼å¼")
                
            logger.info(f"{source_name} ä¸‹è½½æˆåŠŸï¼Œå¤§å°: {len(content)} å­—ç¬¦")
            return content
        except requests.RequestException as e:
            logger.error(f"ä¸‹è½½ {source_name} å¤±è´¥ (å°è¯• {attempt+1}/{retry_count}): {e}")
            if attempt < retry_count - 1:
                wait_time = (attempt + 1) * 2  # é€’å¢ç­‰å¾…æ—¶é—´
                logger.info(f"{wait_time}ç§’åé‡è¯•...")
                time.sleep(wait_time)
    
    logger.error(f"{source_name} ä¸‹è½½å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
    return None

def read_bb_file():
    """è¯»å–BB.m3uæ–‡ä»¶å†…å®¹"""
    try:
        # å…ˆå°è¯•åœ¨å½“å‰ç›®å½•æŸ¥æ‰¾
        if os.path.exists(BB_FILE):
            bb_path = BB_FILE
        else:
            # å°è¯•åœ¨è„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šçº§ç›®å½•æŸ¥æ‰¾
            script_dir = os.path.dirname(os.path.abspath(__file__))
            bb_path = os.path.join(script_dir, "..", BB_FILE)
        
        if os.path.exists(bb_path):
            with open(bb_path, 'r', encoding='utf-8') as f:
                content = f.read()
            logger.info(f"è¯»å–BB.m3uæˆåŠŸï¼Œå¤§å°: {len(content)} å­—ç¬¦")
            return content
        else:
            logger.warning(f"BB.m3uæ–‡ä»¶ä¸å­˜åœ¨: {bb_path}")
            # å°è¯•å…¶ä»–å¯èƒ½ä½ç½®
            possible_paths = [
                BB_FILE,
                f"../{BB_FILE}",
                f"../../{BB_FILE}",
                os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", BB_FILE),
                os.path.join(os.path.dirname(os.path.abspath(__file__)), BB_FILE)
            ]
            
            for path in possible_paths:
                if os.path.exists(path):
                    with open(path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    logger.info(f"ä» {path} è¯»å–BB.m3uæˆåŠŸ")
                    return content
            
            logger.error(f"åœ¨æ‰€æœ‰å¯èƒ½ä½ç½®éƒ½æ‰¾ä¸åˆ°BB.m3uæ–‡ä»¶")
            logger.info("å°†åˆ›å»ºä¸åŒ…å«BB.m3uçš„é¢‘é“åˆ—è¡¨")
            return None
    except Exception as e:
        logger.error(f"è¯»å–BB.m3uå¤±è´¥: {e}")
        return None

def parse_m3u_content(content, default_group):
    """è§£æM3Uå†…å®¹ï¼Œè¿”å›é¢‘é“åˆ—è¡¨"""
    if not content:
        return []
    
    channels = []
    lines = content.split('\n')
    i = 0
    
    while i < len(lines):
        line = lines[i].strip()
        
        if not line:
            i += 1
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é¢‘é“ä¿¡æ¯è¡Œ
        if line.startswith("#EXTINF"):
            # æå–é¢‘é“ä¿¡æ¯
            extinf_line = line
            
            # æŸ¥æ‰¾å¯¹åº”çš„URLè¡Œ
            j = i + 1
            url_line = ""
            while j < len(lines):
                temp_line = lines[j].strip()
                if not temp_line:
                    j += 1
                    continue
                if temp_line.startswith("#EXTINF"):
                    break
                if temp_line and not temp_line.startswith("#"):
                    url_line = temp_line
                    break
                j += 1
            
            if url_line:
                # æå–é¢‘é“åç§°
                channel_name = "æœªçŸ¥é »é“"  # é»˜è®¤ç¹ä½“
                name_match = re.search(r',([^,]+)$', extinf_line)
                if name_match:
                    channel_name = name_match.group(1).strip()
                
                # æå–åŸå§‹åˆ†ç»„
                original_group = default_group
                group_match = re.search(r'group-title="([^"]+)"', extinf_line)
                if group_match:
                    original_group = group_match.group(1)
                
                # æå–tvg-idï¼ˆå¦‚æœæœ‰ï¼‰
                tvg_id = ""
                tvg_match = re.search(r'tvg-id="([^"]+)"', extinf_line)
                if tvg_match:
                    tvg_id = tvg_match.group(1)
                
                # æå–tvg-logoï¼ˆå¦‚æœæœ‰ï¼‰
                tvg_logo = ""
                logo_match = re.search(r'tvg-logo="([^"]+)"', extinf_line)
                if logo_match:
                    tvg_logo = logo_match.group(1)
                
                # åˆ›å»ºæ–°çš„EXTINFè¡Œï¼Œç»Ÿä¸€åˆ†ç»„ä½†ä¿ç•™å…¶ä»–å±æ€§
                new_extinf = extinf_line
                
                # å¦‚æœæœ‰group-titleï¼Œæ›¿æ¢å®ƒ
                if 'group-title=' in new_extinf:
                    new_extinf = re.sub(r'group-title="[^"]+"', f'group-title="{default_group}"', new_extinf)
                else:
                    # å¦‚æœåŸæ¥æ²¡æœ‰åˆ†ç»„ä¿¡æ¯ï¼Œæ·»åŠ åˆ†ç»„
                    if ': ' in new_extinf:
                        new_extinf = new_extinf.replace('#EXTINF:', f'#EXTINF: group-title="{default_group}",', 1)
                    else:
                        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å±æ€§
                        attr_match = re.match(r'#EXTINF:(-?\d+)\s+(.+)', new_extinf)
                        if attr_match:
                            duration = attr_match.group(1)
                            attrs = attr_match.group(2)
                            new_extinf = f'#EXTINF:{duration} group-title="{default_group}",{attrs.split(",")[-1]}'
                        else:
                            new_extinf = f'#EXTINF:-1 group-title="{default_group}",{channel_name}'
                
                channel_data = {
                    'original_extinf': extinf_line,
                    'extinf': new_extinf,
                    'url': url_line,
                    'name': channel_name,
                    'group': default_group,
                    'original_group': original_group,
                    'tvg_id': tvg_id,
                    'tvg_logo': tvg_logo,
                    'working': None  # æ˜¯å¦å¯æ’­æ”¾ï¼ŒNoneè¡¨ç¤ºæœªæ£€æŸ¥
                }
                channels.append(channel_data)
        
        i += 1
    
    return channels

def filter_and_sort_channels(channels, blacklist, group_name):
    """è¿‡æ»¤å’Œæ’åºé¢‘é“"""
    if not channels:
        return []
    
    logger.info(f"å¼€å§‹è¿‡æ»¤å’Œæ’åº {group_name} é¢‘é“...")
    
    # 1. è¿‡æ»¤é»‘åå•é¢‘é“
    filtered_channels = []
    for channel in channels:
        channel_name = channel['name']
        should_skip = False
        
        for black_word in blacklist:
            if black_word.lower() in channel_name.lower():
                logger.info(f"è¿‡æ»¤é¢‘é“: {channel_name} (åŒ¹é…é»‘åå•: {black_word})")
                should_skip = True
                break
        
        if not should_skip:
            filtered_channels.append(channel)
    
    logger.info(f"è¿‡æ»¤åå‰©ä½™ {len(filtered_channels)} ä¸ª{group_name}é¢‘é“")
    
    # 2. å¦‚æœæ˜¯HKé¢‘é“ï¼Œè¿›è¡Œç‰¹æ®Šæ’åº
    if group_name == "HK":
        # åˆ†ç¦»å‡¤å‡°ã€NOWå’Œå…¶ä»–é¢‘é“
        phoenix_channels = []
        now_channels = []
        other_channels = []
        
        for channel in filtered_channels:
            channel_name = channel['name']
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå‡¤å‡°é¢‘é“
            is_phoenix = any(keyword in channel_name for keyword in PHOENIX_KEYWORDS)
            # æ£€æŸ¥æ˜¯å¦ä¸ºNOWé¢‘é“
            is_now = any(keyword in channel_name for keyword in NOW_KEYWORDS)
            
            if is_phoenix:
                phoenix_channels.append(channel)
            elif is_now:
                now_channels.append(channel)
            else:
                other_channels.append(channel)
        
        # å¯¹å‡¤å‡°é¢‘é“è¿›è¡Œç‰¹å®šæ’åº
        phoenix_order = {
            'é³³å‡°è¡›è¦–': 1,
            'é³³å‡°è³‡è¨ŠHD': 2, 
            'é³³å‡°é¦™æ¸¯': 3,
            'é³³å‡°é›»å½±': 4,
            'å‡¤å‡°ä¸­æ–‡': 1,
            'å‡¤å‡°èµ„è®¯': 2,
            'å‡¤å‡°é¦™æ¸¯': 3,
            'å‡¤å‡°ç”µå½±': 4
        }
        
        def get_phoenix_priority(channel_name):
            for key, priority in phoenix_order.items():
                if key in channel_name:
                    return priority
            return 99  # å…¶ä»–å‡¤å‡°é¢‘é“æ”¾åœ¨åé¢
        
        phoenix_channels.sort(key=lambda x: get_phoenix_priority(x['name']))
        
        # åˆå¹¶æ’åºåçš„é¢‘é“åˆ—è¡¨
        sorted_channels = phoenix_channels + now_channels + other_channels
        
        logger.info(f"HKé¢‘é“æ’åºç»“æœ: å‡¤å‡°{len(phoenix_channels)}ä¸ª, NOW{len(now_channels)}ä¸ª, å…¶ä»–{len(other_channels)}ä¸ª")
        return sorted_channels
    
    # å¯¹äºTWé¢‘é“ï¼Œåªè¿‡æ»¤ä¸æ’åº
    return filtered_channels

def check_stream_playable(url, channel_name, retry_count=1):
    """æ£€æŸ¥æµæ˜¯å¦å¯ä»¥æ’­æ”¾"""
    parsed_url = urlparse(url)
    
    # æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ
    if not parsed_url.scheme:
        logger.debug(f"æ— æ•ˆçš„URLæ ¼å¼: {url}")
        return False
    
    # è·³è¿‡æŸäº›åè®®çš„ç›´æ¥æ£€æŸ¥
    skip_protocols = ['rtmp', 'rtsp', 'udp', 'rtp', 'p2p']
    if parsed_url.scheme in skip_protocols:
        logger.debug(f"è·³è¿‡ {parsed_url.scheme} åè®®æ£€æŸ¥: {channel_name}")
        return True  # å‡è®¾è¿™äº›åè®®å¯æ’­æ”¾
    
    for attempt in range(retry_count):
        try:
            # å¯¹äºHTTP/HTTPSæµï¼Œä½¿ç”¨curlæ£€æŸ¥
            if parsed_url.scheme in ['http', 'https']:
                # å°è¯•HEADè¯·æ±‚
                command = [
                    'curl', '-s', '-o', '/dev/null',
                    '-w', '%{http_code}',
                    '--max-time', str(TIMEOUT),
                    '--head',
                    '--location',  # è·Ÿéšé‡å®šå‘
                    '--user-agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    url
                ]
                
                logger.debug(f"æ£€æŸ¥é¢‘é“ [{attempt+1}/{retry_count}]: {channel_name}")
                
                result = subprocess.run(
                    command,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    timeout=TIMEOUT + 2
                )
                
                if result.returncode == 0:
                    http_code = result.stdout.decode('utf-8', errors='ignore').strip()
                    # 2xx æˆ– 3xx æˆ– 4xxï¼ˆæŸäº›æœåŠ¡å™¨è¿”å›403ä½†æµä»ç„¶å¯ç”¨ï¼‰
                    if http_code.startswith('2') or http_code.startswith('3') or http_code == '403':
                        return True
                    else:
                        logger.debug(f"é¢‘é“ {channel_name} è¿”å›HTTPçŠ¶æ€ç : {http_code}")
                else:
                    stderr = result.stderr.decode('utf-8', errors='ignore')[:100]
                    logger.debug(f"é¢‘é“ {channel_name} curlå‘½ä»¤å¤±è´¥: {stderr}")
            
            # å¦‚æœæ˜¯å…¶ä»–æ”¯æŒçš„åè®®ï¼Œå°è¯•ç®€å•è¿æ¥
            else:
                logger.debug(f"å°è¯•è¿æ¥ {parsed_url.scheme} åè®®: {channel_name}")
                # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–åè®®çš„æ£€æŸ¥é€»è¾‘
                return True  # æš‚æ—¶å‡è®¾å¯è¿æ¥
                
        except subprocess.TimeoutExpired:
            logger.warning(f"é¢‘é“æ£€æŸ¥è¶…æ—¶ [{attempt+1}/{retry_count}]: {channel_name}")
            if attempt < retry_count - 1:
                time.sleep(1)  # é‡è¯•å‰ç­‰å¾…
        except Exception as e:
            logger.warning(f"æ£€æŸ¥é¢‘é“å¤±è´¥ [{attempt+1}/{retry_count}] {channel_name}: {str(e)[:100]}")
            if attempt < retry_count - 1:
                time.sleep(1)
    
    return False

def validate_channels(channels, skip_validation=False):
    """éªŒè¯é¢‘é“æ˜¯å¦å¯ä»¥æ’­æ”¾"""
    if not channels:
        return [], []
    
    if skip_validation:
        logger.info(f"è·³è¿‡éªŒè¯ï¼Œæ ‡è®°æ‰€æœ‰ {len(channels)} ä¸ªé¢‘é“ä¸ºå¯æ’­æ”¾")
        for channel in channels:
            channel['working'] = True
        return channels, []
    
    logger.info(f"å¼€å§‹éªŒè¯ {len(channels)} ä¸ªé¢‘é“çš„æ’­æ”¾çŠ¶æ€...")
    
    valid_channels = []
    invalid_channels = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘éªŒè¯
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_channel = {}
        for channel in channels:
            future = executor.submit(
                check_stream_playable, 
                channel['url'], 
                channel['name'],
                2  # é‡è¯•æ¬¡æ•°
            )
            future_to_channel[future] = channel
        
        completed = 0
        start_time = time.time()
        
        for future in as_completed(future_to_channel):
            channel = future_to_channel[future]
            try:
                is_playable = future.result()
                channel['working'] = is_playable
                
                if is_playable:
                    valid_channels.append(channel)
                    if len(valid_channels) % 10 == 0:
                        logger.info(f"âœ… å·²æ‰¾åˆ° {len(valid_channels)} ä¸ªå¯æ’­æ”¾é¢‘é“")
                else:
                    invalid_channels.append(channel)
                    if len(invalid_channels) % 20 == 0:
                        logger.info(f"âŒ å·²æœ‰ {len(invalid_channels)} ä¸ªä¸å¯æ’­æ”¾é¢‘é“")
                
                completed += 1
                if completed % 20 == 0:
                    elapsed = time.time() - start_time
                    logger.info(f"éªŒè¯è¿›åº¦: {completed}/{len(channels)} (å·²ç”¨ {elapsed:.1f}ç§’)")
                    
            except Exception as e:
                logger.error(f"éªŒè¯é¢‘é“å¼‚å¸¸ {channel['name']}: {e}")
                channel['working'] = False
                invalid_channels.append(channel)
    
    elapsed = time.time() - start_time
    logger.info(f"éªŒè¯å®Œæˆ: {len(valid_channels)} ä¸ªå¯æ’­æ”¾, {len(invalid_channels)} ä¸ªä¸å¯æ’­æ”¾ (ç”¨æ—¶ {elapsed:.1f}ç§’)")
    
    # æŒ‰é¢‘é“åç§°æ’åº
    valid_channels.sort(key=lambda x: x['name'])
    invalid_channels.sort(key=lambda x: x['name'])
    
    return valid_channels, invalid_channels

def build_m3u_content(hk_channels, tw_channels):
    """æ„å»ºM3Uæ–‡ä»¶å†…å®¹"""
    lines = []
    
    # æ·»åŠ æ–‡ä»¶å¤´
    lines.append(f'#EXTM3U url-tvg="{EPG_URL}" x-tvg-url="{EPG_URL}"')
    
    # æ·»åŠ ç”Ÿæˆä¿¡æ¯
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    lines.append(f"# ç”Ÿæˆæ™‚é–“: {timestamp}")
    lines.append(f"# HKæºåœ°å€: {HK_SOURCE_URL}")
    lines.append(f"# TWæºåœ°å€: {TW_SOURCE_URL}")
    lines.append(f"# EPGæº: {EPG_URL}")
    lines.append("# åŒ…å«å…§å®¹: BB.m3u + HKé »é“ + TWé »é“")
    lines.append("# è‡ªå‹•æ›´æ–°é »é“åˆ—è¡¨")
    lines.append("")
    
    # æ·»åŠ HKé¢‘é“
    if hk_channels:
        lines.append("#" + "="*60)
        lines.append("# HKé »é“")
        lines.append("#" + "="*60)
        lines.append("")
        
        for channel in hk_channels:
            lines.append(channel['extinf'])
            lines.append(channel['url'])
        
        lines.append("")
    
    # æ·»åŠ TWé¢‘é“
    if tw_channels:
        lines.append("#" + "="*60)
        lines.append("# TWé »é“")
        lines.append("#" + "="*60)
        lines.append("")
        
        for channel in tw_channels:
            lines.append(channel['extinf'])
            lines.append(channel['url'])
    
    return '\n'.join(lines)

def merge_with_bb(tv_content, bb_content):
    """å°†æå–çš„TVå†…å®¹ä¸BB.m3uåˆå¹¶"""
    if not tv_content and not bb_content:
        return ""
    
    merged_lines = []
    
    # æ·»åŠ æ–‡ä»¶å¤´
    merged_lines.append(f'#EXTM3U url-tvg="{EPG_URL}" x-tvg-url="{EPG_URL}"')
    
    # æ·»åŠ ç”Ÿæˆä¿¡æ¯
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    merged_lines.append(f"# ç”Ÿæˆæ™‚é–“: {timestamp}")
    merged_lines.append(f"# HKæºåœ°å€: {HK_SOURCE_URL}")
    merged_lines.append(f"# TWæºåœ°å€: {TW_SOURCE_URL}")
    merged_lines.append(f"# EPGæº: {EPG_URL}")
    merged_lines.append("# åŒ…å«å…§å®¹: BB.m3u + HKé »é“ + TWé »é“")
    merged_lines.append("# è‡ªå‹•æ›´æ–°é »é“åˆ—è¡¨")
    merged_lines.append("")
    
    # å¦‚æœæœ‰BBå†…å®¹ï¼Œå…ˆæ·»åŠ BBçš„å†…å®¹ï¼ˆè·³è¿‡å…¶æ–‡ä»¶å¤´ï¼‰
    if bb_content:
        bb_lines = bb_content.split('\n')
        bb_count = 0
        bb_section_started = False
        
        for line in bb_lines:
            line = line.strip()
            if not line:
                continue
                
            # è·³è¿‡BBçš„æ–‡ä»¶å¤´
            if line.startswith("#EXTM3U") and not bb_section_started:
                continue
            
            bb_section_started = True
            
            if line.startswith("#EXTINF"):
                bb_count += 1
            merged_lines.append(line)
        
        if bb_count > 0:
            logger.info(f"åˆä½µäº† {bb_count} å€‹BBé »é“")
            merged_lines.append("")  # æ·»åŠ ç©ºè¡Œåˆ†éš”
            merged_lines.append("#" + "="*60)
            merged_lines.append("# ä»¥ä¸‹ç‚ºHKå’ŒTWé »é“ï¼ˆå·²é©—è­‰å¯æ’­æ”¾ï¼‰")
            merged_lines.append("#" + "="*60)
            merged_lines.append("")
    
    # æ·»åŠ æå–çš„TVå†…å®¹ï¼ˆè·³è¿‡æ–‡ä»¶å¤´ï¼‰
    if tv_content:
        tv_lines = tv_content.split('\n')
        tv_section_started = False
        
        for line in tv_lines:
            line = line.strip()
            if not line:
                continue
                
            # è·³è¿‡TVçš„æ–‡ä»¶å¤´
            if line.startswith("#EXTM3U") and not tv_section_started:
                continue
            
            tv_section_started = True
            merged_lines.append(line)
    
    return '\n'.join(merged_lines)

def save_m3u_file(content, filename):
    """ä¿å­˜M3Uæ–‡ä»¶"""
    if not content:
        logger.error("æ²’æœ‰å…§å®¹å¯ä¿å­˜")
        return False
    
    try:
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        output_path = filename
        
        # å¦‚æœè„šæœ¬åœ¨scriptsç›®å½•ï¼Œè¾“å‡ºåˆ°ä¸Šçº§ç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        if "scripts" in script_dir:
            output_path = os.path.join(script_dir, "..", filename)
        
        logger.info(f"å°‡ä¿å­˜åˆ°: {os.path.abspath(output_path)}")
        
        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8', newline='\n') as f:
            f.write(content)
        
        # éªŒè¯æ–‡ä»¶
        if os.path.exists(output_path):
            file_size = os.path.getsize(output_path)
            extinf_count = content.count("#EXTINF")
            
            logger.info("âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ")
            logger.info(f"ğŸ“ æ–‡ä»¶è·¯å¾‘: {os.path.abspath(output_path)}")
            logger.info(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:,} å­—ç¯€")
            logger.info(f"ğŸ“ˆ é »é“ç¸½æ•¸: {extinf_count}")
            
            # ç»Ÿè®¡å„åˆ†ç±»æ•°é‡
            hk_count = content.count('group-title="HK"')
            tw_count = content.count('group-title="TW"')
            other_count = extinf_count - hk_count - tw_count
            
            logger.info("=== è©³ç´°åˆ†é¡çµ±è¨ˆ ===")
            logger.info(f"HKé »é“: {hk_count} å€‹")
            logger.info(f"TWé »é“: {tw_count} å€‹")
            logger.info(f"å…¶ä»–é »é“(BB): {other_count} å€‹")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªHKé¢‘é“ï¼ˆéªŒè¯æ’åºï¼‰
            if hk_count > 0:
                logger.info("=== HKé »é“å‰10å€‹ï¼ˆé©—è­‰æ’åºï¼‰ ===")
                lines = content.split('\n')
                hk_shown = 0
                for i, line in enumerate(lines):
                    if 'group-title="HK"' in line:
                        # æå–é¢‘é“å
                        name_match = re.search(r',([^,]+)$', line)
                        if name_match:
                            logger.info(f"{hk_shown+1:2d}. {name_match.group(1)}")
                            hk_shown += 1
                            if hk_shown >= 10:
                                break
            
            return True
        else:
            logger.error("âŒ æ–‡ä»¶å‰µå»ºå¤±æ•—")
            return False
            
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ä»¶å¤±æ•—: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def main(skip_validation=False):
    """ä¸»å‡½æ•°"""
    logger.info("="*60)
    logger.info("M3Ué »é“æå–å™¨é–‹å§‹é‹è¡Œ")
    logger.info("="*60)
    logger.info(f"å°‡æå–HKå’ŒTWé »é“ï¼Œé©—è­‰æ’­æ”¾ç‹€æ…‹: {'è·³é' if skip_validation else 'åŸ·è¡Œ'}")
    
    # 1. è·å–HKæºå†…å®¹
    logger.info("="*40)
    logger.info("è™•ç†HKæº")
    logger.info("="*40)
    hk_content = fetch_m3u_content(HK_SOURCE_URL, "HKæº")
    if hk_content:
        hk_channels = parse_m3u_content(hk_content, "HK")
        logger.info(f"å¾HKæºè§£æå‡º {len(hk_channels)} å€‹é »é“")
        
        # è¿‡æ»¤å’Œæ’åºHKé¢‘é“
        hk_channels = filter_and_sort_channels(hk_channels, HK_BLACKLIST, "HK")
    else:
        hk_channels = []
        logger.warning("HKæºç²å–å¤±æ•—ï¼Œå°‡ä½¿ç”¨ç©ºåˆ—è¡¨")
    
    # 2. è·å–TWæºå†…å®¹
    logger.info("="*40)
    logger.info("è™•ç†TWæº")
    logger.info("="*40)
    tw_content = fetch_m3u_content(TW_SOURCE_URL, "TWæº")
    if tw_content:
        tw_channels = parse_m3u_content(tw_content, "TW")
        logger.info(f"å¾TWæºè§£æå‡º {len(tw_channels)} å€‹é »é“")
        
        # è¿‡æ»¤TWé¢‘é“
        tw_channels = filter_and_sort_channels(tw_channels, TW_BLACKLIST, "TW")
    else:
        tw_channels = []
        logger.warning("TWæºç²å–å¤±æ•—ï¼Œå°‡ä½¿ç”¨ç©ºåˆ—è¡¨")
    
    # 3. éªŒè¯é¢‘é“æ’­æ”¾çŠ¶æ€
    logger.info("="*40)
    logger.info("é©—è­‰é »é“æ’­æ”¾ç‹€æ…‹")
    logger.info("="*40)
    all_channels = hk_channels + tw_channels
    
    if all_channels:
        valid_channels, invalid_channels = validate_channels(all_channels, skip_validation)
        
        # é‡æ–°åˆ†ç»„
        hk_valid = [c for c in valid_channels if c['group'] == 'HK']
        tw_valid = [c for c in valid_channels if c['group'] == 'TW']
        
        logger.info(f"é©—è­‰çµæœ: HKæœ‰æ•ˆ {len(hk_valid)} å€‹, TWæœ‰æ•ˆ {len(tw_valid)} å€‹")
        
        # è®°å½•æ— æ•ˆé¢‘é“
        if invalid_channels and not skip_validation:
            logger.warning(f"ä»¥ä¸‹ {len(invalid_channels)} å€‹é »é“ä¸å¯æ’­æ”¾:")
            for i, channel in enumerate(invalid_channels[:20]):  # åªæ˜¾ç¤ºå‰20ä¸ª
                logger.warning(f"  {i+1:2d}. {channel['name']} ({channel['group']})")
            if len(invalid_channels) > 20:
                logger.warning(f"  ... é‚„æœ‰ {len(invalid_channels) - 20} å€‹")
    else:
        hk_valid = []
        tw_valid = []
        logger.warning("æ²’æœ‰æå–åˆ°ä»»ä½•HK/TWé »é“")
    
    # 4. æ„å»ºTVå†…å®¹
    tv_content = build_m3u_content(hk_valid, tw_valid)
    
    # 5. è¯»å–BB.m3u
    logger.info("="*40)
    logger.info("è®€å–BB.m3u")
    logger.info("="*40)
    bb_content = read_bb_file()
    
    # 6. åˆå¹¶å†…å®¹
    merged_content = merge_with_bb(tv_content, bb_content)
    
    # 7. ä¿å­˜æ–‡ä»¶
    if save_m3u_file(merged_content, OUTPUT_FILE):
        logger.info("="*60)
        logger.info("è™•ç†å®Œæˆ")
        logger.info("="*60)
        
        # æœ€ç»ˆç»Ÿè®¡
        final_hk_count = merged_content.count('group-title="HK"')
        final_tw_count = merged_content.count('group-title="TW"')
        final_total = merged_content.count("#EXTINF")
        final_other = final_total - final_hk_count - final_tw_count
        
        logger.info(f"ğŸ¯ æœ€çµ‚çµæœ:")
        logger.info(f"   ç¸½é »é“æ•¸: {final_total}")
        logger.info(f"   HKé »é“: {final_hk_count}")
        logger.info(f"   TWé »é“: {final_tw_count}")
        logger.info(f"   å…¶ä»–é »é“: {final_other}")
        
        return True
    else:
        logger.error("="*60)
        logger.error("è™•ç†å¤±æ•—")
        logger.error("="*60)
        return False

if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦è·³è¿‡éªŒè¯
    skip_validation = False
    if len(sys.argv) > 1:
        if sys.argv[1] in ['--skip-validation', '--skip', '-s']:
            skip_validation = True
            logger.info("å‘½ä»¤è¡Œåƒæ•¸: è·³éæ’­æ”¾é©—è­‰")
    
    success = main(skip_validation)
    sys.exit(0 if success else 1)
