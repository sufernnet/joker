#!/usr/bin/env python3
"""
ä»TVæºä¸­æå–"æ¸¯æ¾³é »é“"å’Œ"é«”è‚²ä¸–ç•Œ"å¹¶ä¸BB.m3uåˆå¹¶ï¼Œä¿å­˜ä¸ºEE.m3u
"""

import requests
import re
import os
import sys
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¸¸é‡å®šä¹‰
SOURCE_URL = "https://raw.githubusercontent.com/yihad168/tv/refs/heads/main/tv.m3u"
EPG_URL = "http://epg.51zmt.top:8000/e.xml"
BB_FILE = "BB.m3u"  # å‡è®¾BB.m3uåœ¨ä»“åº“æ ¹ç›®å½•
OUTPUT_FILE = "../EE.m3u"  # ä¸Šä¸€çº§ç›®å½•ï¼ˆjokerç›®å½•ï¼‰

def fetch_m3u_content():
    """è·å–åŸå§‹M3Uæ–‡ä»¶å†…å®¹"""
    try:
        logger.info(f"æ­£åœ¨ä» {SOURCE_URL} ä¸‹è½½M3Uæ–‡ä»¶...")
        response = requests.get(SOURCE_URL, timeout=30)
        response.raise_for_status()
        logger.info(f"ä¸‹è½½æˆåŠŸï¼Œå¤§å°: {len(response.text)} å­—ç¬¦")
        
        # è°ƒè¯•ï¼šæŸ¥çœ‹æ–‡ä»¶å†…å®¹çš„å‰1000å­—ç¬¦
        preview = response.text[:1000]
        logger.info(f"æ–‡ä»¶é¢„è§ˆï¼ˆå‰1000å­—ç¬¦ï¼‰:\n{preview}")
        
        return response.text
    except requests.RequestException as e:
        logger.error(f"ä¸‹è½½M3Uæ–‡ä»¶å¤±è´¥: {e}")
        return None

def read_bb_file():
    """è¯»å–BB.m3uæ–‡ä»¶å†…å®¹"""
    try:
        # BB.m3uåœ¨ä»“åº“æ ¹ç›®å½•
        bb_path = "../BB.m3u"
        if os.path.exists(bb_path):
            with open(bb_path, 'r', encoding='utf-8') as f:
                content = f.read()
            logger.info(f"è¯»å–BB.m3uæˆåŠŸï¼Œå¤§å°: {len(content)} å­—ç¬¦")
            return content
        else:
            logger.warning(f"BB.m3uæ–‡ä»¶ä¸å­˜åœ¨: {bb_path}")
            return None
    except Exception as e:
        logger.error(f"è¯»å–BB.m3uå¤±è´¥: {e}")
        return None

def extract_channels(content):
    """æå–æ¸¯æ¾³é »é“å’Œé«”è‚²ä¸–ç•Œ"""
    if not content:
        return None
    
    logger.info("å¼€å§‹æå–æŒ‡å®šåˆ†ç»„é¢‘é“...")
    
    # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„ç¹ä½“å­—åˆ†ç»„å
    target_groups = ["æ¸¯æ¾³é »é“", "é«”è‚²ä¸–ç•Œ"]
    
    # æŒ‰è¡Œåˆ†å‰²å†…å®¹
    lines = content.split('\n')
    extracted_lines = []
    
    # æ·»åŠ æ–‡ä»¶å¤´
    extracted_lines.append(f'#EXTM3U url-tvg="{EPG_URL}"')
    
    # ç”¨äºè°ƒè¯•å’Œç»Ÿè®¡
    found_groups = {}
    for group in target_groups:
        found_groups[group] = 0
    
    # æŸ¥æ‰¾æ‰€æœ‰åˆ†ç»„ç”¨äºè°ƒè¯•
    all_groups = set()
    for line in lines:
        if '#EXTINF' in line and 'group-title="' in line:
            match = re.search(r'group-title="([^"]+)"', line)
            if match:
                all_groups.add(match.group(1))
    
    logger.info(f"æºæ–‡ä»¶ä¸­æ‰¾åˆ°çš„æ‰€æœ‰åˆ†ç»„: {sorted(all_groups)}")
    logger.info(f"ç›®æ ‡åˆ†ç»„: {target_groups}")
    
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        
        if not line:
            i += 1
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†ç»„è¡Œ
        if line.startswith("#EXTINF"):
            # æå–åˆ†ç»„ä¿¡æ¯
            group_match = re.search(r'group-title="([^"]+)"', line)
            if group_match:
                group_name = group_match.group(1)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡åˆ†ç»„
                if group_name in target_groups:
                    logger.info(f"æ‰¾åˆ°ç›®æ ‡åˆ†ç»„ '{group_name}' çš„é¢‘é“")
                    # æ·»åŠ EXTINFè¡Œ
                    extracted_lines.append(line)
                    found_groups[group_name] += 1
                    
                    # æŸ¥æ‰¾å¯¹åº”çš„URLè¡Œ
                    j = i + 1
                    url_added = False
                    while j < len(lines):
                        url_line = lines[j].strip()
                        if not url_line:
                            j += 1
                            continue
                        if url_line.startswith("#EXTINF"):
                            break
                        # æ·»åŠ URLè¡Œ
                        if url_line and not url_line.startswith("#"):
                            extracted_lines.append(url_line)
                            url_added = True
                            logger.info(f"  æ·»åŠ URL: {url_line[:50]}...")
                        j += 1
                    
                    if not url_added:
                        logger.warning(f"åˆ†ç»„ '{group_name}' çš„é¢‘é“æ²¡æœ‰æ‰¾åˆ°URL")
                    
                    i = j - 1  # è·³è¿‡å·²å¤„ç†çš„URLè¡Œ
        i += 1
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    logger.info("=== æå–ç»Ÿè®¡ ===")
    for group, count in found_groups.items():
        logger.info(f"{group}: {count} ä¸ªé¢‘é“")
    
    total_channels = sum(found_groups.values())
    logger.info(f"æ€»è®¡æå–: {total_channels} ä¸ªé¢‘é“")
    
    if total_channels == 0:
        logger.error("æ²¡æœ‰æå–åˆ°ä»»ä½•é¢‘é“ï¼Œè¯·æ£€æŸ¥åˆ†ç»„åç§°")
        return None
    
    return '\n'.join(extracted_lines) if len(extracted_lines) > 1 else None

def merge_with_bb(tv_content, bb_content):
    """å°†æå–çš„TVå†…å®¹ä¸BB.m3uåˆå¹¶"""
    merged_lines = []
    
    # æ·»åŠ æ–‡ä»¶å¤´
    merged_lines.append(f'#EXTM3U url-tvg="{EPG_URL}"')
    
    # æ·»åŠ ç”Ÿæˆä¿¡æ¯
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    merged_lines.append(f"# ç”Ÿæˆæ—¶é—´: {timestamp}")
    merged_lines.append(f"# æºåœ°å€: {SOURCE_URL}")
    merged_lines.append(f"# EPGæº: {EPG_URL}")
    merged_lines.append("# åŒ…å«å†…å®¹: BB.m3u + æ¸¯æ¾³é »é“ + é«”è‚²ä¸–ç•Œ")
    merged_lines.append("# è‡ªåŠ¨æ›´æ–°é¢‘é“åˆ—è¡¨")
    merged_lines.append("")
    
    # å¦‚æœæœ‰BBå†…å®¹ï¼Œå…ˆæ·»åŠ BBçš„å†…å®¹ï¼ˆè·³è¿‡å…¶æ–‡ä»¶å¤´ï¼‰
    if bb_content:
        bb_lines = bb_content.split('\n')
        bb_count = 0
        for line in bb_lines:
            line = line.strip()
            if line:
                if line.startswith("#EXTM3U"):
                    continue  # è·³è¿‡BBçš„æ–‡ä»¶å¤´
                if line.startswith("#EXTINF"):
                    bb_count += 1
                merged_lines.append(line)
        
        if bb_count > 0:
            logger.info(f"åˆå¹¶äº† {bb_count} ä¸ªBBé¢‘é“")
            merged_lines.append("")  # æ·»åŠ ç©ºè¡Œåˆ†éš”
    
    # æ·»åŠ æå–çš„TVå†…å®¹ï¼ˆè·³è¿‡æ–‡ä»¶å¤´ï¼‰
    if tv_content:
        tv_lines = tv_content.split('\n')
        first_extm3u_skipped = False
        tv_count = 0
        
        for line in tv_lines:
            line = line.strip()
            if line:
                if line.startswith("#EXTM3U") and not first_extm3u_skipped:
                    first_extm3u_skipped = True
                    continue
                if line.startswith("#EXTINF"):
                    tv_count += 1
                merged_lines.append(line)
        
        logger.info(f"åˆå¹¶äº† {tv_count} ä¸ªTVé¢‘é“")
    
    return '\n'.join(merged_lines)

def save_m3u_file(content):
    """ä¿å­˜M3Uæ–‡ä»¶"""
    if not content:
        logger.error("æ²¡æœ‰å†…å®¹å¯ä¿å­˜")
        return False
    
    try:
        # è·å–è„šæœ¬ç›®å½•çš„ä¸Šä¸€çº§ç›®å½•ï¼ˆjokerç›®å½•ï¼‰
        script_dir = os.path.dirname(os.path.abspath(__file__))
        parent_dir = os.path.dirname(script_dir)
        output_path = os.path.join(parent_dir, "EE.m3u")
        
        logger.info(f"å°†ä¿å­˜åˆ°: {output_path}")
        
        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        # éªŒè¯æ–‡ä»¶
        if os.path.exists(output_path):
            file_size = os.path.getsize(output_path)
            extinf_count = content.count("#EXTINF")
            
            logger.info("âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ")
            logger.info(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {output_path}")
            logger.info(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
            logger.info(f"ğŸ“ˆ é¢‘é“æ€»æ•°: {extinf_count}")
            
            # ç»Ÿè®¡å„åˆ†ç»„æ•°é‡
            hk_count = content.count("æ¸¯æ¾³é »é“")
            sports_count = content.count("é«”è‚²ä¸–ç•Œ")
            
            logger.info("=== è¯¦ç»†ç»Ÿè®¡ ===")
            logger.info(f"æ¸¯æ¾³é »é“: {hk_count} ä¸ªé¢‘é“")
            logger.info(f"é«”è‚²ä¸–ç•Œ: {sports_count} ä¸ªé¢‘é“")
            
            # æ˜¾ç¤ºæ–‡ä»¶å¼€å¤´å’Œç»“å°¾
            with open(output_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                logger.info(f"ğŸ“ æ–‡ä»¶æ€»è¡Œæ•°: {len(lines)}")
                
                if len(lines) > 0:
                    logger.info("=== æ–‡ä»¶å¼€å¤´ï¼ˆå‰10è¡Œï¼‰===")
                    for j in range(min(10, len(lines))):
                        line = lines[j].rstrip()
                        if line:  # åªæ˜¾ç¤ºéç©ºè¡Œ
                            logger.info(f"  {line}")
                    
                    logger.info("=== æ–‡ä»¶ç»“å°¾ï¼ˆæœ€å5è¡Œï¼‰===")
                    for j in range(max(0, len(lines)-5), len(lines)):
                        line = lines[j].rstrip()
                        if line:  # åªæ˜¾ç¤ºéç©ºè¡Œ
                            logger.info(f"  {line}")
            
            return True
        else:
            logger.error("âŒ æ–‡ä»¶åˆ›å»ºå¤±è´¥")
            return False
            
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("=== M3Ué¢‘é“æå–å™¨å¼€å§‹è¿è¡Œ ===")
    
    # 1. è·å–åŸå§‹TVå†…å®¹
    raw_content = fetch_m3u_content()
    if not raw_content:
        logger.error("æ— æ³•è·å–åŸå§‹TVå†…å®¹ï¼Œç¨‹åºé€€å‡º")
        sys.exit(1)
    
    # 2. æå–æŒ‡å®šé¢‘é“
    extracted_content = extract_channels(raw_content)
    if not extracted_content:
        logger.error("æœªæ‰¾åˆ°æŒ‡å®šçš„åˆ†ç»„é¢‘é“")
        sys.exit(1)
    
    # 3. è¯»å–BB.m3u
    bb_content = read_bb_file()
    
    # 4. åˆå¹¶å†…å®¹
    merged_content = merge_with_bb(extracted_content, bb_content)
    
    # 5. ä¿å­˜æ–‡ä»¶
    if save_m3u_file(merged_content):
        logger.info("=== å¤„ç†å®Œæˆ ===")
    else:
        logger.error("=== å¤„ç†å¤±è´¥ ===")
        sys.exit(1)

if __name__ == "__main__":
    main()
