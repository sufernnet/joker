#!/usr/bin/env python3
"""
CC.m3u åˆå¹¶è„šæœ¬ - æ ‡å‡†M3Uæ ¼å¼ï¼ˆæ”¯æŒEPGã€é¢‘é“æ’åºã€é¢‘é“è¿‡æ»¤ï¼‰
ä» https://stymei.sufern001.workers.dev/ æå–ï¼š
1. ğŸ”¥å…¨ç½‘é€šæ¸¯æ¾³å°
2. ğŸ”®æ¸¯æ¾³å°ç›´æ’­
å°†ç›¸åŒé¢‘é“åˆå¹¶ï¼Œæ”¯æŒå¤šæ’­æ”¾åœ°å€ï¼Œå¹¶æŒ‰æŒ‡å®šè§„åˆ™æ’åºã€è¿‡æ»¤
æ–°å¢åŠŸèƒ½ï¼š
1. NOWç›¸å…³é¢‘é“åˆå¹¶åˆ°NOWåˆ†ç»„
2. çˆ†è°·ã€æ˜Ÿå½±å°æ’åœ¨OWç›´æ’­å°åé¢
3. å‡¤å‡°ä¸­æ–‡æ·»åŠ ç‰¹å®šé“¾æ¥
"""

import requests
from datetime import datetime
import os
import re
import hashlib
from collections import defaultdict

# ================== é…ç½®åŒºåŸŸ ==================
SOURCE_URL = "https://stymei.sufern001.workers.dev/"
BB_FILE = "BB.m3u"
OUTPUT_FILE = "CC.m3u"
EPG_URL = "http://epg.51zmt.top:8000/e.xml"  # EPGèŠ‚ç›®å•åœ°å€

# è¦æå–çš„æºåˆ†ç»„ï¼ˆå¯æ‰©å±•å¤šä¸ªï¼‰
SOURCE_GROUPS = [
    "ğŸ”¥å…¨ç½‘é€šæ¸¯æ¾³å°",
    "ğŸ”®æ¸¯æ¾³å°ç›´æ’­"
]
TARGET_GROUP = "å…¨ç½‘é€šæ¸¯æ¾³å°"  # åˆå¹¶åçš„ç»Ÿä¸€åˆ†ç»„å

# ç‰¹æ®Šé“¾æ¥æ˜ å°„ï¼ˆç‰¹å®šé¢‘é“æ·»åŠ ç‰¹å®šé“¾æ¥ï¼‰
SPECIAL_URLS = {
    "å‡¤å‡°ä¸­æ–‡": [
        "http://iptv.4666888.xyz/iptv2A.php?id=45",
        "http://61.184.46.85:85/tsfile/live/1029_1.m3u8?key=txiptv&playlive=1&authid=0",
        "http://r.jdshipin.com/cCCzW"
    ]
}

# å°æ ‡æºï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
LOGO_SOURCES = [
    "https://raw.githubusercontent.com/iptv-org/iptv/master/logos/",
    "https://raw.githubusercontent.com/fanmingming/live/main/tv/",
    "https://raw.githubusercontent.com/lqist/IPTVlogos/main/",
]

# é¢‘é“æ’åºä¼˜å…ˆçº§ï¼ˆä¾æ¬¡ä¸º:å‡¤å‡°â†’NOWâ†’TVBâ†’HOYâ†’VIUTVâ†’çˆ†è°·â†’æ˜Ÿå½±å°â†’å…¶ä»–ï¼‰
CHANNEL_PRIORITY = {
    # æœ€é«˜ä¼˜å…ˆçº§ï¼šå‡¤å‡°ç³»åˆ—
    "å‡¤å‡°ä¸­æ–‡": 1,
    "å‡¤å‡°èµ„è®¯": 2,
    "å‡¤å‡°é¦™æ¸¯": 3,
    "å‡¤å‡°å«è§†": 5,
    # ç¬¬äºŒä¼˜å…ˆçº§ï¼šNOWç³»åˆ—ï¼ˆæ‰€æœ‰NOWç›¸å…³é¢‘é“ç»Ÿä¸€ä¸ºNOWï¼‰
    "NOW": 10,
    "NOWç›´æ’­å°": 10,      # æ˜ å°„åˆ°NOW
    "NOWæ–°é—»å°": 10,      # æ˜ å°„åˆ°NOW
    "NOWè´¢ç»å°": 10,      # æ˜ å°„åˆ°NOW
    "NOWä½“è‚²å°": 10,      # æ˜ å°„åˆ°NOW
    "NOWç”µå½±å°": 10,      # æ˜ å°„åˆ°NOW
    # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šTVBç³»åˆ—
    "TVB": 20,
    "ç¿¡ç¿ å°": 21,
    "æ˜ç å°": 22,
    "J2": 23,
    "æ— çº¿æ–°é—»": 24,
    "æ— çº¿è´¢ç»": 25,
    # ç¬¬å››ä¼˜å…ˆçº§ï¼šHOYç³»åˆ—
    "HOY": 30,
    "HOY TV": 31,
    "HOYèµ„è®¯å°": 32,
    "é¦™æ¸¯å¼€ç”µè§†": 33,
    # ç¬¬äº”ä¼˜å…ˆçº§ï¼šVIUTVç³»åˆ—
    "VIUTV": 40,
    "VIUTVä¸­æ–‡å°": 41,
    "VIUTVç»¼è‰ºå°": 42,
    # ç¬¬å…­ä¼˜å…ˆçº§ï¼šçˆ†è°·ã€æ˜Ÿå½±å°ï¼ˆæ’åœ¨NOWåé¢ï¼‰
    "çˆ†è°·å°": 50,
    "æ˜Ÿå½±å°": 51,
    # å…¶ä»–é¢‘é“é»˜è®¤ä¼˜å…ˆçº§ï¼š100
}

# é¢‘é“åç§°æ˜ å°„ï¼ˆåˆå¹¶ç›¸ä¼¼é¢‘é“ï¼‰
CHANNEL_NAME_MAPPING = {
    # NOWç›¸å…³é¢‘é“ç»Ÿä¸€ä¸ºNOW
    "NOWç›´æ’­å°": "NOW",
    "NOWæ–°é—»å°": "NOW",
    "NOWè´¢ç»å°": "NOW",
    "NOWä½“è‚²å°": "NOW",
    "NOWç”µå½±å°": "NOW",
    "NOWå¨±ä¹å°": "NOW",
    # å…¶ä»–å¯èƒ½çš„NOWå˜ä½“
    "Nowç›´æ’­å°": "NOW",
    "Nowæ–°é—»å°": "NOW",
    "Nowè´¢ç»å°": "NOW",
    # æ ‡å‡†åŒ–å…¶ä»–é¢‘é“åç§°
    "å‡¤å‡°å«è§†ä¸­æ–‡å°": "å‡¤å‡°ä¸­æ–‡",
    "å‡¤å‡°å«è§†èµ„è®¯å°": "å‡¤å‡°èµ„è®¯",
    "å‡¤å‡°å«è§†é¦™æ¸¯å°": "å‡¤å‡°é¦™æ¸¯",
    "TVBç¿¡ç¿ å°": "ç¿¡ç¿ å°",
    "TVBæ˜ç å°": "æ˜ç å°",
}

# éœ€è¦å‰”é™¤çš„é¢‘é“å…³é”®è¯ï¼ˆå®Œå…¨åŒ¹é…æˆ–éƒ¨åˆ†åŒ¹é…ï¼‰
BLACKLIST_KEYWORDS = [
    # åŸé»‘åå•
    "SPOTV",
    "GOODTV",
    "GOOD2",
    "ç•ªè–¯111",
    "äººé—´å«è§†",
    "å”¯å¿ƒç”µè§†",
    "ä¸­æ—ºç”µè§†",
    "ç”Ÿå‘½ç”µå½±",
    "å”äººå«è§†",
    "é¦™æ¸¯å«è§†",
    "å”NTD",
    "NTDTV",
    "æ–°å”äºº",
    # æ–°å¢å‰”é™¤é¢‘é“
    "å‡¤å‡°ç”µå½±",
    "C+",
    "MoMoTV",
    "DAZN1",
    "DAZN2",
    "ELEVENä½“è‚²1",
    "ELEVENä½“è‚²2",
    "çˆ±å¥‡è‰º",
]

# ================== å·¥å…·å‡½æ•° ==================
def log(msg):
    """æ—¥å¿—è¾“å‡º"""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def normalize_channel_name(channel_name):
    """æ ‡å‡†åŒ–é¢‘é“åç§°ï¼ˆåˆå¹¶ç›¸ä¼¼é¢‘é“ï¼‰"""
    original_name = channel_name
    
    # 1. æ£€æŸ¥æ˜ å°„è¡¨
    for pattern, mapped_name in CHANNEL_NAME_MAPPING.items():
        if pattern in channel_name:
            log(f"  é¢‘é“æ˜ å°„: {original_name} -> {mapped_name}")
            return mapped_name
    
    # 2. ç‰¹æ®Šè§„åˆ™ï¼šåŒ…å«"NOW"ä½†ä¸æ˜¯å·²å®šä¹‰çš„
    if "NOW" in channel_name.upper() and channel_name not in CHANNEL_PRIORITY:
        log(f"  NOWé¢‘é“æ ‡å‡†åŒ–: {channel_name} -> NOW")
        return "NOW"
    
    # 3. ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
    cleaned = re.sub(r'\s+', ' ', channel_name.strip())
    return cleaned

def is_channel_blacklisted(channel_name):
    """æ£€æŸ¥é¢‘é“æ˜¯å¦åœ¨é»‘åå•ä¸­"""
    for keyword in BLACKLIST_KEYWORDS:
        if keyword in channel_name:
            return True
    return False

def get_channel_priority(channel_name):
    """è·å–é¢‘é“æ’åºä¼˜å…ˆçº§"""
    # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
    for key, priority in CHANNEL_PRIORITY.items():
        if key == channel_name:
            return priority
    
    # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…
    for key, priority in CHANNEL_PRIORITY.items():
        if key in channel_name:
            return priority
    
    # ç‰¹æ®Šè§„åˆ™ï¼šåŒ…å«"å‡¤å‡°"ä½†ä¸æ˜¯å·²å®šä¹‰çš„ï¼ˆä¸åŒ…æ‹¬"å‡¤å‡°ç”µå½±"ï¼‰
    if "å‡¤å‡°" in channel_name and channel_name not in CHANNEL_PRIORITY and "å‡¤å‡°ç”µå½±" not in channel_name:
        return 6  # å…¶ä»–å‡¤å‡°é¢‘é“æ”¾åœ¨å·²å®šä¹‰å‡¤å‡°é¢‘é“ä¹‹å
    
    # ç‰¹æ®Šè§„åˆ™ï¼šåŒ…å«"NOW"ä½†ä¸æ˜¯å·²å®šä¹‰çš„
    if "NOW" in channel_name.upper() and channel_name not in CHANNEL_PRIORITY:
        return 10  # ç»Ÿä¸€å½’åˆ°NOWä¼˜å…ˆçº§
    
    # é»˜è®¤ä¼˜å…ˆçº§
    return 100

def sort_channels(channel_dict):
    """æŒ‰æŒ‡å®šè§„åˆ™æ’åºé¢‘é“"""
    # è½¬æ¢ä¸ºåˆ—è¡¨ä¾¿äºæ’åº
    channels_list = [(name, data) for name, data in channel_dict.items()]
    
    # æ’åºè§„åˆ™ï¼š1.ä¼˜å…ˆçº§ 2.é¢‘é“åç§°
    def sort_key(item):
        channel_name = item[0]
        priority = get_channel_priority(channel_name)
        return (priority, channel_name)
    
    sorted_channels = sorted(channels_list, key=sort_key)
    
    # è½¬æ¢å›å­—å…¸
    sorted_dict = {name: data for name, data in sorted_channels}
    
    # è®°å½•æ’åºä¿¡æ¯
    log(f"é¢‘é“æ’åºå®Œæˆï¼Œä¼˜å…ˆçº§åˆ†å¸ƒ:")
    priority_groups = defaultdict(list)
    for name, _ in sorted_channels:
        priority = get_channel_priority(name)
        priority_groups[priority].append(name)
    
    # æ˜¾ç¤ºä¸»è¦ä¼˜å…ˆçº§ç»„
    priority_mapping = {
        1: "å‡¤å‡°ç³»åˆ—",
        2: "å‡¤å‡°ç³»åˆ—",
        3: "å‡¤å‡°ç³»åˆ—",
        5: "å‡¤å‡°ç³»åˆ—",
        6: "å‡¤å‡°ç³»åˆ—",
        10: "NOWç³»åˆ—",
        20: "TVBç³»åˆ—",
        21: "TVBç³»åˆ—",
        22: "TVBç³»åˆ—",
        23: "TVBç³»åˆ—",
        24: "TVBç³»åˆ—",
        25: "TVBç³»åˆ—",
        30: "HOYç³»åˆ—",
        31: "HOYç³»åˆ—",
        32: "HOYç³»åˆ—",
        33: "HOYç³»åˆ—",
        40: "VIUTVç³»åˆ—",
        41: "VIUTVç³»åˆ—",
        42: "VIUTVç³»åˆ—",
        50: "çˆ†è°·å°",
        51: "æ˜Ÿå½±å°",
    }
    
    for priority in sorted(priority_groups.keys()):
        if priority <= 100:  # æ˜¾ç¤ºæ‰€æœ‰ä¼˜å…ˆçº§ç»„
            group_name = priority_mapping.get(priority, f"ä¼˜å…ˆçº§{priority}")
            log(f"  {group_name}: {len(priority_groups[priority])}ä¸ªé¢‘é“")
    
    return sorted_dict

def get_channel_logo(channel_name):
    """æ ¹æ®é¢‘é“ååŒ¹é…å°æ ‡"""
    # é¢‘é“åæ˜ å°„è¡¨ï¼ˆå¯è‡ªè¡Œæ‰©å±•ï¼‰
    logo_map = {
        # å‡¤å‡°ç³»åˆ—
        "å‡¤å‡°ä¸­æ–‡": "phoenix.chinese.png",
        "å‡¤å‡°èµ„è®¯": "phoenix.infonews.png",
        "å‡¤å‡°é¦™æ¸¯": "phoenix.hongkong.png",
        "å‡¤å‡°å«è§†": "phoenix.tv.png",
        # NOWç³»åˆ—
        "NOW": "now.png",
        # TVBç³»åˆ—
        "ç¿¡ç¿ å°": "tvb.jade.png",
        "æ˜ç å°": "tvb.pearl.png",
        "J2": "tvb.j2.png",
        "TVB": "tvb.png",
        # HOYç³»åˆ—
        "HOY": "hoy.png",
        "HOY TV": "hoy.tv.png",
        "é¦™æ¸¯å¼€ç”µè§†": "hoy.tv.png",
        # VIUTVç³»åˆ—
        "VIUTV": "viutv.png",
        # çˆ†è°·ã€æ˜Ÿå½±å°
        "çˆ†è°·å°": "popcorn.png",
        "æ˜Ÿå½±å°": "starmovie.png",
        # å…¶ä»–å¸¸è§é¢‘é“
        "ä¸­å¤©": "cti.png",
        "ä¸œæ£®": "ettv.png",
        "ä¸‰ç«‹": "set.png",
        "æ°‘è§†": "ftv.png",
    }
    
    # 1. ç²¾ç¡®åŒ¹é…
    for key, filename in logo_map.items():
        if key in channel_name:
            for source in LOGO_SOURCES:
                logo_url = f"{source}{filename}"
                return logo_url
    
    # 2. å…³é”®è¯åŒ¹é…
    keywords = {
        "æ–°é—»": "news.png",
        "ä½“è‚²": "sports.png",
        "ç”µå½±": "movie.png",
        "éŸ³ä¹": "music.png",
        "è´¢ç»": "finance.png",
        "ç›´æ’­": "live.png",
    }
    
    for keyword, filename in keywords.items():
        if keyword in channel_name:
            for source in LOGO_SOURCES:
                logo_url = f"{source}{filename}"
                return logo_url
    
    # 3. è¿”å›é»˜è®¤å°æ ‡
    return "https://raw.githubusercontent.com/iptv-org/iptv/master/logos/default.png"

def extract_tvg_info(channel_name):
    """ç”Ÿæˆé¢‘é“çš„tvgä¿¡æ¯"""
    # æ¸…ç†åç§°ç”Ÿæˆtvg-id
    clean_name = re.sub(r'[^\w\u4e00-\u9fff]', '', channel_name)
    
    # ä½¿ç”¨MD5ç”Ÿæˆä¸€è‡´çš„tvg-idï¼Œç¡®ä¿ç›¸åŒé¢‘é“åæœ‰ç›¸åŒID
    tvg_id = f"channel_{hashlib.md5(channel_name.encode()).hexdigest()[:8]}"
    tvg_name = channel_name
    logo_url = get_channel_logo(channel_name)
    
    return tvg_id, tvg_name, logo_url

def download_source():
    """ä¸‹è½½æºæ•°æ®"""
    try:
        log(f"æ­£åœ¨ä¸‹è½½æºæ•°æ®: {SOURCE_URL}")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/plain'
        }
        response = requests.get(SOURCE_URL, headers=headers, timeout=30)
        response.raise_for_status()
        content = response.text
        log(f"âœ… ä¸‹è½½æˆåŠŸï¼Œ{len(content)} å­—ç¬¦")
        return content
    except Exception as e:
        log(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return None

def extract_and_merge_channels(content):
    """
    ä»å†…å®¹ä¸­æå–æŒ‡å®šåˆ†ç»„çš„æ‰€æœ‰é¢‘é“ï¼Œå¹¶åˆå¹¶ç›¸åŒé¢‘é“çš„å¤šä¸ªæº
    è¿”å›ç»“æ„: {channel_name: {tvg_id, tvg_name, logo, group, urls: [url1, url2, ...]}}
    """
    if not content:
        return {}
    
    # ä½¿ç”¨å­—å…¸åˆå¹¶ç›¸åŒé¢‘é“ï¼Œå€¼æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰URLçš„åˆ—è¡¨
    channel_dict = defaultdict(lambda: {
        'name': '',
        'tvg_id': '',
        'tvg_name': '',
        'logo': '',
        'group': TARGET_GROUP,
        'urls': [],  # å­˜å‚¨å¤šä¸ªæ’­æ”¾åœ°å€
        'source_groups': set(),  # è®°å½•æ¥æºåˆ†ç»„
        'original_names': set()  # è®°å½•åŸå§‹åç§°ï¼ˆç”¨äºNOWåˆå¹¶ï¼‰
    })
    
    lines = content.split('\n')
    
    log(f"å¼€å§‹æå–å¹¶åˆå¹¶åˆ†ç»„: {SOURCE_GROUPS}")
    log(f"é¢‘é“æ ‡å‡†åŒ–è§„åˆ™: NOWç›¸å…³é¢‘é“ç»Ÿä¸€ä¸ºNOW")
    log(f"æ–°å¢æ’åº: çˆ†è°·å°ã€æ˜Ÿå½±å°æ’åœ¨NOWåé¢")
    
    for source_group in SOURCE_GROUPS:
        in_section = False
        group_found = False
        group_count = 0
        blacklist_count = 0
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
            
            # æŸ¥æ‰¾åˆ†ç»„å¼€å§‹
            if f"{source_group},#genre#" in line:
                log(f"âœ… æ‰¾åˆ°åˆ†ç»„: {source_group} (ç¬¬{i+1}è¡Œ)")
                in_section = True
                group_found = True
                continue
            
            # å¦‚æœå·²åœ¨åˆ†ç»„ä¸­ï¼Œé‡åˆ°ä¸‹ä¸€ä¸ªåˆ†ç»„åˆ™ç»“æŸ
            if in_section and '#genre#' in line and source_group not in line:
                break
            
            # æå–é¢‘é“
            if in_section and line and ',' in line:
                parts = line.split(',')
                if len(parts) >= 2:
                    original_name = parts[0].strip()
                    url = ','.join(parts[1:]).strip()
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
                    if is_channel_blacklisted(original_name):
                        blacklist_count += 1
                        log(f"  è¿‡æ»¤é»‘åå•é¢‘é“: {original_name}")
                        continue
                    
                    if url and ('://' in url or url.startswith('http')):
                        # æ ‡å‡†åŒ–é¢‘é“åç§°
                        channel_name = normalize_channel_name(original_name)
                        
                        # å¦‚æœæ˜¯é¦–æ¬¡é‡åˆ°è¿™ä¸ªé¢‘é“ï¼Œç”Ÿæˆtvgä¿¡æ¯
                        if channel_name not in channel_dict or not channel_dict[channel_name]['tvg_id']:
                            tvg_id, tvg_name, logo_url = extract_tvg_info(channel_name)
                            channel_dict[channel_name].update({
                                'name': channel_name,
                                'tvg_id': tvg_id,
                                'tvg_name': tvg_name,
                                'logo': logo_url,
                                'group': TARGET_GROUP,
                            })
                        
                        # æ·»åŠ URLåˆ°åˆ—è¡¨
                        if url not in channel_dict[channel_name]['urls']:
                            channel_dict[channel_name]['urls'].append(url)
                            channel_dict[channel_name]['source_groups'].add(source_group)
                            channel_dict[channel_name]['original_names'].add(original_name)
                            group_count += 1
        
        if group_found:
            log(f"  ä»ã€Œ{source_group}ã€æå– {group_count} ä¸ªæ’­æ”¾æºï¼Œè¿‡æ»¤ {blacklist_count} ä¸ªé»‘åå•é¢‘é“")
        else:
            log(f"âš ï¸  æœªæ‰¾åˆ°åˆ†ç»„: {source_group}")
    
    # è½¬æ¢ä¸ºæ™®é€šå­—å…¸
    result = dict(channel_dict)
    
    # æ·»åŠ ç‰¹æ®Šé“¾æ¥
    for channel_name, special_urls in SPECIAL_URLS.items():
        if channel_name in result:
            for url in special_urls:
                if url not in result[channel_name]['urls']:
                    result[channel_name]['urls'].append(url)
                    log(f"âœ… ä¸º {channel_name} æ·»åŠ ç‰¹æ®Šé“¾æ¥: {url[:50]}...")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_channels = len(result)
    total_urls = sum(len(ch['urls']) for ch in result.values())
    
    log(f"âœ… åˆå¹¶åå¾—åˆ° {total_channels} ä¸ªå”¯ä¸€é¢‘é“ï¼Œå…± {total_urls} ä¸ªæ’­æ”¾æº")
    
    # æ˜¾ç¤ºNOWåˆå¹¶ç»Ÿè®¡
    now_channels = [name for name in result.keys() if "NOW" in name.upper()]
    if len(now_channels) > 1:
        log(f"âœ… NOWé¢‘é“åˆå¹¶: å°† {len(now_channels)} ä¸ªNOWç›¸å…³é¢‘é“åˆå¹¶ä¸º'NOW'")
        for name in now_channels:
            if name != "NOW":
                log(f"  åˆå¹¶ {name} -> NOW")
    
    # æ˜¾ç¤ºè¿‡æ»¤ç»Ÿè®¡
    if blacklist_count > 0:
        log(f"âœ… å…±è¿‡æ»¤ {blacklist_count} ä¸ªé»‘åå•é¢‘é“")
    
    # æ˜¾ç¤ºåˆå¹¶ç¤ºä¾‹
    if result:
        log("é¢‘é“åˆå¹¶ç¤ºä¾‹:")
        for name, data in list(result.items())[:5]:
            original_count = len(data['original_names'])
            if original_count > 1:
                log(f"  {name}: {len(data['urls'])}ä¸ªæ’­æ”¾æº (åˆå¹¶è‡ª: {', '.join(list(data['original_names'])[:3])}...)")
            else:
                log(f"  {name}: {len(data['urls'])}ä¸ªæ’­æ”¾æº")
    
    return result

def load_local_m3u():
    """åŠ è½½æœ¬åœ°BB.m3uæ–‡ä»¶"""
    try:
        if not os.path.exists(BB_FILE):
            log(f"âš ï¸  {BB_FILE} ä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤æ–‡ä»¶")
            default_content = f"""#EXTM3U
#EXTINF:-1 tvg-id="" tvg-name="æœ¬åœ°æµ‹è¯•1" tvg-logo="" group-title="æœ¬åœ°",æœ¬åœ°æµ‹è¯•1
http://example.com/local1
#EXTINF:-1 tvg-id="" tvg-name="æœ¬åœ°æµ‹è¯•2" tvg-logo="" group-title="æœ¬åœ°",æœ¬åœ°æµ‹è¯•2
http://example.com/local2"""
            with open(BB_FILE, 'w', encoding='utf-8') as f:
                f.write(default_content)
            return default_content
        
        with open(BB_FILE, 'r', encoding='utf-8') as f:
            content = f.read()
        log(f"âœ… åŠ è½½æœ¬åœ°æ–‡ä»¶æˆåŠŸï¼Œ{len(content.splitlines())} è¡Œ")
        return content
    except Exception as e:
        log(f"âŒ åŠ è½½æœ¬åœ°æ–‡ä»¶å¤±è´¥: {e}")
        return "#EXTM3U\n"

def generate_m3u_content(local_content, channel_dict):
    """ç”Ÿæˆæœ€ç»ˆçš„M3Uå†…å®¹ï¼ˆæ”¯æŒå¤šæ’­æ”¾åœ°å€ï¼‰"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    output_lines = [
        f'#EXTM3U url-tvg="{EPG_URL}"',
        f"# CC.m3u - è‡ªåŠ¨ç”Ÿæˆï¼ˆEPG+æ’åº+è¿‡æ»¤+åˆå¹¶ç‰ˆï¼‰",
        f"# ç”Ÿæˆæ—¶é—´: {timestamp}",
        f"# æºåœ°å€: {SOURCE_URL}",
        f"# EPGåœ°å€: {EPG_URL}",
        f"# æå–åˆ†ç»„: {', '.join(SOURCE_GROUPS)} â†’ {TARGET_GROUP}",
        f"# æ’åºè§„åˆ™: å‡¤å‡°â†’NOWâ†’TVBâ†’HOYâ†’VIUTVâ†’çˆ†è°·â†’æ˜Ÿå½±å°â†’å…¶ä»–",
        f"# é¢‘é“åˆå¹¶: NOWç›¸å…³é¢‘é“ç»Ÿä¸€ä¸ºNOWï¼Œå‡¤å‡°ä¸­æ–‡æ·»åŠ ç‰¹æ®Šé“¾æ¥",
        f"# è¿‡æ»¤é¢‘é“: å…±{len(BLACKLIST_KEYWORDS)}ä¸ªå…³é”®è¯",
        f"# å”¯ä¸€é¢‘é“æ•°: {len(channel_dict)}",
        f"# è‡ªåŠ¨è¿è¡Œ: åŒ—äº¬æ—¶é—´ 06:00, 17:30",
        f"# GitHub Actions è‡ªåŠ¨ç”Ÿæˆ",
        ""
    ]
    
    # æ·»åŠ æœ¬åœ°å†…å®¹
    if local_content and local_content.strip():
        output_lines.append("#" + "=" * 60)
        output_lines.append("# æœ¬åœ°é¢‘é“")
        output_lines.append("#" + "=" * 60)
        output_lines.append("")
        
        local_lines = local_content.split('\n')
        for line in local_lines:
            if line.strip() == "#EXTM3U" and len(output_lines) > 8:
                continue
            output_lines.append(line)
        
        output_lines.append("")
    
    # æ·»åŠ æ¸¯æ¾³å°é¢‘é“ï¼ˆæ”¯æŒå¤šæ’­æ”¾åœ°å€ï¼‰
    if channel_dict:
        output_lines.append("#" + "=" * 60)
        output_lines.append(f"# {TARGET_GROUP} (åˆå¹¶è‡ª: {', '.join(SOURCE_GROUPS)})")
        output_lines.append("# è¯´æ˜ï¼šæ¯ä¸ªé¢‘é“å¯èƒ½åŒ…å«å¤šä¸ªæ’­æ”¾åœ°å€ï¼Œæ’­æ”¾å™¨ä¼šè‡ªåŠ¨é€‰æ‹©å¯ç”¨æº")
        output_lines.append("# æ’åºï¼šå‡¤å‡°ç³»åˆ—â†’NOWç³»åˆ—â†’TVBç³»åˆ—â†’HOYç³»åˆ—â†’VIUTVç³»åˆ—â†’çˆ†è°·å°â†’æ˜Ÿå½±å°â†’å…¶ä»–")
        output_lines.append("# åˆå¹¶ï¼šNOWç›´æ’­å°ã€NOWæ–°é—»å°ç­‰ç»Ÿä¸€åˆå¹¶ä¸ºNOWé¢‘é“")
        output_lines.append("# é“¾æ¥ï¼šå‡¤å‡°ä¸­æ–‡å·²æ·»åŠ ç‰¹æ®Šä¼˜è´¨é“¾æ¥")
        output_lines.append("#" + "=" * 60)
        output_lines.append("")
        
        # æ·»åŠ åˆ†ç»„æ ‡é¢˜ä¾¿äºè¯†åˆ«
        current_priority = None
        priority_mapping = {
            1: "å‡¤å‡°ç³»åˆ—",
            2: "å‡¤å‡°ç³»åˆ—",
            3: "å‡¤å‡°ç³»åˆ—",
            5: "å‡¤å‡°ç³»åˆ—",
            6: "å‡¤å‡°ç³»åˆ—",
            10: "NOWç³»åˆ—ï¼ˆåˆå¹¶æ‰€æœ‰NOWç›¸å…³é¢‘é“ï¼‰",
            20: "TVBç³»åˆ—",
            21: "TVBç³»åˆ—",
            22: "TVBç³»åˆ—",
            23: "TVBç³»åˆ—",
            24: "TVBç³»åˆ—",
            25: "TVBç³»åˆ—",
            30: "HOYç³»åˆ—",
            31: "HOYç³»åˆ—",
            32: "HOYç³»åˆ—",
            33: "HOYç³»åˆ—",
            40: "VIUTVç³»åˆ—",
            41: "VIUTVç³»åˆ—",
            42: "VIUTVç³»åˆ—",
            50: "çˆ†è°·å°",
            51: "æ˜Ÿå½±å°",
        }
        
        for i, (channel_name, data) in enumerate(channel_dict.items(), 1):
            priority = get_channel_priority(channel_name)
            
            # æ·»åŠ åˆ†ç»„åˆ†éš”
            if current_priority != priority:
                current_priority = priority
                group_name = priority_mapping.get(priority, "å…¶ä»–é¢‘é“")
                
                if i > 1:  # ä¸æ˜¯ç¬¬ä¸€ä¸ªé¢‘é“æ‰æ·»åŠ ç©ºè¡Œ
                    output_lines.append("")
                output_lines.append(f"# --- {group_name} ---")
                
                # å¦‚æœæ˜¯NOWç³»åˆ—ï¼Œæ˜¾ç¤ºåˆå¹¶ä¿¡æ¯
                if priority == 10 and len(data.get('original_names', set())) > 1:
                    originals = list(data['original_names'])
                    if len(originals) > 3:
                        originals = originals[:3] + [f"...ç­‰{len(data['original_names'])}ä¸ªé¢‘é“"]
                    output_lines.append(f"# åˆå¹¶è‡ª: {', '.join(originals)}")
            
            # EXTINF è¡Œ
            extinf = f'#EXTINF:-1 tvg-id="{data["tvg_id"]}" tvg-name="{data["tvg_name"]}" tvg-logo="{data["logo"]}" group-title="{TARGET_GROUP}",{channel_name}'
            output_lines.append(extinf)
            
            # å¤šä¸ªæ’­æ”¾åœ°å€ï¼ˆæ¯ä¸ªåœ°å€ä¸€è¡Œï¼‰
            for url in data['urls']:
                output_lines.append(url)
            
            # å¦‚æœæ˜¯å‡¤å‡°ä¸­æ–‡ï¼Œæ ‡è®°ç‰¹æ®Šé“¾æ¥
            if channel_name == "å‡¤å‡°ä¸­æ–‡":
                output_lines.append("# â†‘ ä»¥ä¸Šä¸ºå‡¤å‡°ä¸­æ–‡ç‰¹æ®Šä¼˜è´¨é“¾æ¥")
        
        # ç§»é™¤æœ€åçš„ç©ºè¡Œï¼ˆå¦‚æœæœ‰ï¼‰
        while output_lines and output_lines[-1] == "":
            output_lines.pop()
    
    # ç»Ÿè®¡ä¿¡æ¯
    output_lines.append("")
    output_lines.append("#" + "=" * 60)
    output_lines.append("# ç»Ÿè®¡ä¿¡æ¯")
    local_channels = len([l for l in local_content.split('\n') if l.startswith('#EXTINF')])
    total_urls = sum(len(ch['urls']) for ch in channel_dict.values())
    
    # ç»Ÿè®¡å„ç³»åˆ—æ•°é‡
    series_count = defaultdict(int)
    for channel_name in channel_dict.keys():
        priority = get_channel_priority(channel_name)
        series_mapping = {
            1: "å‡¤å‡°", 2: "å‡¤å‡°", 3: "å‡¤å‡°", 5: "å‡¤å‡°", 6: "å‡¤å‡°",
            10: "NOW",
            20: "TVB", 21: "TVB", 22: "TVB", 23: "TVB", 24: "TVB", 25: "TVB",
            30: "HOY", 31: "HOY", 32: "HOY", 33: "HOY",
            40: "VIUTV", 41: "VIUTV", 42: "VIUTV",
            50: "çˆ†è°·å°",
            51: "æ˜Ÿå½±å°",
        }
        series = series_mapping.get(priority, "å…¶ä»–")
        series_count[series] += 1
    
    # NOWåˆå¹¶ç»Ÿè®¡
    now_original_count = 0
    for channel_name, data in channel_dict.items():
        if channel_name == "NOW":
            now_original_count = len(data.get('original_names', set()))
    
    output_lines.append(f"# æœ¬åœ°é¢‘é“æ•°: {local_channels}")
    output_lines.append(f"# æ¸¯æ¾³å°å”¯ä¸€é¢‘é“æ•°: {len(channel_dict)}")
    output_lines.append(f"# æ¸¯æ¾³å°æ’­æ”¾æºæ€»æ•°: {total_urls}")
    
    if series_count:
        output_lines.append("# é¢‘é“ç³»åˆ—åˆ†å¸ƒ:")
        for series in ["å‡¤å‡°", "NOW", "TVB", "HOY", "VIUTV", "çˆ†è°·å°", "æ˜Ÿå½±å°", "å…¶ä»–"]:
            if series_count.get(series, 0) > 0:
                count_info = f"{series_count[series]}ä¸ªé¢‘é“"
                if series == "NOW" and now_original_count > 1:
                    count_info = f"{series_count[series]}ä¸ªé¢‘é“ (åˆå¹¶è‡ª{now_original_count}ä¸ªç›¸å…³é¢‘é“)"
                output_lines.append(f"#   {series}: {count_info}")
    
    output_lines.append(f"# æ›´æ–°æ—¶é—´: {timestamp}")
    output_lines.append("# EPGèŠ‚ç›®å•: å·²é›†æˆï¼Œæ’­æ”¾å™¨ä¼šè‡ªåŠ¨åŠ è½½")
    output_lines.append("# ç‰¹æ®ŠåŠŸèƒ½: NOWé¢‘é“åˆå¹¶ã€å‡¤å‡°ä¸­æ–‡ç‰¹æ®Šé“¾æ¥ã€çˆ†è°·/æ˜Ÿå½±å°æ’åº")
    output_lines.append("#" + "=" * 60)
    
    return '\n'.join(output_lines)

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    log("å¼€å§‹ç”Ÿæˆ CC.m3uï¼ˆNOWåˆå¹¶+ç‰¹æ®Šæ’åºç‰ˆï¼‰...")
    print("=" * 70)
    
    try:
        # 1. ä¸‹è½½æºæ•°æ®
        source_content = download_source()
        if not source_content:
            log("âŒ æ— æ³•è·å–æºæ•°æ®ï¼Œé€€å‡º")
            return
        
        # 2. æå–å¹¶åˆå¹¶é¢‘é“
        channel_dict = extract_and_merge_channels(source_content)
        
        if not channel_dict:
            log("âš ï¸  æœªæå–åˆ°ä»»ä½•é¢‘é“ï¼Œæ£€æŸ¥æºæ•°æ®æ ¼å¼")
            # æ˜¾ç¤ºå‰5ä¸ªåˆ†ç»„ä¾›è°ƒè¯•
            lines = source_content.split('\n')
            log("æºæ•°æ®ä¸­çš„åˆ†ç»„:")
            count = 0
            for line in lines:
                if '#genre#' in line and count < 5:
                    log(f"  - {line}")
                    count += 1
        
        # 3. æŒ‰è§„åˆ™æ’åºé¢‘é“
        log("å¼€å§‹æŒ‰è§„åˆ™æ’åºé¢‘é“...")
        sorted_channel_dict = sort_channels(channel_dict)
        
        # 4. åŠ è½½æœ¬åœ°æ–‡ä»¶
        local_content = load_local_m3u()
        
        # 5. ç”Ÿæˆå†…å®¹
        m3u_content = generate_m3u_content(local_content, sorted_channel_dict)
        
        # 6. ä¿å­˜æ–‡ä»¶
        with open(OUTPUT_FILE, 'w', encoding='utf-8', newline='\n') as f:
            f.write(m3u_content)
        
        # 7. éªŒè¯ç»“æœ
        if os.path.exists(OUTPUT_FILE):
            file_size = os.path.getsize(OUTPUT_FILE)
            line_count = m3u_content.count('\n') + 1
            
            print("\n" + "=" * 70)
            log("âœ… CC.m3u ç”ŸæˆæˆåŠŸ!")
            log(f"   æ–‡ä»¶ä½ç½®: {os.path.abspath(OUTPUT_FILE)}")
            log(f"   æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
            log(f"   æ€»è¡Œæ•°: {line_count}")
            log(f"   å”¯ä¸€é¢‘é“æ•°: {len(sorted_channel_dict)}")
            log(f"   EPGåœ°å€: {EPG_URL}")
            
            # æ˜¾ç¤ºNOWåˆå¹¶è¯¦æƒ…
            print("\nğŸ“‹ NOWé¢‘é“åˆå¹¶è¯¦æƒ…:")
            print("-" * 70)
            for name, data in sorted_channel_dict.items():
                if name == "NOW" and len(data.get('original_names', set())) > 1:
                    print(f"âœ… NOWé¢‘é“åˆå¹¶å®Œæˆ:")
                    print(f"   åˆå¹¶ååç§°: NOW")
                    print(f"   åˆå¹¶é¢‘é“æ•°: {len(data['original_names'])} ä¸ª")
                    print(f"   æ’­æ”¾æºæ•°é‡: {len(data['urls'])} ä¸ª")
                    print(f"   åŸå§‹é¢‘é“: {', '.join(list(data['original_names'])[:5])}")
                    if len(data['original_names']) > 5:
                        print(f"            ... ç­‰{len(data['original_names'])}ä¸ªé¢‘é“")
            
            # æ˜¾ç¤ºå‡¤å‡°ä¸­æ–‡ç‰¹æ®Šé“¾æ¥
            print("\nğŸ“‹ å‡¤å‡°ä¸­æ–‡ç‰¹æ®Šé“¾æ¥:")
            print("-" * 70)
            if "å‡¤å‡°ä¸­æ–‡" in sorted_channel_dict:
                phoenix_data = sorted_channel_dict["å‡¤å‡°ä¸­æ–‡"]
                print(f"é¢‘é“: å‡¤å‡°ä¸­æ–‡")
                print(f"æ€»é“¾æ¥æ•°: {len(phoenix_data['urls'])} ä¸ª")
                print("ç‰¹æ®Šé“¾æ¥:")
                for url in SPECIAL_URLS.get("å‡¤å‡°ä¸­æ–‡", []):
                    if url in phoenix_data['urls']:
                        print(f"  âœ“ {url[:80]}..." if len(url) > 80 else f"  âœ“ {url}")
            
            # æ˜¾ç¤ºæ’åºç»“æœ
            print("\nğŸ“‹ é¢‘é“æ’åºç»“æœï¼ˆæŒ‰æ–°è§„åˆ™ï¼‰:")
            print("-" * 70)
            
            # ç³»åˆ—æ˜ å°„
            series_mapping = {
                1: "å‡¤å‡°", 2: "å‡¤å‡°", 3: "å‡¤å‡°", 5: "å‡¤å‡°", 6: "å‡¤å‡°",
                10: "NOW",
                20: "TVB", 21: "TVB", 22: "TVB", 23: "TVB", 24: "TVB", 25: "TVB",
                30: "HOY", 31: "HOY", 32: "HOY", 33: "HOY",
                40: "VIUTV", 41: "VIUTV", 42: "VIUTV",
                50: "çˆ†è°·å°",
                51: "æ˜Ÿå½±å°",
            }
            
            for i, (name, data) in enumerate(list(sorted_channel_dict.items())[:15]):
                priority = get_channel_priority(name)
                series = series_mapping.get(priority, "å…¶ä»–")
                source_count = len(data['urls'])
                if name == "NOW" and len(data.get('original_names', set())) > 1:
                    name_display = f"{name} ({len(data['original_names'])}åˆ1)"
                else:
                    name_display = name
                print(f"{i+1:2d}. [{series}] {name_display} ({source_count}æº)")
            print("-" * 70)
            
            # æ˜¾ç¤ºæ–‡ä»¶å¤´
            print("\nğŸ“„ ç”Ÿæˆæ–‡ä»¶å¤´éƒ¨:")
            print("-" * 50)
            lines = m3u_content.split('\n')
            for line in lines[:20]:
                print(line)
            print("-" * 50)
            
        else:
            log("âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥")
    
    except Exception as e:
        log(f"âŒ æ‰§è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print("=" * 70)
    log("æ‰§è¡Œå®Œæˆ")
    print("=" * 70)

if __name__ == "__main__":
    main()
