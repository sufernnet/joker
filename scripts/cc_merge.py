#!/usr/bin/env python3
"""
CCåˆå¹¶è„šæœ¬ - å®Œæ•´ç‰ˆ
ç”ŸæˆCC.m3uæ–‡ä»¶ï¼Œç»Ÿä¸€ä½¿ç”¨CCå‰ç¼€ä¾¿äºè®°å¿†
1. ä¸‹è½½BB.m3uï¼ˆåŒ…å«EPGä¿¡æ¯ï¼‰
2. ä»Cloudflareä»£ç†è·å–å†…å®¹
3. æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHKï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—ï¼ˆåˆå¹¶ç›¸åŒé¢‘é“çš„å¤šä¸ªæºï¼‰
4. æå–4gtvå‰30ä¸ªç›´æ’­ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“
5. åˆå¹¶ç”ŸæˆCC.m3uï¼ŒåŒ…å«å¤šä¸ªEPGæº
åŒ—äº¬æ—¶é—´æ¯å¤©6:00ã€17:00è‡ªåŠ¨è¿è¡Œ
"""

import requests
import re
import os
import sys
from datetime import datetime
import urllib3
import time

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®
BB_URL = "https://raw.githubusercontent.com/sufernnet/joker/main/BB.m3u"
CLOUDFLARE_PROXY = "https://smt-proxy.sufern001.workers.dev/"
OUTPUT_FILE = "CC.m3u"

# éœ€è¦è¿‡æ»¤æ‰çš„TWé¢‘é“å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
BLACKLIST_TW = [
    "Bloomberg TV",
    "Bloomberg",
    "SBNå…¨çƒè´¢ç»å°",
    "SBNè´¢ç»",
    "FRANCE24è‹±æ–‡å°",
    "FRANCE24",
    "åŠå²›å›½é™…æ–°é—»å°",
    "åŠå³¶å›½é™…",
    "NHK world-japan",
    "NHK world",
    "SBN",
    "æ—¥æœ¬",
    "NHK",
    "CNBC Asia",
    "CNBC"
]

# HKé¢‘é“ä¼˜å…ˆé¡ºåºï¼ˆæŒ‰è¿™ä¸ªé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢ï¼‰
HK_PRIORITY_ORDER = [
    "å‡¤å‡°ä¸­æ–‡",
    "å‡¤å‡°èµ„è®¯", 
    "å‡¤å‡°é¦™æ¸¯",
    "NOWæ–°é—»å°",
    "NOWæ˜Ÿå½±",
    "NOWçˆ†è°·"
]

# å¤‡é€‰EPGæºï¼ˆå¦‚æœä¸»è¦EPGå¤±æ•ˆï¼‰
BACKUP_EPG_URLS = [
    "https://epg.112114.xyz/pp.xml",
    "https://epg.946985.filegear-sg.me/t.xml.gz",
    "http://epg.51zmt.top:8000/e.xml",
]

def log(msg):
    """è¾“å‡ºæ—¥å¿—"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{timestamp}] {msg}")

def get_output_path():
    """è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„"""
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    root_dir = os.path.dirname(script_dir)
    
    # æ£€æŸ¥å½“å‰å·¥ä½œç›®å½•
    cwd = os.getcwd()
    log(f"å½“å‰å·¥ä½œç›®å½•: {cwd}")
    log(f"è„šæœ¬ç›®å½•: {script_dir}")
    log(f"æ ¹ç›®å½•: {root_dir}")
    
    # ä¼˜å…ˆä¿å­˜åˆ°æ ¹ç›®å½•
    root_output = os.path.join(root_dir, OUTPUT_FILE)
    log(f"è¾“å‡ºè·¯å¾„: {root_output}")
    
    return root_output

def test_epg_url(epg_url):
    """æµ‹è¯•EPG URLæ˜¯å¦å¯è®¿é—®"""
    try:
        log(f"æµ‹è¯•EPG: {epg_url}")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/xml, text/xml, */*',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive'
        }
        
        # è®¾ç½®è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´
        timeout = 15
        
        # å¯¹äº.gzæ–‡ä»¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        if epg_url.endswith('.gz'):
            response = requests.get(epg_url, headers=headers, timeout=timeout, stream=True, verify=False)
            if response.status_code == 200:
                # å°è¯•è¯»å–å‰å‡ ä¸ªå­—èŠ‚æ£€æŸ¥æ˜¯å¦æ˜¯gzip
                try:
                    # è¯»å–å‰100å­—èŠ‚æ£€æŸ¥
                    chunk = response.raw.read(100)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯gzipæ–‡ä»¶ï¼ˆå‰ä¸¤ä¸ªå­—èŠ‚æ˜¯1f 8bï¼‰
                    if len(chunk) >= 2 and chunk[0] == 0x1f and chunk[1] == 0x8b:
                        log(f"âœ… EPGå¯ç”¨ (GZIPæ ¼å¼): {epg_url}")
                        return True
                    else:
                        log(f"âš ï¸  EPGä¸æ˜¯æœ‰æ•ˆçš„GZIPæ ¼å¼: {epg_url}")
                        return False
                except Exception as e:
                    log(f"âš ï¸  GZIPæ–‡ä»¶è¯»å–å¤±è´¥: {e}")
                    return False
        else:
            # å¸¸è§„XMLæ–‡ä»¶
            response = requests.get(epg_url, headers=headers, timeout=timeout, stream=True, verify=False)
            
            if response.status_code == 200:
                # æ£€æŸ¥å†…å®¹ç±»å‹
                content_type = response.headers.get('content-type', '').lower()
                
                # è¯»å–å‰1KBæ£€æŸ¥
                try:
                    chunk = response.raw.read(1024)
                    text = chunk.decode('utf-8', errors='ignore')
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯XMLæ ¼å¼
                    if '<?xml' in text or '<tv' in text or '<programme' in text:
                        log(f"âœ… EPGå¯ç”¨: {epg_url}")
                        return True
                    else:
                        log(f"âš ï¸  EPGä¸æ˜¯XMLæ ¼å¼: {epg_url}")
                        return False
                except UnicodeDecodeError:
                    # å¯èƒ½æ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå°è¯•å…¶ä»–ç¼–ç 
                    try:
                        text = chunk.decode('gbk', errors='ignore')
                        if '<?xml' in text or '<tv' in text or '<programme' in text:
                            log(f"âœ… EPGå¯ç”¨ (GBKç¼–ç ): {epg_url}")
                            return True
                        else:
                            log(f"âš ï¸  EPGä¸æ˜¯XMLæ ¼å¼: {epg_url}")
                            return False
                    except:
                        log(f"âš ï¸  EPGè§£ç å¤±è´¥: {epg_url}")
                        return False
            else:
                log(f"âŒ EPGä¸å¯è®¿é—®: {epg_url} (çŠ¶æ€ç : {response.status_code})")
                return False
            
    except Exception as e:
        log(f"âŒ EPGæµ‹è¯•å¤±è´¥ {epg_url}: {str(e)[:100]}")
        return False

def get_best_epg_url(epg_urls):
    """è·å–æœ€ä½³çš„EPG URL"""
    log("å¯»æ‰¾æœ€ä½³EPGæº...")
    
    # æµ‹è¯•æ‰€æœ‰EPG
    working_epgs = []
    for epg_url in epg_urls:
        if test_epg_url(epg_url):
            working_epgs.append(epg_url)
    
    if working_epgs:
        # ä¼˜å…ˆä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„
        best_epg = working_epgs[0]
        log(f"âœ… é€‰æ‹©EPG: {best_epg}")
        log(f"   å…¶ä»–å¯ç”¨EPG: {len(working_epgs)-1}ä¸ª")
        return best_epg
    else:
        log("âš ï¸  æ²¡æœ‰å¯ç”¨çš„EPGæº")
        return None

def download_bb_m3u():
    """ä¸‹è½½BB.m3uå¹¶æå–EPG"""
    try:
        log("ä¸‹è½½BB.m3u...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Referer': 'https://github.com/'
        }
        
        response = requests.get(BB_URL, headers=headers, timeout=10, verify=False)
        response.raise_for_status()
        
        bb_content = response.text
        log(f"âœ… BB.m3uä¸‹è½½æˆåŠŸ ({len(bb_content)} å­—ç¬¦)")
        
        return bb_content
        
    except Exception as e:
        log(f"âŒ BB.m3uä¸‹è½½å¤±è´¥: {e}")
        return None

def get_content_from_proxy():
    """ä»Cloudflareä»£ç†è·å–å†…å®¹"""
    try:
        log("ä»Cloudflareä»£ç†è·å–å†…å®¹...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://smart.946985.filegear-sg.me/',
            'Origin': 'https://smart.946985.filegear-sg.me',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'cross-site'
        }
        
        # å°è¯•å¤šæ¬¡è¯·æ±‚
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.get(CLOUDFLARE_PROXY, headers=headers, timeout=30, verify=False)
                
                if response.status_code == 200:
                    content = response.text
                    
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ•ˆ
                    if not content or len(content.strip()) < 100:
                        log(f"å°è¯• {attempt + 1}/{max_retries}: å†…å®¹è¿‡çŸ­ ({len(content)} å­—ç¬¦)")
                        if attempt < max_retries - 1:
                            time.sleep(2)
                            continue
                    
                    # å¦‚æœæ˜¯HTMLï¼Œå°è¯•æå–M3Uå†…å®¹
                    if '<html' in content.lower():
                        log(f"å°è¯• {attempt + 1}/{max_retries}: æ£€æµ‹åˆ°HTMLå“åº”")
                        
                        # æ–¹æ³•1ï¼šæŸ¥æ‰¾M3Uå†…å®¹
                        m3u_patterns = [
                            r'(#EXTM3U.*?)(?:\n\n|\Z)',  # ç›´åˆ°ä¸¤ä¸ªæ¢è¡Œæˆ–ç»“å°¾
                            r'(#EXTM3U.*?)(?:</pre>|</code>|\Z)',  # ç›´åˆ°æ ‡ç­¾ç»“æŸæˆ–ç»“å°¾
                            r'<pre[^>]*>(.*?)</pre>',  # preæ ‡ç­¾å†…
                            r'<code[^>]*>(.*?)</code>'  # codeæ ‡ç­¾å†…
                        ]
                        
                        for pattern in m3u_patterns:
                            match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
                            if match:
                                extracted = match.group(1) if len(match.groups()) > 0 else match.group(0)
                                if '#EXTM3U' in extracted or '#EXTINF:' in extracted:
                                    content = extracted.strip()
                                    log(f"âœ… ä½¿ç”¨æ¨¡å¼æå–åˆ°M3Uå†…å®¹ ({len(content)} å­—ç¬¦)")
                                    break
                        
                        # å¦‚æœæ²¡æå–åˆ°ï¼Œå°è¯•é€è¡Œæå–
                        if '<html' in content.lower() or not ('#EXTM3U' in content or '#EXTINF:' in content):
                            lines = content.split('\n')
                            m3u_lines = []
                            for line in lines:
                                line = line.strip()
                                if line.startswith('#EXTM3U') or line.startswith('#EXTINF:') or ('.m3u8' in line and '://' in line):
                                    m3u_lines.append(line)
                            
                            if m3u_lines:
                                content = '\n'.join(m3u_lines)
                                log(f"âœ… é€è¡Œæå–åˆ° {len(m3u_lines)} è¡ŒM3Uå†…å®¹")
                    
                    # ç¡®ä¿ä»¥#EXTM3Uå¼€å¤´
                    if content and '#EXTM3U' not in content[:20]:
                        if '#EXTINF:' in content or '.m3u8' in content:
                            content = '#EXTM3U\n' + content
                            log("å·²æ·»åŠ #EXTM3Uå¤´éƒ¨")
                    
                    if content and len(content.strip()) > 100:
                        log(f"âœ… è·å–åˆ°å†…å®¹ ({len(content)} å­—ç¬¦)")
                        return content
                    else:
                        log(f"å°è¯• {attempt + 1}/{max_retries}: å†…å®¹æ— æ•ˆ ({len(content) if content else 0} å­—ç¬¦)")
                        
                else:
                    log(f"å°è¯• {attempt + 1}/{max_retries}: HTTP {response.status_code}")
                    
            except requests.exceptions.Timeout:
                log(f"å°è¯• {attempt + 1}/{max_retries}: è¯·æ±‚è¶…æ—¶")
            except Exception as e:
                log(f"å°è¯• {attempt + 1}/{max_retries}: {str(e)[:100]}")
            
            if attempt < max_retries - 1:
                time.sleep(3)
        
        log("âŒ æ‰€æœ‰é‡è¯•å¤±è´¥")
            
    except Exception as e:
        log(f"âŒ ä»£ç†è®¿é—®å¤±è´¥: {str(e)[:100]}")
    
    return None

def get_channel_priority(channel_name):
    """è·å–é¢‘é“çš„ä¼˜å…ˆçº§ï¼ˆè¶Šå°è¶Šé å‰ï¼‰"""
    channel_name_lower = channel_name.lower()
    
    for i, priority_channel in enumerate(HK_PRIORITY_ORDER):
        if priority_channel.lower() in channel_name_lower:
            return i  # è¿”å›ä¼˜å…ˆçº§ç´¢å¼•ï¼Œè¶Šå°è¶Šé å‰
    
    return len(HK_PRIORITY_ORDER)  # éä¼˜å…ˆé¢‘é“æ’åœ¨æœ€å

def extract_and_sort_hk_channels(content):
    """æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHKï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—ï¼Œåˆå¹¶ç›¸åŒé¢‘é“çš„å¤šä¸ªæº"""
    if not content:
        return []
    
    log("æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHKï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—...")
    log(f"HKä¼˜å…ˆé¡ºåº: {', '.join(HK_PRIORITY_ORDER)}")
    
    # è§£æM3Uå†…å®¹
    lines = content.split('\n')
    channels = []
    current_extinf = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            current_extinf = line
        elif current_extinf and '://' in line and not line.startswith('#'):
            channels.append((current_extinf, line))
            current_extinf = None
    
    log(f"æ€»é¢‘é“æ•°: {len(channels)}")
    
    # åˆ›å»ºé¢‘é“å­—å…¸æ¥åˆå¹¶ç›¸åŒé¢‘é“çš„å¤šä¸ªæº
    channel_dict = {}
    
    for extinf, url in channels:
        # åªå¤„ç†JULIé¢‘é“ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        if 'juli' in extinf.lower():
            # æå–åŸå§‹é¢‘é“å
            if ',' in extinf:
                # è·å–é¢‘é“åç§°éƒ¨åˆ†
                parts = extinf.split(',', 1)
                channel_info = parts[0]
                channel_name = parts[1]
                
                # æ¸…ç†é¢‘é“åï¼šå»æ‰SMT_å‰ç¼€ï¼Œä¿ç•™åŸå§‹åç§°
                clean_channel_name = re.sub(r'^SMT_', '', channel_name)
                
                # åˆ›å»ºæ ‡å‡†åŒ–çš„é¢‘é“ä¿¡æ¯ï¼ˆä½¿ç”¨æ¸…ç†åçš„åç§°ï¼‰
                # ç¡®ä¿group-titleä¸ºHK
                if 'group-title=' in channel_info:
                    clean_channel_info = re.sub(r'group-title="[^"]*"', 'group-title="HK"', channel_info)
                else:
                    clean_channel_info = channel_info + ' group-title="HK"'
                
                # å®Œæ•´çš„EXTINFè¡Œ
                clean_extinf = f'{clean_channel_info},{clean_channel_name}'
                
                # æ·»åŠ åˆ°å­—å…¸
                if clean_extinf not in channel_dict:
                    channel_dict[clean_extinf] = []
                
                # æ·»åŠ URLåˆ°åˆ—è¡¨
                channel_dict[clean_extinf].append(url)
            else:
                log(f"âš ï¸  æ— æ³•è§£æEXTINFè¡Œ: {extinf}")
    
    # ç»Ÿè®¡åˆå¹¶æ•ˆæœ
    juli_channels = [c for c in channels if 'juli' in c[0].lower()]
    log(f"åˆå¹¶å‰JULIé¢‘é“æ•°: {len(juli_channels)}")
    log(f"åˆå¹¶åå”¯ä¸€é¢‘é“æ•°: {len(channel_dict)}")
    
    # æ˜¾ç¤ºåˆå¹¶ç»Ÿè®¡
    if channel_dict:
        total_sources = sum(len(urls) for urls in channel_dict.values())
        log(f"æ€»æºæ•°é‡: {total_sources}")
        log(f"å¹³å‡æ¯ä¸ªé¢‘é“æºæ•°: {total_sources/len(channel_dict):.1f}")
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    hk_channels_with_priority = []
    
    for extinf, urls in channel_dict.items():
        # æå–é¢‘é“å
        channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
        
        # è®¡ç®—ä¼˜å…ˆçº§
        priority = get_channel_priority(channel_name)
        
        # å­˜å‚¨ï¼šä¼˜å…ˆçº§, EXTINFè¡Œ, URLåˆ—è¡¨, é¢‘é“å
        hk_channels_with_priority.append((priority, extinf, urls, channel_name))
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    hk_channels_with_priority.sort(key=lambda x: x[0])
    
    # æå–æ’åºåçš„é¢‘é“
    hk_channels = [(extinf, urls) for _, extinf, urls, _ in hk_channels_with_priority]
    
    log(f"âœ… æå–åˆ° {len(hk_channels)} ä¸ªHKé¢‘é“ï¼ˆåŸJULIï¼Œå·²åˆå¹¶é‡å¤æºï¼‰")
    
    # æ˜¾ç¤ºæ’åºç»“æœ
    if hk_channels:
        log("HKé¢‘é“åˆå¹¶ç»“æœ (å‰10ä¸ª):")
        for i, (extinf, urls) in enumerate(hk_channels[:10], 1):
            channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
            log(f"  {i:2d}. {channel_name[:40]} - {len(urls)} ä¸ªæº")
        if len(hk_channels) > 10:
            log(f"  ... è¿˜æœ‰ {len(hk_channels) - 10} ä¸ªé¢‘é“")
    
    return hk_channels

def should_skip_channel(channel_name):
    """æ£€æŸ¥é¢‘é“æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤"""
    channel_name_lower = channel_name.lower()
    
    # æ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
    for black_word in BLACKLIST_TW:
        if black_word.lower() in channel_name_lower:
            return True
    
    return False

def extract_filtered_4gtv_channels(content, limit=30):
    """æå–4gtvé¢‘é“ï¼ˆå‰30ä¸ªï¼‰ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“"""
    if not content:
        return []
    
    log(f"æå–4gtvå‰{limit}ä¸ªç›´æ’­ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“...")
    log(f"è¿‡æ»¤åˆ—è¡¨: {', '.join(BLACKLIST_TW[:5])}...")
    
    # è§£æM3Uå†…å®¹
    lines = content.split('\n')
    channels = []
    current_extinf = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            current_extinf = line
        elif current_extinf and '://' in line and not line.startswith('#'):
            channels.append((current_extinf, line))
            current_extinf = None
    
    log(f"æ€»é¢‘é“æ•°: {len(channels)}")
    
    # è¿‡æ»¤4gtvé¢‘é“ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    filtered_channels = []
    for extinf, url in channels:
        if '4gtv' in extinf.lower():
            filtered_channels.append((extinf, url))
    
    log(f"æ‰¾åˆ° {len(filtered_channels)} ä¸ª4gtvé¢‘é“")
    
    # è¿‡æ»¤é»‘åå•é¢‘é“
    filtered_by_blacklist = []
    skipped_channels = []
    
    for extinf, url in filtered_channels:
        # æå–é¢‘é“å
        channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        if not should_skip_channel(channel_name):
            filtered_by_blacklist.append((extinf, url))
        else:
            skipped_channels.append(channel_name[:40])
    
    if skipped_channels:
        log(f"è¿‡æ»¤æ‰ {len(skipped_channels)} ä¸ªé¢‘é“")
        for i, channel in enumerate(skipped_channels[:5], 1):
            log(f"  â›” {i}. {channel}")
        if len(skipped_channels) > 5:
            log(f"  ... è¿˜æœ‰ {len(skipped_channels) - 5} ä¸ªè¢«è¿‡æ»¤çš„é¢‘é“")
    
    log(f"è¿‡æ»¤åå‰©ä½™ {len(filtered_by_blacklist)} ä¸ª4gtvé¢‘é“")
    
    # åªå–å‰limitä¸ª
    if len(filtered_by_blacklist) > limit:
        filtered_by_blacklist = filtered_by_blacklist[:limit]
        log(f"åªå–å‰ {limit} ä¸ªè¿‡æ»¤åçš„4gtvé¢‘é“")
    
    # é‡å‘½åä¸ºTWåˆ†ç»„
    tw_channels = []
    seen = set()
    
    for extinf, url in filtered_by_blacklist:
        # æ›¿æ¢åˆ†ç»„ä¸ºTW
        new_extinf = extinf
        
        # æ›¿æ¢4gtvä¸ºTWï¼ˆåœ¨é¢‘é“åä¸­ï¼‰
        if '4gtv' in new_extinf.lower():
            new_extinf = re.sub(r'4gtv', 'TW', new_extinf, flags=re.IGNORECASE)
        
        # ç¡®ä¿group-titleä¸ºTW
        if 'group-title=' in new_extinf:
            new_extinf = re.sub(r'group-title="[^"]*"', 'group-title="TW"', new_extinf)
        else:
            # æ·»åŠ group-title
            if ',' in new_extinf:
                parts = new_extinf.split(',', 1)
                new_extinf = f'{parts[0]} group-title="TW",{parts[1]}'
        
        # å»é‡
        key = f"{new_extinf}|{url}"
        if key not in seen:
            seen.add(key)
            tw_channels.append((new_extinf, url))
    
    log(f"âœ… æå–åˆ° {len(tw_channels)} ä¸ªTWé¢‘é“ï¼ˆåŸ4gtvï¼Œå·²è¿‡æ»¤ï¼‰")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªTWé¢‘é“
    if tw_channels:
        log("TWé¢‘é“ç¤ºä¾‹ (å‰5ä¸ª):")
        for i, (extinf, url) in enumerate(tw_channels[:5], 1):
            channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
            log(f"  {i:2d}. {channel_name[:40]}")
    
    return tw_channels

def main():
    """ä¸»å‡½æ•°"""
    log("ğŸš€ CCè„šæœ¬å¼€å§‹è¿è¡Œ...")
    
    # è·å–è¾“å‡ºè·¯å¾„
    output_path = get_output_path()
    log(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
    
    # æ˜¾ç¤ºå½“å‰æ—¶é—´ï¼ˆç”¨äºè°ƒè¯•å®šæ—¶ä»»åŠ¡ï¼‰
    current_time = datetime.now()
    log(f"å½“å‰æ—¶é—´: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
    log(f"ä¸‹æ¬¡è¿è¡Œ: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 17:00")
    log(f"HKä¼˜å…ˆé¡ºåº: {', '.join(HK_PRIORITY_ORDER)}")
    log(f"TWé¢‘é“è¿‡æ»¤åˆ—è¡¨: {', '.join(BLACKLIST_TW[:3])}...")
    
    # 1. ä¸‹è½½BB.m3u
    bb_content = download_bb_m3u()
    if not bb_content:
        log("âŒ æ— æ³•ç»§ç»­ï¼ŒBB.m3uä¸‹è½½å¤±è´¥")
        return
    
    # 2. ä»ä»£ç†è·å–å†…å®¹
    proxy_content = get_content_from_proxy()
    
    # 3. æ”¶é›†æ‰€æœ‰EPGæº
    epg_urls = []
    
    # ä»BB.m3uæå–EPG
    bb_epg_match = re.search(r'url-tvg="([^"]+)"', bb_content)
    if bb_epg_match:
        epg_urls.append(bb_epg_match.group(1))
        log(f"âœ… æ‰¾åˆ°BB EPG: {bb_epg_match.group(1)}")
    
    # ä»ä»£ç†å†…å®¹æå–EPG
    if proxy_content:
        proxy_epg_match = re.search(r'x-tvg-url="([^"]+)"', proxy_content)
        if proxy_epg_match:
            epg_urls.append(proxy_epg_match.group(1))
            log(f"âœ… æ‰¾åˆ°JULI EPG: {proxy_epg_match.group(1)}")
    
    # æ·»åŠ å¤‡é€‰EPG
    epg_urls.extend(BACKUP_EPG_URLS)
    
    # å»é‡
    unique_epgs = []
    for url in epg_urls:
        if url not in unique_epgs:
            unique_epgs.append(url)
    
    log(f"æ‰¾åˆ° {len(unique_epgs)} ä¸ªEPGæº")
    
    # 4. è·å–æœ€ä½³EPG
    best_epg = get_best_epg_url(unique_epgs)
    
    # 5. å…ˆæå–HKé¢‘é“ï¼ˆJULIï¼‰- æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢
    hk_channels = []
    if proxy_content:
        hk_channels = extract_and_sort_hk_channels(proxy_content)
    else:
        log("âš ï¸  æ— æ³•ä»ä»£ç†è·å–å†…å®¹ï¼Œè·³è¿‡HKé¢‘é“")
    
    # 6. å†æå–TWé¢‘é“ï¼ˆ4gtvå‰30ä¸ªï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“ï¼‰- æ’åœ¨åé¢
    tw_channels = []
    if proxy_content:
        tw_channels = extract_filtered_4gtv_channels(proxy_content, limit=30)
    else:
        log("âš ï¸  æ— æ³•ä»ä»£ç†è·å–å†…å®¹ï¼Œè·³è¿‡TWé¢‘é“")
    
    # 7. æ„å»ºM3Uå†…å®¹
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # M3Uå¤´éƒ¨ï¼ˆä½¿ç”¨æœ€ä½³EPGï¼‰
    if best_epg:
        m3u_header = f'#EXTM3U url-tvg="{best_epg}" x-tvg-url="{best_epg}"\n'
        log(f"âœ… ä½¿ç”¨EPG: {best_epg}")
    else:
        m3u_header = '#EXTM3U\n'
        log("âš ï¸  æœªæ‰¾åˆ°å¯ç”¨EPG")
    
    # è®¡ç®—BBé¢‘é“æ•°é‡
    bb_count = len(re.findall(r'^#EXTINF:', bb_content, re.MULTILINE))
    
    output = m3u_header + f"""# =============================================
# CC.m3u - ç»Ÿä¸€é¢‘é“åˆ—è¡¨
# ç”± cc_merge.py è‡ªåŠ¨ç”Ÿæˆ
# =============================================
# ç”Ÿæˆæ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# ä¸‹æ¬¡æ›´æ–°: æ¯å¤© 06:00 å’Œ 17:00 (åŒ—äº¬æ—¶é—´)
# BBæº: {BB_URL}
# ä»£ç†æº: {CLOUDFLARE_PROXY}
# JULIåˆ†ç»„å·²æ”¹ä¸ºHK (æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢)
# HKä¼˜å…ˆé¡ºåº: {', '.join(HK_PRIORITY_ORDER)}
# 4gtvåˆ†ç»„å·²æ”¹ä¸ºTW (å‰30ä¸ªï¼Œæ’åœ¨åé¢ï¼Œå·²è¿‡æ»¤æŒ‡å®šé¢‘é“)
# è¿‡æ»¤é¢‘é“: {', '.join(BLACKLIST_TW)}
# EPGæº: {best_epg if best_epg else 'æ— å¯ç”¨EPG'}
# æµ‹è¯•çš„EPGæº: {len(unique_epgs)} ä¸ª
# GitHub Actions è‡ªåŠ¨ç”Ÿæˆ

"""
    
    # æ·»åŠ BBå†…å®¹ï¼ˆè·³è¿‡ç¬¬ä¸€è¡Œï¼‰
    bb_lines = bb_content.split('\n')
    bb_actual_count = 0
    skip_first = True
    
    for line in bb_lines:
        line = line.rstrip()
        if not line:
            continue
        
        if skip_first and line.startswith('#EXTM3U'):
            skip_first = False
            continue
        
        output += line + '\n'
        if line.startswith('#EXTINF:'):
            bb_actual_count += 1
    
    # æ·»åŠ HKé¢‘é“ï¼ˆJULIï¼‰- æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢
    if hk_channels:
        output += f"""
# =============================================
# HKé¢‘é“ (åŸJULIï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢)
# =============================================
# ä¼˜å…ˆé¡ºåº: {', '.join(HK_PRIORITY_ORDER)}
# è¯´æ˜ï¼šç›¸åŒé¢‘é“çš„å¤šä¸ªæºå·²åˆå¹¶ï¼Œæ¯ä¸ªURLå•ç‹¬ä¸€è¡Œï¼Œæä¾›å†—ä½™å¤‡ä»½
"""
        
        # æ˜¾ç¤ºä¼˜å…ˆé¢‘é“
        priority_added = False
        for channel_type in HK_PRIORITY_ORDER:
            type_channels = [(extinf, urls) for extinf, urls in hk_channels if channel_type.lower() in extinf.lower()]
            if type_channels:
                if not priority_added:
                    output += f"\n# --- ä¼˜å…ˆé¢‘é“ï¼ˆæŒ‰æŒ‡å®šé¡ºåºï¼‰ ---\n"
                    priority_added = True
                
                for extinf, urls in type_channels:
                    output += extinf + '\n'
                    # æ¯ä¸ªURLå•ç‹¬ä¸€è¡Œ
                    for url in urls:
                        output += url + '\n'
                    output += '\n'  # é¢‘é“é—´ç©ºè¡Œ
        
        # æ˜¾ç¤ºå…¶ä»–HKé¢‘é“
        other_hk_channels = [(extinf, urls) for extinf, urls in hk_channels 
                           if not any(channel_type.lower() in extinf.lower() for channel_type in HK_PRIORITY_ORDER)]
        
        if other_hk_channels:
            output += f"\n# --- å…¶ä»–HKé¢‘é“ ---\n"
            for extinf, urls in other_hk_channels:
                output += extinf + '\n'
                # æ¯ä¸ªURLå•ç‹¬ä¸€è¡Œ
                for url in urls:
                    output += url + '\n'
                output += '\n'  # é¢‘é“é—´ç©ºè¡Œ
    
    # æ·»åŠ TWé¢‘é“ï¼ˆ4gtvï¼‰- æ’åœ¨åé¢ï¼ˆå·²è¿‡æ»¤ï¼‰
    if tw_channels:
        output += f"""
# =============================================
# TWé¢‘é“ (åŸ4gtvï¼Œå‰30ä¸ªï¼Œå·²è¿‡æ»¤æŒ‡å®šé¢‘é“ï¼Œæ’åœ¨HKä¹‹å)
# =============================================
# å·²è¿‡æ»¤: {', '.join(BLACKLIST_TW)}
"""
        for extinf, url in tw_channels:
            output += extinf + '\n'
            output += url + '\n'
    
    # æ·»åŠ EPGä¿¡æ¯è¯´æ˜
    if unique_epgs:
        output += f"""
# =============================================
# EPGä¿¡æ¯
# =============================================
# ä½¿ç”¨EPG: {best_epg if best_epg else 'æ— '}
# æµ‹è¯•çš„EPGæº ({len(unique_epgs)}ä¸ª):"""
        for i, epg in enumerate(unique_epgs, 1):
            status = "âœ…" if epg == best_epg else "  "
            output += f"\n#   {status} {i:2d}. {epg}"
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    # è®¡ç®—HKé¢‘é“çš„æ€»æºæ•°
    hk_total_sources = sum(len(urls) for _, urls in hk_channels) if hk_channels else 0
    
    output += f"""
# =============================================
# ç»Ÿè®¡ä¿¡æ¯
# =============================================
# BB é¢‘é“æ•°: {bb_actual_count}
# HK é¢‘é“æ•°: {len(hk_channels)} (åŸJULIï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—)
# HK æ€»æºæ•°: {hk_total_sources} (ç›¸åŒé¢‘é“å¤šä¸ªæºå·²åˆå¹¶)
# TW é¢‘é“æ•°: {len(tw_channels)} (åŸ4gtvå‰30ä¸ªï¼Œå·²è¿‡æ»¤ï¼Œæ’åœ¨å)
# è¿‡æ»¤é¢‘é“æ•°: {len(BLACKLIST_TW)} ä¸ª
# æ€»é¢‘é“æ•°: {bb_actual_count + len(hk_channels) + len(tw_channels)}
# EPGçŠ¶æ€: {'âœ… æ­£å¸¸' if best_epg else 'âŒ æ— å¯ç”¨EPG'}
# æ›´æ–°æ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# æ›´æ–°é¢‘ç‡: æ¯å¤© 06:00 å’Œ 17:00 (åŒ—äº¬æ—¶é—´)
# æ’åºè§„åˆ™: BB â†’ HK(å‡¤å‡°/NOWä¼˜å…ˆ) â†’ TW(å·²è¿‡æ»¤)
# è„šæœ¬æ–‡ä»¶: scripts/cc_merge.py
# å·¥ä½œæµ: .github/workflows/cc-workflow.yml
# ä»“åº“: https://github.com/${{ github.repository }}
"""
    
    # 8. ä¿å­˜æ–‡ä»¶
    try:
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(output)
        
        log(f"\nğŸ‰ CCè„šæœ¬å®Œæˆ!")
        log(f"ğŸ“ æ–‡ä»¶: {output_path}")
        log(f"ğŸ“ å¤§å°: {len(output)} å­—ç¬¦")
        log(f"ğŸ“¡ EPG: {best_epg if best_epg else 'æ— å¯ç”¨EPG'}")
        log(f"ğŸ“º BBé¢‘é“: {bb_actual_count}")
        log(f"ğŸ“º HKé¢‘é“: {len(hk_channels)} (æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—)")
        log(f"ğŸ“º HKæ€»æºæ•°: {hk_total_sources}")
        log(f"ğŸ“º TWé¢‘é“: {len(tw_channels)} (å·²è¿‡æ»¤æŒ‡å®šé¢‘é“)")
        log(f"ğŸ“º æ€»è®¡é¢‘é“æ•°: {bb_actual_count + len(hk_channels) + len(tw_channels)}")
        log(f"ğŸ•’ ä¸‹æ¬¡è‡ªåŠ¨æ›´æ–°: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 17:00")
        log(f"ğŸ”— å·¥ä½œæµ: .github/workflows/cc-workflow.yml")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¿å­˜æˆåŠŸ
        if os.path.exists(output_path):
            file_size = os.path.getsize(output_path)
            log(f"âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸï¼Œå¤§å°: {file_size} å­—èŠ‚")
            
            # æ˜¾ç¤ºéƒ¨åˆ†å†…å®¹ç¡®è®¤
            with open(output_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()[:20]
                log(f"ğŸ“„ æ–‡ä»¶å‰20è¡Œé¢„è§ˆ:")
                for i, line in enumerate(lines[:10], 1):
                    log(f"  {i:2d}: {line.strip()}")
                log("  ...")
        else:
            log("âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥")
            
    except Exception as e:
        log(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
        # å°è¯•ä¿å­˜åˆ°å½“å‰ç›®å½•ä½œä¸ºå¤‡ä»½
        try:
            backup_path = OUTPUT_FILE
            with open(backup_path, "w", encoding="utf-8") as f:
                f.write(output)
            log(f"âš ï¸  å·²ä¿å­˜å¤‡ä»½æ–‡ä»¶åˆ°: {backup_path}")
        except:
            log("âŒ å¤‡ä»½æ–‡ä»¶ä¿å­˜ä¹Ÿå¤±è´¥")

if __name__ == "__main__":
    main()
