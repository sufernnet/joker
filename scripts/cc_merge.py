#!/usr/bin/env python3
"""
CCåˆå¹¶è„šæœ¬ - å®Œæ•´ç‰ˆ
ç”ŸæˆCC.m3uæ–‡ä»¶ï¼Œç»Ÿä¸€ä½¿ç”¨CCå‰ç¼€ä¾¿äºè®°å¿†
1. ä¸‹è½½BB.m3uï¼ˆåŒ…å«EPGä¿¡æ¯ï¼‰
2. ä»Cloudflareä»£ç†è·å–å†…å®¹
3. æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHKï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—
4. æå–4gtvå‰30ä¸ªç›´æ’­ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“
5. åˆå¹¶ç”ŸæˆCC.m3uï¼ŒåŒ…å«å¤šä¸ªEPGæº
åŒ—äº¬æ—¶é—´æ¯å¤©6:00ã€17:00è‡ªåŠ¨è¿è¡Œ
"""

import requests
import re
import os
import time
from datetime import datetime
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®
BB_URL = "https://raw.githubusercontent.com/sufernnet/joker/main/BB.m3u"
CLOUDFLARE_PROXY = "https://smt-proxy.sufern001.workers.dev/"
OUTPUT_FILE = "CC.m3u"

# éœ€è¦è¿‡æ»¤æ‰çš„TWé¢‘é“å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
BLACKLIST_TW = [
    "Bloomberg TV",
    "Bloomberg",
    "SBNå…¨çƒè´¢ç»å°",
    "SBNè´¢ç»",
    "FRANCE24è‹±æ–‡å°",
    "FRANCE24",
    "åŠå²›å›½é™…æ–°é—»å°",
    "åŠå³¶å›½é™…",
    "NHK world-japan",
    "NHK world",
    "SBN",
    "æ—¥æœ¬",
    "NHK",
    "CNBC Asia",
    "CNBC"
]

# HKé¢‘é“ä¼˜å…ˆé¡ºåºï¼ˆæŒ‰è¿™ä¸ªé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢ï¼‰
HK_PRIORITY_ORDER = [
    "å‡¤å‡°ä¸­æ–‡",
    "å‡¤å‡°èµ„è®¯", 
    "å‡¤å‡°é¦™æ¸¯",
    "NOWæ–°é—»å°",
    "NOWæ˜Ÿå½±",
    "NOWçˆ†è°·"
]

# å¤‡é€‰EPGæºï¼ˆå¦‚æœä¸»è¦EPGå¤±æ•ˆï¼‰
BACKUP_EPG_URLS = [
    "https://epg.112114.xyz/pp.xml",
    "https://epg.946985.filegear-sg.me/t.xml.gz",
    "http://epg.51zmt.top:8000/e.xml",
    "https://epg.112114.xyz/pp.xml",
]

def log(msg):
    """è¾“å‡ºæ—¥å¿—"""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def test_epg_url(epg_url):
    """æµ‹è¯•EPG URLæ˜¯å¦å¯è®¿é—®"""
    try:
        log(f"æµ‹è¯•EPG: {epg_url}")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/xml, text/xml, */*',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive'
        }
        
        # è®¾ç½®è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´
        timeout = 15
        
        # å¯¹äº.gzæ–‡ä»¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        if epg_url.endswith('.gz'):
            response = requests.get(epg_url, headers=headers, timeout=timeout, stream=True, verify=False)
            if response.status_code == 200:
                # å°è¯•è¯»å–å‰å‡ ä¸ªå­—èŠ‚æ£€æŸ¥æ˜¯å¦æ˜¯gzip
                import gzip
                try:
                    # è¯»å–å‰100å­—èŠ‚æ£€æŸ¥
                    chunk = response.raw.read(100)
                    # å°è¯•è§£å‹
                    try:
                        gzip.decompress(chunk)
                        log(f"âœ… EPGå¯ç”¨ (GZIPæ ¼å¼): {epg_url}")
                        return True
                    except:
                        log(f"âš ï¸  EPGä¸æ˜¯æœ‰æ•ˆçš„GZIPæ ¼å¼: {epg_url}")
                        return False
                except:
                    return False
        else:
            # å¸¸è§„XMLæ–‡ä»¶
            response = requests.get(epg_url, headers=headers, timeout=timeout, stream=True, verify=False)
            
            if response.status_code == 200:
                # æ£€æŸ¥å†…å®¹ç±»å‹
                content_type = response.headers.get('content-type', '').lower()
                
                # è¯»å–å‰1KBæ£€æŸ¥
                chunk = response.raw.read(1024)
                text = chunk.decode('utf-8', errors='ignore')
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯XMLæ ¼å¼
                if '<?xml' in text or '<tv' in text or '<programme' in text:
                    log(f"âœ… EPGå¯ç”¨: {epg_url}")
                    return True
                else:
                    log(f"âš ï¸  EPGä¸æ˜¯XMLæ ¼å¼: {epg_url}")
                    return False
            else:
                log(f"âŒ EPGä¸å¯è®¿é—®: {epg_url} (çŠ¶æ€ç : {response.status_code})")
                return False
            
    except Exception as e:
        log(f"âŒ EPGæµ‹è¯•å¤±è´¥ {epg_url}: {e}")
        return False

def get_best_epg_url(epg_urls):
    """è·å–æœ€ä½³çš„EPG URL"""
    log("å¯»æ‰¾æœ€ä½³EPGæº...")
    
    # æµ‹è¯•æ‰€æœ‰EPG
    working_epgs = []
    for epg_url in epg_urls:
        if test_epg_url(epg_url):
            working_epgs.append(epg_url)
    
    if working_epgs:
        # ä¼˜å…ˆä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„
        best_epg = working_epgs[0]
        log(f"âœ… é€‰æ‹©EPG: {best_epg}")
        log(f"   å…¶ä»–å¯ç”¨EPG: {len(working_epgs)-1}ä¸ª")
        return best_epg
    else:
        log("âš ï¸  æ²¡æœ‰å¯ç”¨çš„EPGæº")
        return None

def download_bb_m3u():
    """ä¸‹è½½BB.m3uå¹¶æå–EPG"""
    try:
        log("ä¸‹è½½BB.m3u...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Referer': 'https://github.com/'
        }
        
        response = requests.get(BB_URL, headers=headers, timeout=10, verify=False)
        response.raise_for_status()
        
        bb_content = response.text
        log(f"âœ… BB.m3uä¸‹è½½æˆåŠŸ ({len(bb_content)} å­—ç¬¦)")
        
        return bb_content
        
    except Exception as e:
        log(f"âŒ BB.m3uä¸‹è½½å¤±è´¥: {e}")
        return None

def get_content_from_proxy():
    """ä»Cloudflareä»£ç†è·å–å†…å®¹"""
    try:
        log("ä»Cloudflareä»£ç†è·å–å†…å®¹...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://smart.946985.filegear-sg.me/',
            'Origin': 'https://smart.946985.filegear-sg.me',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'cross-site'
        }
        
        response = requests.get(CLOUDFLARE_PROXY, headers=headers, timeout=20, verify=False)
        
        if response.status_code == 200:
            content = response.text
            
            # å¦‚æœæ˜¯HTMLï¼Œå°è¯•æå–M3Uå†…å®¹
            if '<html' in content.lower():
                log("æ£€æµ‹åˆ°HTMLå“åº”ï¼Œå°è¯•æå–M3Uå†…å®¹...")
                
                # æ–¹æ³•1ï¼šæŸ¥æ‰¾<pre>æˆ–<code>æ ‡ç­¾ä¸­çš„å†…å®¹
                m3u_match = re.search(r'(?i)<pre[^>]*>(.*?)</pre>', content, re.DOTALL)
                if not m3u_match:
                    m3u_match = re.search(r'(?i)<code[^>]*>(.*?)</code>', content, re.DOTALL)
                
                if m3u_match:
                    content = m3u_match.group(1).strip()
                    log("âœ… ä»HTMLæ ‡ç­¾æå–åˆ°M3Uå†…å®¹")
                else:
                    # æ–¹æ³•2ï¼šæŸ¥æ‰¾#EXTM3Uå¼€å¤´çš„è¡Œ
                    lines = content.split('\n')
                    m3u_lines = []
                    in_m3u = False
                    
                    for line in lines:
                        line = line.strip()
                        if line.startswith('#EXTM3U'):
                            in_m3u = True
                            m3u_lines.append(line)
                        elif in_m3u:
                            if line.startswith('#EXTINF:') or ('://' in line and not line.startswith('<') and not line.startswith('<!')):
                                m3u_lines.append(line)
                            elif not line:
                                m3u_lines.append(line)
                            else:
                                # é‡åˆ°éM3Uå†…å®¹ï¼Œåœæ­¢æ”¶é›†
                                break
                    
                    if m3u_lines:
                        content = '\n'.join(m3u_lines)
                        log(f"âœ… æå–åˆ° {len(m3u_lines)} è¡ŒM3Uå†…å®¹")
                    else:
                        # æ–¹æ³•3ï¼šæå–æ‰€æœ‰çœ‹èµ·æ¥åƒé¢‘é“çš„è¡Œ
                        channel_lines = []
                        for line in lines:
                            line = line.strip()
                            if line.startswith('#EXTINF:') or ('.m3u8' in line and '://' in line):
                                channel_lines.append(line)
                        
                        if channel_lines:
                            content = '#EXTM3U\n' + '\n'.join(channel_lines)
                            log(f"âœ… æå–åˆ° {len(channel_lines)} ä¸ªé¢‘é“è¡Œ")
                        else:
                            log("âš ï¸  æ— æ³•ä»HTMLæå–M3Uå†…å®¹")
                            return None
            
            if content and content.strip():
                log(f"âœ… è·å–åˆ°å†…å®¹ ({len(content)} å­—ç¬¦)")
                
                # ç¡®ä¿ä»¥#EXTM3Uå¼€å¤´
                if not content.startswith('#EXTM3U'):
                    content = '#EXTM3U\n' + content
                
                return content
            else:
                log("âš ï¸  å†…å®¹ä¸ºç©º")
        else:
            log(f"âŒ ä»£ç†è¿”å›é”™è¯¯: {response.status_code}")
            
    except Exception as e:
        log(f"âŒ ä»£ç†è®¿é—®å¤±è´¥: {e}")
    
    return None

def get_channel_priority(channel_name):
    """è·å–é¢‘é“çš„ä¼˜å…ˆçº§ï¼ˆè¶Šå°è¶Šé å‰ï¼‰"""
    channel_name_lower = channel_name.lower()
    
    for i, priority_channel in enumerate(HK_PRIORITY_ORDER):
        if priority_channel.lower() in channel_name_lower:
            return i  # è¿”å›ä¼˜å…ˆçº§ç´¢å¼•ï¼Œè¶Šå°è¶Šé å‰
    
    return len(HK_PRIORITY_ORDER)  # éä¼˜å…ˆé¢‘é“æ’åœ¨æœ€å

def extract_and_sort_hk_channels(content):
    """æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHKï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—"""
    if not content:
        return []
    
    log("æå–JULIé¢‘é“ï¼Œåˆ†ç»„æ”¹ä¸ºHKï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—...")
    log(f"HKä¼˜å…ˆé¡ºåº: {', '.join(HK_PRIORITY_ORDER)}")
    
    # è§£æM3Uå†…å®¹
    lines = content.split('\n')
    channels = []
    current_extinf = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            current_extinf = line
        elif current_extinf and '://' in line and not line.startswith('#'):
            channels.append((current_extinf, line))
            current_extinf = None
    
    log(f"æ€»é¢‘é“æ•°: {len(channels)}")
    
    # è¿‡æ»¤JULIé¢‘é“å¹¶é‡å‘½å
    hk_channels_with_priority = []
    seen = set()
    juli_count = 0
    
    for extinf, url in channels:
        # æŸ¥æ‰¾JULIé¢‘é“ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        if 'juli' in extinf.lower():
            juli_count += 1
            # æå–åŸå§‹é¢‘é“å
            channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
            
            # é‡å‘½åä¸ºHKåˆ†ç»„
            new_extinf = re.sub(r'juli', 'HK', extinf, flags=re.IGNORECASE)
            
            # ç¡®ä¿group-titleä¸ºHK
            if 'group-title=' in new_extinf:
                new_extinf = re.sub(r'group-title="[^"]*"', 'group-title="HK"', new_extinf)
            else:
                # æ·»åŠ group-title
                if ',' in new_extinf:
                    parts = new_extinf.split(',', 1)
                    new_extinf = f'{parts[0]} group-title="HK",{parts[1]}'
            
            # å»é‡
            key = f"{new_extinf}|{url}"
            if key not in seen:
                seen.add(key)
                # è®¡ç®—ä¼˜å…ˆçº§
                priority = get_channel_priority(channel_name)
                hk_channels_with_priority.append((priority, new_extinf, url, channel_name))
    
    log(f"æ‰¾åˆ° {juli_count} ä¸ªJULIé¢‘é“")
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    hk_channels_with_priority.sort(key=lambda x: x[0])
    
    # æå–æ’åºåçš„é¢‘é“
    hk_channels = [(extinf, url) for _, extinf, url, _ in hk_channels_with_priority]
    
    log(f"âœ… æå–åˆ° {len(hk_channels)} ä¸ªHKé¢‘é“ï¼ˆåŸJULIï¼‰")
    
    # æ˜¾ç¤ºæ’åºç»“æœ
    if hk_channels:
        log("HKé¢‘é“æ’åºç»“æœ:")
        for i, (extinf, url) in enumerate(hk_channels[:10], 1):  # æ˜¾ç¤ºå‰10ä¸ª
            channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
            log(f"  {i:2d}. {channel_name}")
        if len(hk_channels) > 10:
            log(f"  ... è¿˜æœ‰ {len(hk_channels) - 10} ä¸ªé¢‘é“")
    
    return hk_channels

def should_skip_channel(channel_name):
    """æ£€æŸ¥é¢‘é“æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤"""
    channel_name_lower = channel_name.lower()
    
    # æ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
    for black_word in BLACKLIST_TW:
        if black_word.lower() in channel_name_lower:
            log(f"  è¿‡æ»¤æ‰: {channel_name} (åŒ…å«: {black_word})")
            return True
    
    return False

def extract_filtered_4gtv_channels(content, limit=30):
    """æå–4gtvé¢‘é“ï¼ˆå‰30ä¸ªï¼‰ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“"""
    if not content:
        return []
    
    log(f"æå–4gtvå‰{limit}ä¸ªç›´æ’­ï¼Œåˆ†ç»„æ”¹ä¸ºTWï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“...")
    log(f"è¿‡æ»¤åˆ—è¡¨: {', '.join(BLACKLIST_TW)}")
    
    # è§£æM3Uå†…å®¹
    lines = content.split('\n')
    channels = []
    current_extinf = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith('#EXTINF:'):
            current_extinf = line
        elif current_extinf and '://' in line and not line.startswith('#'):
            channels.append((current_extinf, line))
            current_extinf = None
    
    log(f"æ€»é¢‘é“æ•°: {len(channels)}")
    
    # è¿‡æ»¤4gtvé¢‘é“ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    filtered_channels = []
    for extinf, url in channels:
        if '4gtv' in extinf.lower():
            filtered_channels.append((extinf, url))
    
    log(f"æ‰¾åˆ° {len(filtered_channels)} ä¸ª4gtvé¢‘é“")
    
    # è¿‡æ»¤é»‘åå•é¢‘é“
    filtered_by_blacklist = []
    for extinf, url in filtered_channels:
        # æå–é¢‘é“å
        channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        if not should_skip_channel(channel_name):
            filtered_by_blacklist.append((extinf, url))
        else:
            log(f"  â›” è¿‡æ»¤: {channel_name}")
    
    log(f"è¿‡æ»¤åå‰©ä½™ {len(filtered_by_blacklist)} ä¸ª4gtvé¢‘é“")
    
    # åªå–å‰limitä¸ª
    if len(filtered_by_blacklist) > limit:
        filtered_by_blacklist = filtered_by_blacklist[:limit]
        log(f"åªå–å‰ {limit} ä¸ªè¿‡æ»¤åçš„4gtvé¢‘é“")
    
    # é‡å‘½åä¸ºTWåˆ†ç»„
    tw_channels = []
    seen = set()
    
    for extinf, url in filtered_by_blacklist:
        # æ›¿æ¢åˆ†ç»„ä¸ºTW
        new_extinf = extinf
        
        # æ›¿æ¢4gtvä¸ºTWï¼ˆåœ¨é¢‘é“åä¸­ï¼‰
        if '4gtv' in new_extinf.lower():
            new_extinf = re.sub(r'4gtv', 'TW', new_extinf, flags=re.IGNORECASE)
        
        # ç¡®ä¿group-titleä¸ºTW
        if 'group-title=' in new_extinf:
            new_extinf = re.sub(r'group-title="[^"]*"', 'group-title="TW"', new_extinf)
        else:
            # æ·»åŠ group-title
            if ',' in new_extinf:
                parts = new_extinf.split(',', 1)
                new_extinf = f'{parts[0]} group-title="TW",{parts[1]}'
        
        # å»é‡
        key = f"{new_extinf}|{url}"
        if key not in seen:
            seen.add(key)
            tw_channels.append((new_extinf, url))
    
    log(f"âœ… æå–åˆ° {len(tw_channels)} ä¸ªTWé¢‘é“ï¼ˆåŸ4gtvï¼Œå·²è¿‡æ»¤ï¼‰")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªTWé¢‘é“
    if tw_channels:
        log("TWé¢‘é“ç¤ºä¾‹:")
        for i, (extinf, url) in enumerate(tw_channels[:5], 1):
            channel_name = extinf.split(',', 1)[1] if ',' in extinf else extinf
            log(f"  {i:2d}. {channel_name}")
    
    return tw_channels

def main():
    """ä¸»å‡½æ•°"""
    log("ğŸš€ CCè„šæœ¬å¼€å§‹è¿è¡Œ...")
    log(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    
    # æ˜¾ç¤ºå½“å‰æ—¶é—´ï¼ˆç”¨äºè°ƒè¯•å®šæ—¶ä»»åŠ¡ï¼‰
    current_time = datetime.now()
    log(f"å½“å‰æ—¶é—´: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
    log(f"ä¸‹æ¬¡è¿è¡Œ: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 17:00")
    log(f"HKä¼˜å…ˆé¡ºåº: {', '.join(HK_PRIORITY_ORDER)}")
    log(f"TWé¢‘é“è¿‡æ»¤åˆ—è¡¨: {', '.join(BLACKLIST_TW)}")
    
    # 1. ä¸‹è½½BB.m3u
    bb_content = download_bb_m3u()
    if not bb_content:
        log("âŒ æ— æ³•ç»§ç»­ï¼ŒBB.m3uä¸‹è½½å¤±è´¥")
        return
    
    # 2. ä»ä»£ç†è·å–å†…å®¹
    proxy_content = get_content_from_proxy()
    
    # 3. æ”¶é›†æ‰€æœ‰EPGæº
    epg_urls = []
    
    # ä»BB.m3uæå–EPG
    bb_epg_match = re.search(r'url-tvg="([^"]+)"', bb_content)
    if bb_epg_match:
        epg_urls.append(bb_epg_match.group(1))
        log(f"âœ… æ‰¾åˆ°BB EPG: {bb_epg_match.group(1)}")
    
    # ä»ä»£ç†å†…å®¹æå–EPG
    if proxy_content:
        proxy_epg_match = re.search(r'x-tvg-url="([^"]+)"', proxy_content)
        if proxy_epg_match:
            epg_urls.append(proxy_epg_match.group(1))
            log(f"âœ… æ‰¾åˆ°JULI EPG: {proxy_epg_match.group(1)}")
    
    # æ·»åŠ å¤‡é€‰EPG
    epg_urls.extend(BACKUP_EPG_URLS)
    
    # å»é‡
    unique_epgs = []
    for url in epg_urls:
        if url not in unique_epgs:
            unique_epgs.append(url)
    
    log(f"æ‰¾åˆ° {len(unique_epgs)} ä¸ªEPGæº")
    
    # 4. è·å–æœ€ä½³EPG
    best_epg = get_best_epg_url(unique_epgs)
    
    # 5. å…ˆæå–HKé¢‘é“ï¼ˆJULIï¼‰- æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢
    hk_channels = []
    if proxy_content:
        hk_channels = extract_and_sort_hk_channels(proxy_content)
    else:
        log("âš ï¸  æ— æ³•ä»ä»£ç†è·å–å†…å®¹ï¼Œè·³è¿‡HKé¢‘é“")
    
    # 6. å†æå–TWé¢‘é“ï¼ˆ4gtvå‰30ä¸ªï¼Œè¿‡æ»¤æŒ‡å®šé¢‘é“ï¼‰- æ’åœ¨åé¢
    tw_channels = []
    if proxy_content:
        tw_channels = extract_filtered_4gtv_channels(proxy_content, limit=30)
    else:
        log("âš ï¸  æ— æ³•ä»ä»£ç†è·å–å†…å®¹ï¼Œè·³è¿‡TWé¢‘é“")
    
    # 7. æ„å»ºM3Uå†…å®¹
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # M3Uå¤´éƒ¨ï¼ˆä½¿ç”¨æœ€ä½³EPGï¼‰
    if best_epg:
        m3u_header = f'#EXTM3U url-tvg="{best_epg}" x-tvg-url="{best_epg}"\n'
        log(f"âœ… ä½¿ç”¨EPG: {best_epg}")
    else:
        m3u_header = '#EXTM3U\n'
        log("âš ï¸  æœªæ‰¾åˆ°å¯ç”¨EPG")
    
    output = m3u_header + f"""# =============================================
# CC.m3u - ç»Ÿä¸€é¢‘é“åˆ—è¡¨
# ç”± cc_merge.py è‡ªåŠ¨ç”Ÿæˆ
# =============================================
# ç”Ÿæˆæ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# ä¸‹æ¬¡æ›´æ–°: æ¯å¤© 06:00 å’Œ 17:00 (åŒ—äº¬æ—¶é—´)
# BBæº: {BB_URL}
# ä»£ç†æº: {CLOUDFLARE_PROXY}
# JULIåˆ†ç»„å·²æ”¹ä¸ºHK (æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢)
# HKä¼˜å…ˆé¡ºåº: {', '.join(HK_PRIORITY_ORDER)}
# 4gtvåˆ†ç»„å·²æ”¹ä¸ºTW (å‰30ä¸ªï¼Œæ’åœ¨åé¢ï¼Œå·²è¿‡æ»¤æŒ‡å®šé¢‘é“)
# è¿‡æ»¤é¢‘é“: {', '.join(BLACKLIST_TW)}
# EPGæº: {best_epg if best_epg else 'æ— å¯ç”¨EPG'}
# æµ‹è¯•çš„EPGæº: {len(unique_epgs)} ä¸ª
# GitHub Actions è‡ªåŠ¨ç”Ÿæˆ

"""
    
    # æ·»åŠ BBå†…å®¹ï¼ˆè·³è¿‡ç¬¬ä¸€è¡Œï¼‰
    bb_lines = bb_content.split('\n')
    bb_count = 0
    skip_first = True
    
    for line in bb_lines:
        line = line.rstrip()
        if not line:
            continue
        
        if skip_first and line.startswith('#EXTM3U'):
            skip_first = False
            continue
        
        output += line + '\n'
        if line.startswith('#EXTINF:'):
            bb_count += 1
    
    # æ·»åŠ HKé¢‘é“ï¼ˆJULIï¼‰- æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢
    if hk_channels:
        output += f"""
# =============================================
# HKé¢‘é“ (åŸJULIï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åœ¨æœ€å‰é¢)
# =============================================
# ä¼˜å…ˆé¡ºåº: {', '.join(HK_PRIORITY_ORDER)}
"""
        
        # æ˜¾ç¤ºä¼˜å…ˆé¢‘é“
        priority_added = False
        for channel_type in HK_PRIORITY_ORDER:
            type_channels = [(extinf, url) for extinf, url in hk_channels if channel_type.lower() in extinf.lower()]
            if type_channels:
                if not priority_added:
                    output += f"\n# --- ä¼˜å…ˆé¢‘é“ï¼ˆæŒ‰æŒ‡å®šé¡ºåºï¼‰ ---\n"
                    priority_added = True
                
                for extinf, url in type_channels:
                    output += extinf + '\n'
                    output += url + '\n'
        
        # æ˜¾ç¤ºå…¶ä»–HKé¢‘é“
        other_hk_channels = [(extinf, url) for extinf, url in hk_channels 
                           if not any(channel_type.lower() in extinf.lower() for channel_type in HK_PRIORITY_ORDER)]
        
        if other_hk_channels:
            output += f"\n# --- å…¶ä»–HKé¢‘é“ ---\n"
            for extinf, url in other_hk_channels:
                output += extinf + '\n'
                output += url + '\n'
    
    # æ·»åŠ TWé¢‘é“ï¼ˆ4gtvï¼‰- æ’åœ¨åé¢ï¼ˆå·²è¿‡æ»¤ï¼‰
    if tw_channels:
        output += f"""
# =============================================
# TWé¢‘é“ (åŸ4gtvï¼Œå‰30ä¸ªï¼Œå·²è¿‡æ»¤æŒ‡å®šé¢‘é“ï¼Œæ’åœ¨HKä¹‹å)
# =============================================
# å·²è¿‡æ»¤: {', '.join(BLACKLIST_TW)}
"""
        for extinf, url in tw_channels:
            output += extinf + '\n'
            output += url + '\n'
    
    # æ·»åŠ EPGä¿¡æ¯è¯´æ˜
    if unique_epgs:
        output += f"""
# =============================================
# EPGä¿¡æ¯
# =============================================
# ä½¿ç”¨EPG: {best_epg if best_epg else 'æ— '}
# æµ‹è¯•çš„EPGæº ({len(unique_epgs)}ä¸ª):"""
        for i, epg in enumerate(unique_epgs, 1):
            status = "âœ…" if epg == best_epg else "  "
            output += f"\n#   {status} {epg}"
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    output += f"""
# =============================================
# ç»Ÿè®¡ä¿¡æ¯
# =============================================
# BB é¢‘é“æ•°: {bb_count}
# HK é¢‘é“æ•°: {len(hk_channels)} (åŸJULIï¼ŒæŒ‰æŒ‡å®šé¡ºåºæ’åˆ—)
# TW é¢‘é“æ•°: {len(tw_channels)} (åŸ4gtvå‰30ä¸ªï¼Œå·²è¿‡æ»¤ï¼Œæ’åœ¨å)
# è¿‡æ»¤é¢‘é“: {len(BLACKLIST_TW)} ä¸ª
# æ€»é¢‘é“æ•°: {bb_count + len(hk_channels) + len(tw_channels)}
# EPGçŠ¶æ€: {'âœ… æ­£å¸¸' if best_epg else 'âŒ æ— å¯ç”¨EPG'}
# æ›´æ–°æ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)
# æ›´æ–°é¢‘ç‡: æ¯å¤© 06:00 å’Œ 17:00 (åŒ—äº¬æ—¶é—´)
# æ’åºè§„åˆ™: BB â†’ HK(å‡¤å‡°/NOWä¼˜å…ˆ) â†’ TW(å·²è¿‡æ»¤)
# è„šæœ¬æ–‡ä»¶: cc_merge.py
# å·¥ä½œæµ: .github/workflows/cc-workflow.yml
"""
    
    # 8. ä¿å­˜æ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(output)
    
    log(f"\nğŸ‰ CCè„šæœ¬å®Œæˆ!")
    log(f"ğŸ“ æ–‡ä»¶: {OUTPUT_FILE}")
    log(f"ğŸ“ å¤§å°: {len(output)} å­—ç¬¦")
    log(f"ğŸ“¡ EPG: {best_epg if best_epg else 'æ— å¯ç”¨EPG'}")
    log(f"ğŸ“º BBé¢‘é“: {bb_count}")
    log(f"ğŸ“º HKé¢‘é“: {len(hk_channels)} (æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—)")
    log(f"ğŸ“º TWé¢‘é“: {len(tw_channels)} (å·²è¿‡æ»¤æŒ‡å®šé¢‘é“)")
    log(f"ğŸ“º æ€»è®¡: {bb_count + len(hk_channels) + len(tw_channels)}")
    log(f"ğŸ•’ ä¸‹æ¬¡è‡ªåŠ¨æ›´æ–°: åŒ—äº¬æ—¶é—´ 06:00 å’Œ 17:00")
    log(f"ğŸ”— å·¥ä½œæµ: .github/workflows/cc-workflow.yml")

if __name__ == "__main__":
    main()
